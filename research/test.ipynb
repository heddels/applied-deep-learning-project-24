{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:52.232193Z",
     "start_time": "2024-12-08T21:42:52.227763Z"
    }
   },
   "source": "import os",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:52.255322Z",
     "start_time": "2024-12-08T21:42:52.252682Z"
    }
   },
   "cell_type": "code",
   "source": "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"",
   "id": "fc51803bfb9245a1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:52.290420Z",
     "start_time": "2024-12-08T21:42:52.287146Z"
    }
   },
   "cell_type": "code",
   "source": "%pwd",
   "id": "e0e2eebc1f516b7f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/heddafiedler/Documents/MASTER_DATA_SCIENCE/Semester_3/DL/DL_Project/research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:52.377786Z",
     "start_time": "2024-12-08T21:42:52.374909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# changing working directory to the root of the project:/Users/heddafiedler/Documents/MASTER_DATA_SCIENCE/Semester_3/DL/DL_Project\n",
    "os.chdir(\"/Users/heddafiedler/Documents/MASTER_DATA_SCIENCE/Semester_3/DL/DL_Project\")"
   ],
   "id": "4c54405f4edde846",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Purpose of the Notebook\n",
    "\n",
    "In this notebook I create the pipeline for using the model built in the MAGPIE Repository in order to understand its components and test how the elements work together.\n",
    "Besides, I already look into the parts I want to change, like more detailed logging / debugging steps to better understand the process. Apart from that I will use the code provided by the Repository.\n",
    "\n",
    "In order to understand the core elements of the model architecture and pipeline, I will only display the main parts here and import utils and other functions."
   ],
   "id": "34ee8ec75e326ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Ingestion and Preprocessing\n",
    "Since the repository already provides the datasets in a preprocessed way, I will use these files for the model training according to the data sets I chose (see README file).\n",
    "Nevertheless, I will need to include data preprocessing in order to do inference on new data. Therefore, the following part tests the preporcessing of random text input and the tokenization of the text. This test is later used to try out how the model works with the data."
   ],
   "id": "4bfa411557c18a2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:54.606428Z",
     "start_time": "2024-12-08T21:42:52.387552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Tokenization\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Initialize the fast tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ],
   "id": "3aeab84bb699e614",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:54.627281Z",
     "start_time": "2024-12-08T21:42:54.615271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"This is a test\"\n",
    "tokenized = tokenizer(text, truncation=True, return_tensors=\"pt\")\n",
    "print(tokenized)"
   ],
   "id": "87f5ba0464052811",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Initialization with Task and Subtask Classes\n",
    "Since the MTL approach is about combining different tasks, the data needs to contain the information bout which model head it needs in the model pipeline.\n",
    "I will use the Task and Subtask classes from the MAGPIE Repository to create the tasks I want to use. The tasks are defined in the following part.\n",
    "\n",
    "The Sub Tasks I am going to use according to the datasets I chose are:\n",
    "- Token-Level Classification (POS) --> noch besser verstehen was das genau ist\n",
    "- Binary Classification\n",
    "- Multi-Class Classification\n",
    "- (Regression) - not used in current implementation\n",
    "- (Masked Language Modelling) - not used in current implementation\n",
    "\n",
    "The Subtask class defines how to load and structure the respective data set, as well as other functions like weight scaling and class weights for imbalanced datasets\n",
    "\n",
    "The Task class is a wrapper for the subtasks and contains the task id and the subtasks list. Since I am using only one dataset for each subtask, the subtask list contains only one subtask. Nevertheless, I am leaving the wrapper in the code in order to use the other steps in the same way as in the repository.\n",
    "\n",
    "The SubTaskDataset class creates then the actual data loaders and also contains the BatchList class for Training and for Evaluation.\n",
    "\n"
   ],
   "id": "a01c107148bf6d4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:54.683829Z",
     "start_time": "2024-12-08T21:42:54.642226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from media_bias_detection.config.config import DEV_RATIO, MAX_LENGTH, REGRESSION_SCALAR, TRAIN_RATIO\n",
    "from media_bias_detection.utils.common import get_class_weights\n",
    "from media_bias_detection.utils.enums import Split\n"
   ],
   "id": "5bf648f5c7dc733c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:54.694599Z",
     "start_time": "2024-12-08T21:42:54.691699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"This part contains the Task class.\"\"\"\n",
    "\n",
    "class Task:\n",
    "    \"\"\"Wrap subtasks.\"\"\"\n",
    "\n",
    "    def __init__(self, task_id, subtasks_list):\n",
    "        \"\"\"Initialize a Task.\"\"\"\n",
    "        self.task_id = task_id\n",
    "        self.subtasks_list = subtasks_list\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a task.\"\"\"\n",
    "        return (\n",
    "            f\"Task {self.task_id} with {len(self.subtasks_list)} subtask{'s' if len(self.subtasks_list) > 1 else ''}\"\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.task_id)"
   ],
   "id": "8c71e739a9294f01",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:54.728159Z",
     "start_time": "2024-12-08T21:42:54.709908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"This part contains the Subtask.\"\"\"\n",
    "\n",
    "def get_pos_idxs(pos: str, text: str):\n",
    "    \"\"\"\n",
    "    Get the correct idxs of the pos for a given text.\n",
    "\n",
    "    @param pos: A pattern as text.\n",
    "    @param text: The text to search trough.\n",
    "    @return: The ids of the tokens in the text that match the pattern.\n",
    "    \"\"\"\n",
    "    if pos == text:\n",
    "        mask = np.array(np.ones((len(text))), dtype=\"int\")\n",
    "    else:\n",
    "        pos = pos.replace(\"[\", \"\\[\")\n",
    "        pos = pos.replace(\"$\", \"\\$\")\n",
    "        pos = pos.replace(\"?\", \"\\?\")\n",
    "        pos = pos.replace(\")\", \"\\)\")\n",
    "        pos = pos.replace(\"(\", \"\\(\")\n",
    "        pos = pos.replace(\"*\", \"\\*\")\n",
    "        pos = pos.replace(\"+\", \"\\+\")\n",
    "        start, end = re.search(pos, text).span()\n",
    "\n",
    "        mask = np.zeros((len(text)), dtype=int)\n",
    "        mask[start:end] = 1\n",
    "    c, idx_list = 0, []\n",
    "    for t in text.split():\n",
    "        idx_list.append(c)\n",
    "        c += len(t) + 1\n",
    "    mask_idxs = [mask[i] for i in idx_list]\n",
    "    return mask_idxs\n",
    "\n",
    "\n",
    "def align_labels_with_tokens(labels: List[int], word_ids: List[int]):\n",
    "    \"\"\"Align labels with tokens.\n",
    "\n",
    "    C/p from https://huggingface.co/course/chapter7/2\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = (\n",
    "                -100 if word_id is None else labels[word_id]\n",
    "            )  # -100 is an index that will be ignored by cross entropy\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def get_tokens_and_labels(pos_list_list, text_list, labels):\n",
    "    \"\"\"Get tokens and labels for scattered POS.\n",
    "\n",
    "    In this objective, we have a list of consecutive spans.\n",
    "    For each of these consecutive spans, find the correct index of the corresponding tokens in the text_list.\n",
    "    Returns the bitwise or ('union') of this ids.\n",
    "    \"\"\"\n",
    "    mask_idxs_list = []\n",
    "    for i, pos_list in enumerate(pos_list_list):\n",
    "        label = labels[i]\n",
    "        text = text_list[i]\n",
    "        observation_mask_idxs = []\n",
    "        for pos in pos_list:\n",
    "            if len(pos) == 0:\n",
    "                # If there is no POS, we just return zeros\n",
    "                observation_mask_idxs.append(get_pos_idxs(\"\", text))\n",
    "            else:\n",
    "                for pos in pos_list:\n",
    "                    if label == 0:  # In that case, the label is the neutral class\n",
    "                        observation_mask_idxs.append(get_pos_idxs(pos, text))\n",
    "                    else:\n",
    "                        pos_idxs = get_pos_idxs(pos, text)\n",
    "                        pos_idxs = [label if idx == 1 else 0 for idx in pos_idxs]\n",
    "                        observation_mask_idxs.append(pos_idxs)\n",
    "\n",
    "        # reduce observation_mask_idxs\n",
    "        observation_mask_idxs = np.bitwise_or.reduce(observation_mask_idxs, axis=0)\n",
    "        mask_idxs_list.append(observation_mask_idxs)\n",
    "\n",
    "    return [t.split() for t in text_list], mask_idxs_list\n",
    "\n",
    "\n",
    "class SubTask:\n",
    "    \"\"\"A Subtask.\"\"\"\n",
    "\n",
    "    def __init__(self, id, task_id, filename, src_col=\"text\", tgt_cols_list=[\"label\"], *args, **kwargs):\n",
    "        \"\"\"Raise RuntimeError if this SubTask is instantiated.\"\"\"\n",
    "        general_logger.info(f\"Initializing SubTask {id} for task {task_id}\")\n",
    "        if type(self) == SubTask:\n",
    "            raise RuntimeError(\"Abstract class <SubTask> must not be instantiated.\")\n",
    "        self.attention_masks = None\n",
    "        self.Y = None\n",
    "        self.X = None\n",
    "        self.class_weights = None\n",
    "        self.id = id\n",
    "        self.src_col = src_col\n",
    "        self.tgt_cols_list = tgt_cols_list\n",
    "        self.task_id = task_id\n",
    "        self.filename = os.path.join(\"datasets\", filename)\n",
    "        self.processed = False\n",
    "\n",
    "    def process(self, force_download: bool = False):\n",
    "        \"\"\"Process a SubTask.\n",
    "\n",
    "        Load the data for this subtask, set properties X, Y and attention_mask.\n",
    "        \"\"\"\n",
    "        general_logger.info(f\"Processing SubTask {self.id}\")\n",
    "        X, Y, attention_masks = self.load_data()\n",
    "\n",
    "        train_split = int(len(X) * TRAIN_RATIO)\n",
    "        dev_split = train_split + int(len(X) * DEV_RATIO)\n",
    "\n",
    "        self.X = {Split.TRAIN: X[:train_split], Split.DEV: X[train_split:dev_split], Split.TEST: X[dev_split:]}\n",
    "        self.attention_masks = {\n",
    "            Split.TRAIN: attention_masks[:train_split],\n",
    "            Split.DEV: attention_masks[train_split:dev_split],\n",
    "            Split.TEST: attention_masks[dev_split:],\n",
    "        }\n",
    "        self.Y = {Split.TRAIN: Y[:train_split], Split.DEV: Y[train_split:dev_split], Split.TEST: Y[dev_split:]}\n",
    "        self.create_class_weights()\n",
    "        self.processed = True\n",
    "        general_logger.info(f\"SubTask {self.id} processed successfully\")\n",
    "\n",
    "    def load_data(self) -> Tuple:\n",
    "        \"\"\"Load the data of a SubTask.\n",
    "\n",
    "        Must be implemented for inherited.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_class_weights(self):\n",
    "        \"\"\"Compute the weights for imbalanced classes.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_scaling_weight(self):\n",
    "        \"\"\"Get the scaling weight of a Subtask.\n",
    "\n",
    "        Needs to be overwritten.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_X(self, split: Split):\n",
    "        \"\"\"Get all X of a given split.\"\"\"\n",
    "        return self.X[split]\n",
    "\n",
    "    def get_att_mask(self, split: Split):\n",
    "        \"\"\"Get attention_masks for inputs of a given split.\"\"\"\n",
    "        return self.attention_masks[split]\n",
    "\n",
    "    def get_Y(self, split: Split):\n",
    "        \"\"\"Get all Y of a given split.\"\"\"\n",
    "        return self.Y[split]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.id)\n",
    "\n",
    "\n",
    "# a[43485:43500]\n",
    "class ClassificationSubTask(SubTask):\n",
    "    \"\"\"A ClassificationSubTask.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=2, *args, **kwargs):\n",
    "        \"\"\"Initialize a ClassificationSubTask.\"\"\"\n",
    "        super(ClassificationSubTask, self).__init__(num_classes=num_classes, *args, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def load_data(self) -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"Load the data of a ClassificationSubTask.\"\"\"\n",
    "        df = pd.read_csv(self.filename)\n",
    "\n",
    "        X, Y = df[self.src_col], df[self.tgt_cols_list]\n",
    "        tokenized_inputs = tokenizer(X.to_list(), padding=\"max_length\", truncation=True,\n",
    "                                     max_length=MAX_LENGTH)\n",
    "        X = tokenized_inputs.get(\"input_ids\")\n",
    "        attention_masks = tokenized_inputs.get(\"attention_mask\")\n",
    "        assert Y.nunique().squeeze() == self.num_classes\n",
    "        assert Y[self.tgt_cols_list[0]].min(axis=0) == 0\n",
    "        if self.num_classes == 2:  # if it's binary classification\n",
    "            Y = Y.to_numpy()\n",
    "        else:\n",
    "            Y = Y[self.tgt_cols_list].to_numpy()\n",
    "        return torch.LongTensor(X), torch.LongTensor(Y), torch.LongTensor(attention_masks)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a Classification Subtask.\"\"\"\n",
    "        return f\"{'Multi-class' if self.num_classes != 2 else 'Binary'} Classification\"\n",
    "\n",
    "    def create_class_weights(self):\n",
    "        \"\"\"Compute the weights.\"\"\"\n",
    "        self.class_weights = get_class_weights(self.Y[Split.TRAIN], method=\"isns\")\n",
    "\n",
    "    def get_scaling_weight(self):\n",
    "        \"\"\"Get the weight of a Classification Subtask.\n",
    "\n",
    "        As with the other tasks, we normalize by the natural logarithm of the domain size.\n",
    "        \"\"\"\n",
    "        return 1 / np.log(self.num_classes)\n",
    "\n",
    "\n",
    "# in current implementation, the regression subtask is not used\n",
    "class RegressionSubTask(SubTask):\n",
    "    \"\"\"A RegressionSubTask.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize a RegressionSubTask.\"\"\"\n",
    "        super(RegressionSubTask, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def load_data(self) -> Tuple[torch.LongTensor, torch.FloatTensor, torch.LongTensor]:\n",
    "        \"\"\"Load the data of a RegressionSubTask.\"\"\"\n",
    "        df = pd.read_csv(self.filename)\n",
    "        X, Y = df[self.src_col], df[self.tgt_cols_list]\n",
    "        tokenized_inputs = tokenizer(X.to_list(), padding=\"max_length\", truncation=True,\n",
    "                                     max_length=MAX_LENGTH)\n",
    "        X = tokenized_inputs.get(\"input_ids\")\n",
    "        attention_masks = tokenized_inputs.get(\"attention_mask\")\n",
    "        Y = (((Y - Y.min()) / (Y.max() - Y.min())).to_numpy()).astype(\"float32\")  # scale from 0 to 1\n",
    "        return torch.LongTensor(X), torch.FloatTensor(Y), torch.LongTensor(attention_masks)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a Regression Subtask.\"\"\"\n",
    "        return \"Regression\"\n",
    "\n",
    "    def get_scaling_weight(self):\n",
    "        \"\"\"Get the scaling weight of a Regression Subtask.\n",
    "\n",
    "        As of now, this scaling weight is a simple scalar and is a mere heuristic-based approximation (ie. we eyeballed it).\n",
    "        \"\"\"\n",
    "        return REGRESSION_SCALAR\n",
    "\n",
    "\n",
    "class MultiLabelClassificationSubTask(SubTask):\n",
    "    \"\"\"A MultiLabelClassificationSubTask.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=2, num_labels=2, *args, **kwargs):\n",
    "        \"\"\"Initialize a MultiLabelClassificationSubTask.\"\"\"\n",
    "        super(MultiLabelClassificationSubTask, self).__init__(num_classes=2, num_labels=2, *args, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def load_data(self) -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"Load the data of a MultiLabelClassificationSubTask.\"\"\"\n",
    "        df = pd.read_csv(self.filename)\n",
    "        X, Y = df[self.src_col], df[self.tgt_cols_list]\n",
    "        tokenized_inputs = tokenizer(X.to_list(), padding=\"max_length\", truncation=True,\n",
    "                                     max_length=MAX_LENGTH)\n",
    "        X = tokenized_inputs.get(\"input_ids\")\n",
    "        attention_masks = tokenized_inputs.get(\"attention_mask\")\n",
    "        assert Y.max(axis=0).to_numpy().max() == 1\n",
    "        Y = Y.to_numpy()\n",
    "        return torch.LongTensor(X), torch.LongTensor(Y), torch.LongTensor(attention_masks)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a Multi-label Classification Subtask.\"\"\"\n",
    "        return \"Multi-label Classification\"\n",
    "\n",
    "    def get_scaling_weight(self):\n",
    "        \"\"\"Get the weight of a Multi-label Classification Subtask.\n",
    "\n",
    "        As with the other tasks, we normalize by the natural logarithm of the domain size.\n",
    "        \"\"\"\n",
    "        return 1 / np.log(self.num_classes * self.num_labels)\n",
    "\n",
    "\n",
    "class POSSubTask(SubTask):\n",
    "    \"\"\"A POSSubTask.\n",
    "\n",
    "    Each POSSubTask can be either binary classification or multiclass classification.\n",
    "    If it is binary classification, zero (0) must be the neutral class.\n",
    "    This neutral class is also applied to all other, 'normal' tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tgt_cols_list, label_col=None, *args, **kwargs):\n",
    "        \"\"\"Initialize a POSSubTask.\n",
    "\n",
    "        Normally, we have 3 classes: (0=no-tag, 1=tag-start, 2=tag-continue)\n",
    "        However, we have POS-tasks where we have more than just 'binary token level classification'.\n",
    "        In these scenarios, each class has two tags: 'tag-start' and 'tag-continue'.\n",
    "        The 'no-class' tag has no 'tag-continue'.\n",
    "        \"\"\"\n",
    "        super(POSSubTask, self).__init__(tgt_cols_list=tgt_cols_list, *args, **kwargs)\n",
    "        self.num_classes = 3   # The default num_classes is 2 or 3 (0=no-tag, 1=tag-start, 2=tag-continue)\n",
    "        self.label_col = label_col\n",
    "        assert len(tgt_cols_list) == 1\n",
    "\n",
    "    def load_data(self) -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"Load the data of a POSSubTask.\"\"\"\n",
    "        df = pd.read_csv(self.filename)\n",
    "\n",
    "        df[self.tgt_cols_list] = df[self.tgt_cols_list].fillna(\"\")\n",
    "        mask = df.apply(\n",
    "            lambda row: all([p in row[self.src_col] for p in row[self.tgt_cols_list[0]].split(\";\")]), axis=1\n",
    "        )\n",
    "        df = df[mask].reset_index(drop=True)\n",
    "        assert sum(mask) == len(df[self.tgt_cols_list]), \"At least one POS is not contained in the source column.\"\n",
    "\n",
    "        pos_list_list = df[self.tgt_cols_list[0]].apply(lambda x: x.split(\";\")).to_list()\n",
    "        X = df[self.src_col].values\n",
    "        # If we do not provide a labels column, we assume that, whenever a pos is present, that is the non-neutral class\n",
    "        labels = (\n",
    "            df[self.label_col]\n",
    "            if self.label_col\n",
    "            else [1 if len(pos) > 0 else 0 for pos in df[self.tgt_cols_list[0]].to_list()]\n",
    "        )\n",
    "        tokens, labels = get_tokens_and_labels(pos_list_list=pos_list_list, text_list=X, labels=labels)\n",
    "        tokenized_inputs = tokenizer(\n",
    "            tokens, padding=\"max_length\", is_split_into_words=True, truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        new_labels = []\n",
    "        for i, labels in enumerate(labels):\n",
    "            word_ids = tokenized_inputs.word_ids(i)\n",
    "            new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        Y = np.array(new_labels)\n",
    "        # This should in most cases not alter self.num_classes, as we only use binary tags (+ tag-continue = 3 classes).\n",
    "        # However, we leave this generic implementation for future tasks.\n",
    "        self.num_classes = len(np.unique(Y)) - 1\n",
    "        X = tokenized_inputs.get(\"input_ids\")\n",
    "        attention_masks = tokenized_inputs.get(\"attention_mask\")\n",
    "        return torch.LongTensor(X), torch.LongTensor(Y), torch.LongTensor(attention_masks)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a Token-level classification Subtask.\"\"\"\n",
    "        return \"Token-level classification\"\n",
    "\n",
    "    def create_class_weights(self):\n",
    "        \"\"\"Compute the weights.\"\"\"\n",
    "        labels = self.Y[Split.TRAIN]\n",
    "        only_class_labels = labels[labels != -100]\n",
    "        self.class_weights = get_class_weights(only_class_labels, method=\"isns\")\n",
    "\n",
    "    def get_scaling_weight(self):\n",
    "        \"\"\"Get the weight of a POS Subtask.\n",
    "\n",
    "        As with the other tasks, we normalize by the natural logarithm of the domain size.\n",
    "        In case of POS subtask, the domain size equals the vocab size.\n",
    "        \"\"\"\n",
    "        return 1 / np.log(self.num_classes)\n",
    "\n",
    "# not used in current implementation, but important for testing?\n",
    "class MLMSubTask(SubTask):\n",
    "    \"\"\"A Masked Language Modelling Subtask.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize a MLMSubTask.\"\"\"\n",
    "        super(MLMSubTask, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def load_data(self) -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"Load the data of a MLMSubTask.\"\"\"\n",
    "        df = pd.read_csv(self.filename)\n",
    "        X = df[self.src_col]\n",
    "        tokenized_inputs = tokenizer(X.to_list(), padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "        X = torch.LongTensor(tokenized_inputs.get(\"input_ids\"))\n",
    "        attention_masks = tokenized_inputs.get(\"attention_mask\")\n",
    "\n",
    "        MASK_TOKEN = tokenizer.mask_token_id\n",
    "        SEP_TOKEN = tokenizer.sep_token_id\n",
    "        CLS_TOKEN = tokenizer.cls_token_id\n",
    "        PAD_TOKEN = tokenizer.pad_token_id\n",
    "\n",
    "        Y = X.clone()\n",
    "        rand = torch.rand(X.shape)\n",
    "        masking_mask = (rand < 0.15) * (X != SEP_TOKEN) * (X != CLS_TOKEN) * (X != PAD_TOKEN)\n",
    "        X[masking_mask] = MASK_TOKEN\n",
    "        Y[~masking_mask] = -100\n",
    "        return torch.LongTensor(X), torch.LongTensor(Y), torch.LongTensor(attention_masks)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a MLM Subtask.\"\"\"\n",
    "        return \"Masked Language Modelling\"\n",
    "\n",
    "    def get_scaling_weight(self):\n",
    "        \"\"\"Get the weights for imbalanced classes.\"\"\"\n",
    "        return 1 / np.log(len(tokenizer))"
   ],
   "id": "2d702ae4dd058b68",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:54.745043Z",
     "start_time": "2024-12-08T21:42:54.737443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"This module contains the SubTaskDataset.\"\"\"\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from media_bias_detection.utils.logger import general_logger\n",
    "from media_bias_detection.utils.enums import Split\n",
    "\n",
    "\n",
    "class SubTaskDataset(Dataset):\n",
    "    \"\"\"A Datset for a single SubTask.\"\"\"\n",
    "\n",
    "    def __init__(self, subtask: SubTask, split: Split):\n",
    "        \"\"\"Initialize a SubTaskDataset.\"\"\"\n",
    "        general_logger.info(f\"Initializing dataset for subtask {subtask.id} with split {split}\")\n",
    "        self.split = split\n",
    "        self.subtask = subtask\n",
    "        self.observations: List = []\n",
    "        self._reset()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the length of the Dataset.\"\"\"\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Get the next observation from the Dataset.\"\"\"\n",
    "        \n",
    "        if self._counter == len(self.observations):\n",
    "            self._reset()\n",
    "        i = self.observations[self._counter]\n",
    "        x = self.subtask.get_X(split=self.split)[i]\n",
    "        masks = self.subtask.get_att_mask(split=self.split)[i]\n",
    "        y = self.subtask.get_Y(split=self.split)[i]\n",
    "        self._counter += 1\n",
    "        return x, masks, y, self.subtask.id\n",
    "\n",
    "    def _reset(self):\n",
    "        general_logger.info(f\"Resetting dataset for subtask {self.subtask.id}\")\n",
    "        self.observations = [i for i in range(len(self.subtask.get_X(split=self.split)))]\n",
    "        set_random_seed()\n",
    "        np.random.shuffle(self.observations)  # Not a real 'reshuffling' as it will always arrange same.\n",
    "        self._counter = 0\n",
    "\n",
    "\n",
    "class BatchList:\n",
    "    \"\"\"A BatchList is a wrapper around dataloaders for each subtask.\n",
    "\n",
    "    This BatchList will never stop; it will always yield super-batches containing one sub-batch per task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, subtask_list: List[SubTask], sub_batch_size, split=Split.TRAIN):\n",
    "        \"\"\"Initialize a BatchList.\"\"\"\n",
    "        general_logger.info(f\"Creating BatchList with {len(subtask_list)} subtasks, batch size {sub_batch_size}\")\n",
    "        self.sub_batch_size = sub_batch_size\n",
    "        self.datasets = {f\"{st.id}\": SubTaskDataset(subtask=st, split=split) for st in subtask_list}\n",
    "        self.dataloaders = {\n",
    "            f\"{st_id}\": DataLoader(ds, batch_size=self.sub_batch_size) for st_id, ds in self.datasets.items()\n",
    "        }\n",
    "        self.iter_dataloaders = {f\"{st_id}\": iter(dl) for st_id, dl in self.dataloaders.items()}\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Yield a batch of sub-batches.\"\"\"\n",
    "        data = []\n",
    "        items = list(self.iter_dataloaders.items())  # List of tuples of (key,values)\n",
    "        random.shuffle(items)\n",
    "        for st_id, dl in items:\n",
    "            try:\n",
    "                batch = next(dl)\n",
    "            except StopIteration:\n",
    "                self.iter_dataloaders[st_id] = iter(self.dataloaders[st_id])  # Reset the iter_dataloader\n",
    "                batch = next(self.iter_dataloaders[st_id])\n",
    "            data.append(batch)\n",
    "        general_logger.info(f\"Generated batch with {len(data)} sub-batches\")\n",
    "        return data  # Batch contains Sub-batches.\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset this BatchListEvalTest.\"\"\"\n",
    "        self.iter_dataloaders = {f\"{st_id}\": iter(dl) for st_id, dl in self.dataloaders.items()}\n",
    "\n",
    "class BatchListEvalTest:\n",
    "    \"\"\"A BatchListEvalTest is a wrapper around dataloaders for each subtask.\n",
    "\n",
    "    If one task is exhausted, it will stop yielding sub-batches from this task.\n",
    "    Instead, it will continue until it has yielded all sub-batches from all tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, subtask_list: List[SubTask], sub_batch_size, split=Split.TRAIN):\n",
    "        \"\"\"Initialize a BatchList.\"\"\"\n",
    "        general_logger.info(f\"Creating BatchListEvalTest with {len(subtask_list)} subtasks\")\n",
    "        self.sub_batch_size = sub_batch_size\n",
    "        self.datasets = {f\"{st.id}\": SubTaskDataset(subtask=st, split=split) for st in subtask_list}\n",
    "        self.dataloaders = {\n",
    "            f\"{st_id}\": DataLoader(ds, batch_size=self.sub_batch_size) for st_id, ds in self.datasets.items()\n",
    "        }\n",
    "        self.iter_dataloaders = {f\"{st_id}\": iter(dl) for st_id, dl in self.dataloaders.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of this BatchListEvalTest.\n",
    "\n",
    "        The length is the maximum length of all subtask-datadloaders.\n",
    "        \"\"\"\n",
    "        return sum([len(dl) for dl in self.dataloaders.values()])\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset this BatchListEvalTest.\"\"\"\n",
    "        self.iter_dataloaders = {f\"{st_id}\": iter(dl) for st_id, dl in self.dataloaders.items()}"
   ],
   "id": "a4d50837491e4497",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:54.763132Z",
     "start_time": "2024-12-08T21:42:54.752127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "# initializing the sub-tasks I want to use\n",
    "st_1_cw_hard_03 = ClassificationSubTask(\n",
    "task_id=3,\n",
    "filename=\"03_CW_HARD/preprocessed.csv\",\n",
    "id=300001)\n",
    "st_1_me_too_ma_108 = MultiLabelClassificationSubTask(\n",
    "num_classes=2,\n",
    "num_labels=2,\n",
    "task_id=108,\n",
    "filename=\"108_MeTooMA/preprocessed.csv\",\n",
    "id=10801,\n",
    "tgt_cols_list=[\"hate_speech_label\", \"sarcasm_label\"],\n",
    ")\n",
    "st_1_mdgender_116 = ClassificationSubTask(\n",
    "task_id=116,\n",
    "id=11601,\n",
    "filename=\"116_MDGender/preprocessed.csv\",\n",
    "num_classes=6\n",
    ")\n",
    "st_1_mpqa_103 = ClassificationSubTask(\n",
    "task_id=103,\n",
    "id=10301,\n",
    "filename=\"103_MPQA/preprocessed.csv\")\n",
    "st_1_stereotype_109 = ClassificationSubTask(\n",
    "task_id=109,\n",
    "id=10901,\n",
    "filename=\"109_stereotype/preprocessed.csv\")\n",
    "st_2_stereotype_109 = MultiLabelClassificationSubTask(\n",
    "task_id=109,\n",
    "id=10902,\n",
    "filename=\"109_stereotype/preprocessed.csv\",\n",
    "tgt_cols_list=[\"stereotype_explicit_label\", \"stereotype_explicit_label\"],\n",
    "num_classes=2,\n",
    "num_labels=2,\n",
    ")\n",
    "st_1_good_news_everyone_42 = POSSubTask(\n",
    "tgt_cols_list=[\"cue_pos\"],\n",
    "task_id=42,\n",
    "id=42001,\n",
    "filename=\"42_GoodNewsEveryone/preprocessed.csv\"\n",
    ")\n",
    "st_2_good_news_everyone_42 = POSSubTask(\n",
    "tgt_cols_list=[\"experiencer_pos\"],\n",
    "task_id=42,\n",
    "id=42002,\n",
    "filename=\"42_GoodNewsEveryone/preprocessed.csv\",\n",
    ")\n",
    "st_1_pheme_12 = ClassificationSubTask(\n",
    "task_id=12,\n",
    "id=12001,\n",
    "filename=\"12_PHEME/preprocessed.csv\")\n",
    "st_2_pheme_12 = ClassificationSubTask(\n",
    "task_id=12,\n",
    "id=12002,\n",
    "filename=\"12_PHEME/preprocessed.csv\",\n",
    "tgt_cols_list=[\"veracity_label\"],\n",
    "num_classes=3,\n",
    ")\n",
    "st_1_babe_10 = ClassificationSubTask(\n",
    "task_id=10,\n",
    "id=10001,\n",
    "filename=\"10_BABE/preprocessed.csv\",\n",
    "num_classes=2)\n",
    "st_2_babe_10 = POSSubTask(\n",
    "task_id=10,\n",
    "id=10002,\n",
    "filename=\"10_BABE/preprocessed.csv\",\n",
    "tgt_cols_list=[\"biased_words\"])\n",
    "st_1_gwsd_128 = ClassificationSubTask(\n",
    "task_id=128,\n",
    "num_classes=3,\n",
    "filename=\"128_GWSD/preprocessed.csv\",\n",
    "id=12801)\n",
    "\n",
    "# Tasks\n",
    "cw_hard_03 = Task(task_id=3, subtasks_list=[st_1_cw_hard_03])\n",
    "babe_10 = Task(task_id=10, subtasks_list=[st_1_babe_10, st_2_babe_10])\n",
    "me_too_ma_108 = Task(task_id=108, subtasks_list=[st_1_me_too_ma_108])\n",
    "mdgender_116 = Task(task_id=116, subtasks_list=[st_1_mdgender_116])\n",
    "pheme_12 = Task(task_id=12, subtasks_list=[st_2_pheme_12, st_1_pheme_12])\n",
    "mpqa_103 = Task(task_id=103, subtasks_list=[st_1_mpqa_103])\n",
    "stereotype_109 = Task(task_id=109, subtasks_list=[st_1_stereotype_109,\n",
    "                                              st_2_stereotype_109])\n",
    "good_news_everyone_42 = Task(task_id=42,\n",
    "                         subtasks_list=[st_1_good_news_everyone_42,\n",
    "                                        st_2_good_news_everyone_42])\n",
    "gwsd_128 = Task(task_id=128, subtasks_list=[st_1_gwsd_128])\n",
    "\n",
    "\n",
    "# MBIB ###\n",
    "# st_linguistic = ClassificationSubTask(task_id=11111, id=11111, filename=\"mbib_linguistic/preprocessed.csv\", num_classes=2)\n",
    "# mbib_lingustic = Task(task_id=11111, subtasks_list=[st_linguistic])\n",
    "\n",
    "# Create task object\n",
    "all_tasks = [\n",
    "babe_10,\n",
    "cw_hard_03,\n",
    "me_too_ma_108,\n",
    "pheme_12,\n",
    "mdgender_116,\n",
    "mpqa_103,\n",
    "stereotype_109,\n",
    "good_news_everyone_42,\n",
    "gwsd_128,\n",
    "]\n",
    "\n",
    "# Get all subtasks\n",
    "all_subtasks = list(itertools.chain.from_iterable(t.subtasks_list for t in all_tasks))\n",
    "\n",
    "# Task families\n",
    "media_bias = [babe_10]\n",
    "subjective_bias = [cw_hard_03]\n",
    "hate_speech = [me_too_ma_108]\n",
    "gender_bias = [mdgender_116]\n",
    "sentiment_analysis = [mpqa_103]\n",
    "fake_news = [pheme_12]\n",
    "group_bias = [stereotype_109]\n",
    "emotionality = [good_news_everyone_42]\n",
    "stance_detection = [gwsd_128]\n",
    "#mlm = [mlm_0]"
   ],
   "id": "6b35e68946eadf4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-08 22:42:54,755: INFO: 2101821432: Initializing SubTask 300001 for task 3]\n",
      "[2024-12-08 22:42:54,756: INFO: 2101821432: Initializing SubTask 10801 for task 108]\n",
      "[2024-12-08 22:42:54,757: INFO: 2101821432: Initializing SubTask 11601 for task 116]\n",
      "[2024-12-08 22:42:54,758: INFO: 2101821432: Initializing SubTask 10301 for task 103]\n",
      "[2024-12-08 22:42:54,758: INFO: 2101821432: Initializing SubTask 10901 for task 109]\n",
      "[2024-12-08 22:42:54,759: INFO: 2101821432: Initializing SubTask 10902 for task 109]\n",
      "[2024-12-08 22:42:54,759: INFO: 2101821432: Initializing SubTask 42001 for task 42]\n",
      "[2024-12-08 22:42:54,760: INFO: 2101821432: Initializing SubTask 42002 for task 42]\n",
      "[2024-12-08 22:42:54,760: INFO: 2101821432: Initializing SubTask 12001 for task 12]\n",
      "[2024-12-08 22:42:54,760: INFO: 2101821432: Initializing SubTask 12002 for task 12]\n",
      "[2024-12-08 22:42:54,761: INFO: 2101821432: Initializing SubTask 10001 for task 10]\n",
      "[2024-12-08 22:42:54,761: INFO: 2101821432: Initializing SubTask 10002 for task 10]\n",
      "[2024-12-08 22:42:54,761: INFO: 2101821432: Initializing SubTask 12801 for task 128]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Building the Model\n",
    "After initializing the datasets I want to use in the last step, I now build the model:\n",
    "- The backbone is changes to DistilBERT as it is a smaller model and therefore faster to train.\n",
    "- For each task a specific model head is needed to fulfill the task. For this a head factory is used to decide which head to use for the specific task type.\n",
    "- Apart from that, the model needs a GradsWrapper to get and set the gradients of the weights and biases of all trainable layers.\n",
    "- In the model factory the model is then instantiated by combining the backbone with the different model heads for the different tasks."
   ],
   "id": "a4e6c80fbf45a85d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:54.794805Z",
     "start_time": "2024-12-08T21:42:54.791666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict\n",
    "from torch import nn\n",
    "from media_bias_detection.utils.common import rsetattr\n",
    "\n",
    "class GradsWrapper(nn.Module):\n",
    "    \"\"\"Abstract class for a GradsWrapper.\n",
    "\n",
    "    This class must be extended and not instantiated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Raise RuntimeError if this class is instantiated.\"\"\"\n",
    "        if type(self) == GradsWrapper:\n",
    "            raise RuntimeError(\"Abstract class <GradsWrapper> must not be instantiated.\")\n",
    "        super(GradsWrapper, self).__init__()\n",
    "\n",
    "    def get_grads(self) -> Dict:\n",
    "        \"\"\"Get the gradients of the weights and biases of all trainable layers.\"\"\"\n",
    "        return {k: v.grad.clone() if v.grad is not None else None for k, v in dict(self.named_parameters()).items()}\n",
    "\n",
    "    def set_grads(self, grads: Dict):\n",
    "        \"\"\"Set the gradients of the weights and biases of all trainable layers.\"\"\"\n",
    "        for k, v in grads.items():\n",
    "            rsetattr(self, f\"{k}.grad\", v)"
   ],
   "id": "de475ddd25fedd07",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:54.822310Z",
     "start_time": "2024-12-08T21:42:54.818382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BackboneLM(GradsWrapper):\n",
    "    \"\"\"Language encoder model which is shared across all tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Fetch Language model from huggingface.\"\"\"\n",
    "        super(BackboneLM, self).__init__()\n",
    "        from transformers import DistilBertModel\n",
    "        self.backbone = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"Torch-based module.\"\"\"\n",
    "    \n",
    "    def __init__(self, stl: List, *args, **kwargs):\n",
    "        \"\"\"Initialize model and create heads.\"\"\"\n",
    "        super().__init__()\n",
    "        self.stl = stl\n",
    "        self.subtask_id_to_subtask = {int(f\"{st.id}\"): st for st in stl}\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        \n",
    "        # Initialize the backbone language model\n",
    "        self.language_model = BackboneLM()\n",
    "        self.language_model.backbone.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        # Create the task-specific heads\n",
    "        self.heads = nn.ModuleDict({str(st.id): HeadFactory(st, *args, **kwargs) for st in stl})\n",
    "\n",
    "    def forward(self, X, attention_masks, Y, st_id):\n",
    "        \"\"\"Pass the data through the model and according head decided from heads dict.\"\"\"\n",
    "        # Pass through the backbone model\n",
    "        x_enc = self.language_model.backbone(input_ids=X, attention_mask=attention_masks).last_hidden_state\n",
    "        # Pass through the appropriate head\n",
    "        head = self.heads[str(st_id.item())]\n",
    "        logits, loss, metric_values = head(x_enc, Y)\n",
    "        return loss, metric_values"
   ],
   "id": "21c1ea37f8afd8d2",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:42:54.833584Z",
     "start_time": "2024-12-08T21:42:54.829880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Module for creating instantiating the appropriate model defined by the task list only.\"\"\"\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def ModelFactory(\n",
    "    task_list: List, \n",
    "    sub_batch_size: int, \n",
    "    eval_batch_size: int, \n",
    "    pretrained_path: str = None,\n",
    "    *args, \n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Create model and return it along with dataloaders.\"\"\"\n",
    "    # Get all subtasks from task list\n",
    "    subtask_list = [st for t in task_list for st in t.subtasks_list]\n",
    "    \n",
    "    # Verify data is processed\n",
    "    for st in subtask_list:\n",
    "        assert st.processed, \"Data must be loaded at this point.\"\n",
    "\n",
    "    # Create model\n",
    "    model = Model(stl=subtask_list, **kwargs)\n",
    "\n",
    "    if pretrained_path is not None:\n",
    "        model = load_pretrained_weights(model, pretrained_path=pretrained_path)\n",
    "\n",
    "    # Move model to appropriate device\n",
    "    model.to(model.device)\n",
    "\n",
    "    # Create dataloaders\n",
    "    batch_list_train = BatchList(\n",
    "        subtask_list=subtask_list, \n",
    "        sub_batch_size=sub_batch_size, \n",
    "        split=Split.TRAIN\n",
    "    )\n",
    "    \n",
    "    batch_list_dev = BatchList(\n",
    "        subtask_list=subtask_list, \n",
    "        sub_batch_size=eval_batch_size, \n",
    "        split=Split.DEV\n",
    "    )\n",
    "    \n",
    "    batch_list_eval = BatchListEvalTest(\n",
    "        subtask_list=subtask_list, \n",
    "        sub_batch_size=sub_batch_size, \n",
    "        split=Split.DEV\n",
    "    )\n",
    "    \n",
    "    batch_list_test = BatchListEvalTest(\n",
    "        subtask_list=subtask_list, \n",
    "        sub_batch_size=sub_batch_size, \n",
    "        split=Split.TEST\n",
    "    )\n",
    "\n",
    "    return model, batch_list_train, batch_list_dev, batch_list_eval, batch_list_test\n",
    "\n",
    "\n",
    "def save_head_initializations(model):\n",
    "    \"\"\"Save weight initialization of the head. This method will not be called anymore.\n",
    "     It's only for the initial saving of weight inits for all tasks.\"\"\"\n",
    "    for head_name in model.heads.keys():\n",
    "        torch.save(model.heads[head_name].state_dict(), 'model_files/heads/' + head_name + '_init.pth')\n",
    "    \n",
    "def load_head_initializations(model):\n",
    "    \"\"\"Load fixed weight initialization for each head in order to ensure reproducibility.\"\"\"\n",
    "    for head_name in model.heads.keys():\n",
    "        weights_path = 'model_files/heads/' + head_name + '_init.pth'\n",
    "        head_weights = torch.load(weights_path)\n",
    "        model.heads[head_name].load_state_dict(head_weights,strict=True)\n",
    "\n",
    "def load_pretrained_weights(model, pretrained_path):\n",
    "    \"\"\"Load the weights of a pretrained model.\"\"\"\n",
    "    weight_dict = torch.load(pretrained_path)\n",
    "    model.load_state_dict(weight_dict, strict=False)\n",
    "    return model\n"
   ],
   "id": "5dc74770576b2888",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T22:03:38.220674Z",
     "start_time": "2024-12-08T22:03:38.201789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Model heads implementation for MTL model.\n",
    "\n",
    "This module contains all task-specific heads and the factory for creating them.\n",
    "Each head implements specific logic for different types of tasks while maintaining\n",
    "consistent interfaces for the MTL architecture.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Tuple, Optional, Union\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy, F1Score, MeanSquaredError, Perplexity, R2Score\n",
    "\n",
    "\n",
    "from media_bias_detection.utils.logger import general_logger\n",
    "from media_bias_detection.utils.common import get_class_weights\n",
    "\n",
    "\n",
    "class HeadError(Exception):\n",
    "    \"\"\"Custom exception for head-related errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def HeadFactory(st: SubTask, *args, **kwargs) -> 'BaseHead':\n",
    "    \"\"\"Create appropriate head based on subtask type.\n",
    "\n",
    "    Args:\n",
    "        st: Subtask to create head for\n",
    "        *args, **kwargs: Additional arguments for head initialization\n",
    "\n",
    "    Returns:\n",
    "        Initialized head instance\n",
    "\n",
    "    Raises:\n",
    "        HeadError: If head creation fails or subtask type is unsupported\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(st, ClassificationSubTask):\n",
    "            return ClassificationHead(\n",
    "                num_classes=st.num_classes,\n",
    "                class_weights=st.class_weights,\n",
    "                *args,\n",
    "                **kwargs\n",
    "            )\n",
    "        elif isinstance(st, MultiLabelClassificationSubTask):\n",
    "            return MultiLabelClassificationHead(\n",
    "                num_classes=st.num_classes,\n",
    "                num_labels=st.num_labels,\n",
    "                class_weights=st.class_weights,\n",
    "                *args,\n",
    "                **kwargs\n",
    "            )\n",
    "        elif isinstance(st, POSSubTask):\n",
    "            return TokenClassificationHead(\n",
    "                num_classes=st.num_classes,\n",
    "                class_weights=st.class_weights,\n",
    "                *args,\n",
    "                **kwargs\n",
    "            )\n",
    "        elif isinstance(st, RegressionSubTask):\n",
    "            return RegressionHead(*args, **kwargs)\n",
    "        elif isinstance(st, MLMSubTask):\n",
    "            return LanguageModellingHead(*args, **kwargs)\n",
    "        else:\n",
    "            raise HeadError(f\"Unsupported subtask type: {type(st)}\")\n",
    "    except Exception as e:\n",
    "        raise HeadError(f\"Head creation failed: {str(e)}\")\n",
    "\n",
    "\n",
    "class BaseHead(GradsWrapper):\n",
    "    \"\"\"Base class for all model heads.\n",
    "\n",
    "    Attributes:\n",
    "        metrics: Dictionary of metric names to metric instances\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics: Dict = {}\n",
    "\n",
    "    def forward(self, X: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
    "        \"\"\"Forward pass through the head.\n",
    "\n",
    "        Args:\n",
    "            X: Input features (batch_size, seq_len, hidden_dim)\n",
    "            y: Target labels\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (logits, loss, metric_values)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ClassificationHead(BaseHead):\n",
    "    \"\"\"Classification head for binary and multi-class tasks.\n",
    "\n",
    "    Attributes:\n",
    "        dense: Dense layer for feature transformation\n",
    "        dropout: Dropout layer for regularization\n",
    "        out_proj: Output projection layer\n",
    "        num_classes: Number of classes\n",
    "        num_labels: Number of labels (1 for standard classification)\n",
    "        loss: Loss function\n",
    "        metrics: Dictionary of metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dimension: int,\n",
    "            hidden_dimension: int,\n",
    "            dropout_prob: float,\n",
    "            num_classes: int = 2,\n",
    "            num_labels: int = 1,\n",
    "            class_weights: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(input_dimension, hidden_dimension)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.out_proj = nn.Linear(hidden_dimension, num_classes * num_labels)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_labels = num_labels\n",
    "        self.loss = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        self.metrics = {\n",
    "            \"f1\": F1Score(\n",
    "                num_classes=num_classes,\n",
    "                task=\"binary\" if num_classes == 2 else \"multiclass\",\n",
    "                average=\"macro\"\n",
    "            ),\n",
    "            \"acc\": Accuracy(task=\"binary\" if num_classes == 2 else \"multiclass\",\n",
    "        num_classes=num_classes),\n",
    "        }\n",
    "\n",
    "        general_logger.info(\n",
    "            f\"Initialized ClassificationHead with {num_classes} classes\"\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
    "        try:\n",
    "            batch_size = y.shape[0]\n",
    "\n",
    "            # Get CLS token representation\n",
    "            x = X[:, 0, :]\n",
    "\n",
    "            # Pass through layers\n",
    "            x = self.dropout(x)\n",
    "            x = self.dense(x)\n",
    "            x = torch.tanh(x)\n",
    "            x = self.dropout(x)\n",
    "            logits = self.out_proj(x)\n",
    "            \n",
    "            print(\"logits\", logits)\n",
    "            print(\"y\", y)\n",
    "            \n",
    "            logits = logits.squeeze(-1)  # Remove the extra dimension, making logits shape (32, 2)\n",
    "            print(\"logits_after reshaping\", logits.shape)  # Should print torch.Size([32, 2])\n",
    "\n",
    "            # Compute loss and metrics\n",
    "            loss = self.loss(logits.view(-1, self.num_classes), y.view(-1))\n",
    "            logits = logits.view(batch_size, self.num_classes, self.num_labels)\n",
    "            print(\"logits_after reshaping\", logits)\n",
    "\n",
    "            metrics = {\n",
    "                name: metric(logits.cpu(), y.cpu())\n",
    "                for name, metric in self.metrics.items()\n",
    "            }\n",
    "\n",
    "            return logits, loss, metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HeadError(f\"Classification forward pass failed: {str(e)}\")\n",
    "\n",
    "\n",
    "class TokenClassificationHead(BaseHead):\n",
    "    \"\"\"Head for token-level classification tasks.\n",
    "\n",
    "    Attributes:\n",
    "        dropout: Dropout layer\n",
    "        classifier: Classification layer\n",
    "        num_classes: Number of classes\n",
    "        loss: Loss function\n",
    "        metrics: Dictionary of metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes: int,\n",
    "            class_weights: Optional[torch.Tensor],\n",
    "            hidden_dimension: int,\n",
    "            dropout_prob: float,\n",
    "            *args,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_dimension, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        self.loss = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        self.metrics = {\n",
    "            \"f1\": F1Score(\n",
    "                num_classes=num_classes,\n",
    "                task=\"multiclass\",\n",
    "                average=\"macro\"\n",
    "            ),\n",
    "            \"acc\": Accuracy(\n",
    "                task=\"multiclass\",\n",
    "                num_classes=num_classes\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        general_logger.info(\n",
    "            f\"Initialized TokenClassificationHead with {num_classes} classes\"\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
    "        try:\n",
    "            # Process sequence\n",
    "            sequence_output = self.dropout(X)\n",
    "            logits = self.classifier(sequence_output)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.loss(logits.view(-1, self.num_classes), y.view(-1))\n",
    "\n",
    "            # Mask padding tokens for metrics\n",
    "            mask = torch.where(y != -100, 1, 0)\n",
    "            logits = torch.masked_select(\n",
    "                logits,\n",
    "                mask.unsqueeze(-1).expand(logits.size()) == 1\n",
    "            )\n",
    "            y = torch.masked_select(y, mask == 1)\n",
    "            logits = logits.view(y.shape[0], self.num_classes)\n",
    "\n",
    "            metrics = {\n",
    "                name: metric(logits.cpu(), y.cpu())\n",
    "                for name, metric in self.metrics.items()\n",
    "            }\n",
    "\n",
    "            return logits, loss, metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HeadError(f\"Token classification forward pass failed: {str(e)}\")\n",
    "\n",
    "\n",
    "class MultiLabelClassificationHead(BaseHead):\n",
    "    \"\"\"Head for multi-label classification tasks.\n",
    "\n",
    "    Attributes:\n",
    "        dense: Dense layer\n",
    "        dropout: Dropout layer\n",
    "        out_proj: Output projection layer\n",
    "        num_classes: Number of classes per label\n",
    "        num_labels: Number of labels to predict\n",
    "        loss: Loss function\n",
    "        metrics: Dictionary of metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dimension: int,\n",
    "            hidden_dimension: int,\n",
    "            dropout_prob: float,\n",
    "            num_classes: int = 2,\n",
    "            num_labels: int = 2,\n",
    "            class_weights: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(input_dimension, hidden_dimension)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.out_proj = nn.Linear(hidden_dimension, num_classes * num_labels)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # Use BCEWithLogitsLoss for multi-label\n",
    "        self.loss = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "        self.metrics = {\n",
    "            \"f1\": F1Score(\n",
    "                num_classes=num_classes * num_labels,\n",
    "                task=\"multilabel\",\n",
    "                average=\"macro\"\n",
    "            ),\n",
    "            \"acc\": Accuracy(\n",
    "                task=\"multilabel\",\n",
    "                num_classes=num_classes * num_labels\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        general_logger.info(\n",
    "            f\"Initialized MultiLabelClassificationHead with {num_classes} \"\n",
    "            f\"classes and {num_labels} labels\"\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
    "        try:\n",
    "            batch_size = y.shape[0]\n",
    "\n",
    "            # Get CLS token\n",
    "            x = X[:, 0, :]\n",
    "\n",
    "            # Pass through layers\n",
    "            x = self.dropout(x)\n",
    "            x = self.dense(x)\n",
    "            x = torch.tanh(x)\n",
    "            x = self.dropout(x)\n",
    "            logits = self.out_proj(x)\n",
    "\n",
    "            # Reshape for multi-label\n",
    "            logits = logits.view(batch_size, self.num_labels, self.num_classes)\n",
    "            y = y.view(batch_size, -1)\n",
    "\n",
    "            loss = self.loss(logits, y.float())\n",
    "\n",
    "            metrics = {\n",
    "                name: metric(logits.cpu(), y.cpu())\n",
    "                for name, metric in self.metrics.items()\n",
    "            }\n",
    "\n",
    "            return logits, loss, metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HeadError(f\"Multi-label classification forward pass failed: {str(e)}\")\n",
    "\n",
    "\n",
    "class RegressionHead(BaseHead):\n",
    "    \"\"\"Head for regression tasks.\n",
    "\n",
    "    Attributes:\n",
    "        dense: Dense layer\n",
    "        dropout: Dropout layer\n",
    "        out_proj: Output projection layer\n",
    "        loss: Loss function\n",
    "        metrics: Dictionary of metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dimension: int,\n",
    "            hidden_dimension: int,\n",
    "            dropout_prob: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(input_dimension, hidden_dimension)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.out_proj = nn.Linear(hidden_dimension, 1)\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.metrics = {\n",
    "            \"R2\": R2Score(),\n",
    "            \"MSE\": MeanSquaredError()\n",
    "        }\n",
    "\n",
    "        general_logger.info(\"Initialized RegressionHead\")\n",
    "\n",
    "    def forward(self, X: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
    "        try:\n",
    "            # Get CLS token\n",
    "            x = X[:, 0, :]\n",
    "\n",
    "            # Pass through layers\n",
    "            x = self.dropout(x)\n",
    "            x = self.dense(x)\n",
    "            x = torch.tanh(x)\n",
    "            x = self.dropout(x)\n",
    "            logits = self.out_proj(x)\n",
    "\n",
    "            loss = self.loss(logits.squeeze(), y.squeeze())\n",
    "\n",
    "            metrics = {\n",
    "                name: metric(logits.cpu(), y.cpu()).detach()\n",
    "                for name, metric in self.metrics.items()\n",
    "            }\n",
    "\n",
    "            return logits, loss, metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HeadError(f\"Regression forward pass failed: {str(e)}\")\n",
    "\n",
    "\n",
    "class LanguageModellingHead(BaseHead):\n",
    "    \"\"\"Head for masked language modeling tasks.\n",
    "\n",
    "    Attributes:\n",
    "        dense: Dense layer\n",
    "        layer_norm: Layer normalization\n",
    "        decoder: Output decoder\n",
    "        loss: Loss function\n",
    "        metrics: Dictionary of metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dimension: int,\n",
    "            hidden_dimension: int,\n",
    "            dropout_prob: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(input_dimension, hidden_dimension)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dimension, eps=1e-5)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        self.decoder = nn.Linear(hidden_dimension, tokenizer.vocab_size)\n",
    "        self.bias = nn.Parameter(torch.zeros(tokenizer.vocab_size))\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.metrics = {\"perplexity\": Perplexity()}\n",
    "\n",
    "        general_logger.info(\"Initialized LanguageModellingHead\")\n",
    "\n",
    "    def forward(self, X: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
    "        try:\n",
    "            x = self.dense(X)\n",
    "            x = self.gelu(x)\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            logits = self.decoder(x)\n",
    "            loss = self.loss(\n",
    "                logits.view(-1, tokenizer.vocab_size),\n",
    "                y.view(-1)\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                name: metric(logits.cpu(), y.cpu())\n",
    "                for name, metric in self.metrics.items()\n",
    "            }\n",
    "\n",
    "            return logits, loss, metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HeadError(f\"Language modeling forward pass failed: {str(e)}\")"
   ],
   "id": "b9ddb6cdf35bbf95",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training the Model\n",
    "\n",
    "For the model training the MAGPIE repository first introduces some helper functions. Since they are specific to the training, I include them into the notebook, instead of using them as a separate module like the other utility functions."
   ],
   "id": "58fc758d8464619"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:45:20.861570Z",
     "start_time": "2024-12-08T21:45:20.845608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\"\"\"This module contains helper classes for model training.\"\"\"\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import math\n",
    "from enum import Enum\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from media_bias_detection.utils.enums import Split\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"Logger to keep track of metrics, losses and artifacts.\n",
    "\n",
    "    This logger is used as an abstraction. If we want to integrate with third party providers (wandb, GCS, ...),\n",
    "    use this logger.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, experiment_name: str):\n",
    "        \"\"\"Initialize a Logger.\"\"\"\n",
    "        PATH = \"logging/\" + experiment_name\n",
    "        os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "        self.experiment_logfilename = PATH + \"/train_data.log\"\n",
    "        experiment_logfile_handler = logging.FileHandler(filename=self.experiment_logfilename)\n",
    "        experiment_logfile_formatter = logging.Formatter(fmt=\"%(message)s\")\n",
    "        experiment_logfile_handler.setFormatter(experiment_logfile_formatter)\n",
    "\n",
    "        self.experiment_logger = logging.getLogger(\"experiment_logger\")\n",
    "        self.experiment_logger.addHandler(experiment_logfile_handler)\n",
    "        self.experiment_logger.setLevel(\"INFO\")\n",
    "\n",
    "    def log(self, out):\n",
    "        \"\"\"Log.\"\"\"\n",
    "        self.experiment_logger.info(out)\n",
    "        wandb.log(out)\n",
    "\n",
    "\n",
    "class EarlyStopperSingle:\n",
    "    \"\"\"\n",
    "    EarlyStopper for a single branch of the model.\n",
    "\n",
    "    Inspired by .https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int, min_delta: int, resurrection: bool):\n",
    "        \"\"\"Initialize an EarlyStopperSingle.\"\"\"\n",
    "        self.patience = patience\n",
    "        self.patience_zombie = 10\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.counter_zombie = 0\n",
    "        self.min_dev_loss = np.inf\n",
    "        self.min_dev_loss_zombie = np.inf\n",
    "        self.resurrection = resurrection\n",
    "\n",
    "    def early_stop(self, dev_loss):\n",
    "        \"\"\"Return True if dev_loss is steadily increasing.\"\"\"\n",
    "        if math.isnan(dev_loss):\n",
    "            return False\n",
    "        if dev_loss < self.min_dev_loss:\n",
    "            self.min_dev_loss = dev_loss\n",
    "            self.counter = 0\n",
    "        elif dev_loss > (self.min_dev_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def resurrect(self, dev_loss):\n",
    "        \"\"\"Return True if dev_loss is steadily increasing and a dead task should resurrect.\"\"\"\n",
    "        if math.isnan(dev_loss) or not self.resurrection:\n",
    "            return False\n",
    "        if dev_loss < self.min_dev_loss_zombie:\n",
    "            self.min_dev_loss_zombie = dev_loss\n",
    "            self.counter_zombie = 0\n",
    "        elif dev_loss > self.min_dev_loss_zombie:\n",
    "            self.counter_zombie += 1\n",
    "            if self.counter_zombie >= self.patience_zombie:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def reset_early_stopper(self):\n",
    "        \"\"\"Reset the state of an early stopper.\n",
    "\n",
    "        As zombies can resurrect and die multiple times, we have to reset their internal variables,\n",
    "        counter and the min_dev_loss each time a zombie resurrects or dies.\n",
    "        \"\"\"\n",
    "        self.counter_zombie = 0\n",
    "        self.counter = 0\n",
    "        self.min_dev_loss_zombie = np.inf\n",
    "        self.min_dev_loss = np.inf\n",
    "\n",
    "\n",
    "class EarlyStoppingMode(Enum):\n",
    "    \"\"\"Enum for early stopping mode.\"\"\"\n",
    "\n",
    "    HEADS = \"heads\"  # Only stop heads\n",
    "    BACKBONE = \"backbone\"  # Also stop backbone\n",
    "    NONE = \"none\"\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    \"\"\"EarlyStopper container for all heads.\"\"\"\n",
    "\n",
    "    def __init__(self, st_ids: List[str], mode: EarlyStoppingMode, patience, resurrection: bool, min_delta=0):\n",
    "        \"\"\"Initialize an EarlyStopper.\"\"\"\n",
    "        self.mode = mode\n",
    "        self.early_stoppers = {\n",
    "            st_id: EarlyStopperSingle(patience=patience[st_id], min_delta=min_delta, resurrection=resurrection)\n",
    "            for st_id in st_ids\n",
    "        }\n",
    "\n",
    "    def early_stop(self, st_id, dev_loss):\n",
    "        \"\"\"Return True if dev_loss is steadily increasing.\"\"\"\n",
    "        return (\n",
    "            False if self.mode == EarlyStoppingMode.NONE else self.early_stoppers[st_id].early_stop(dev_loss=dev_loss)\n",
    "        )\n",
    "\n",
    "    def resurrect(self, st_id, dev_loss):\n",
    "        \"\"\"Return True if dev_loss is steadily increasing and a dead task should resurrect.\"\"\"\n",
    "        return (\n",
    "            False if self.mode == EarlyStoppingMode.NONE else self.early_stoppers[st_id].resurrect(dev_loss=dev_loss)\n",
    "        )\n",
    "\n",
    "    def reset_early_stopper(self, st_id):\n",
    "        \"\"\"Reset the state of an early stopper.\"\"\"\n",
    "        self.early_stoppers[st_id].reset_early_stopper()\n",
    "\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"Abstract Accumulator.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Raise RuntimeError if this Accumulator is instantiated.\"\"\"\n",
    "        if type(self) == Accumulator:\n",
    "            raise RuntimeError(\"Abstract class <Accumulator> must not be instantiated.\")\n",
    "        self.gradients = None\n",
    "        self.n = 0\n",
    "\n",
    "    def update(self, gradients):\n",
    "        \"\"\"Update the values of a gradient.\n",
    "\n",
    "        Must be overwritten by concrete implementation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_avg_gradients(self):\n",
    "        \"\"\"Return the gradients, normalized across 0-axis.\"\"\"\n",
    "        out_gradients = copy.deepcopy(self.gradients)\n",
    "        for k, v in self.gradients.items():\n",
    "            out_gradients[k] /= self.n\n",
    "            out_gradients[k] = out_gradients[k].squeeze(dim=0)\n",
    "        return out_gradients\n",
    "\n",
    "    def get_gradients(self):\n",
    "        \"\"\"Return the gradients.\n",
    "\n",
    "        Must be overwritten by concrete implementation.\n",
    "        \"\"\"\n",
    "        return self.gradients\n",
    "\n",
    "\n",
    "class StackedAccumulator(Accumulator):\n",
    "    \"\"\"Accumulate the gradients for one SubTask within on Super-Batch.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize a StackedAccumulator.\"\"\"\n",
    "        super(StackedAccumulator, self).__init__()\n",
    "\n",
    "    def update(self, gradients, weight=1.0):\n",
    "        \"\"\"Update. Concatenate new set of gradients along 0-axis.\"\"\"\n",
    "        if not self.gradients:\n",
    "            self.gradients = gradients\n",
    "            # unsqueeze all gradients for later concatenation\n",
    "            for k, v in self.gradients.items():\n",
    "                self.gradients[k] = self.gradients[k].unsqueeze(dim=0) * weight\n",
    "        else:\n",
    "            for k, v in self.gradients.items():\n",
    "                new_value = gradients[k].unsqueeze(dim=0) * weight\n",
    "                self.gradients[k] = torch.cat((v, new_value), dim=0)\n",
    "        self.n += 1\n",
    "\n",
    "    def set_gradients(self, gradients: Dict[str, torch.tensor]):\n",
    "        \"\"\"Set the gradients.\"\"\"\n",
    "        for k, v in self.gradients.items():\n",
    "            self.gradients[k] = gradients[k].unsqueeze(dim=0)\n",
    "\n",
    "\n",
    "class RunningSumAccumulator(Accumulator):\n",
    "    \"\"\"Keep track of the running sum of gradients.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize a RunningSumAccumulator.\"\"\"\n",
    "        super(RunningSumAccumulator, self).__init__()\n",
    "\n",
    "    def update(self, gradients: Dict[str, torch.tensor], weight=1.0) -> None:\n",
    "        \"\"\"Update. Sum the gradients along 0-axis.\"\"\"\n",
    "        if not self.gradients:\n",
    "            self.gradients = gradients\n",
    "            # unsqueeze all gradients for later concatenation\n",
    "            for k, v in self.gradients.items():\n",
    "                self.gradients[k] = self.gradients[k].unsqueeze(dim=0) * weight\n",
    "        else:\n",
    "            for k, v in self.gradients.items():\n",
    "                new_value = gradients[k].unsqueeze(dim=0) * weight\n",
    "                self.gradients[k] = torch.add(v, new_value)\n",
    "        self.n += 1\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"The AverageMeter keeps track of a metric.\"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"Initialize an AverageMeter.\"\"\"\n",
    "        self.values = []\n",
    "        self.name = name\n",
    "\n",
    "    def mean_last_k(self, k=10):\n",
    "        \"\"\"Return the mean of the last k values.\"\"\"\n",
    "        assert 1 <= k\n",
    "        vals = self.values[-k:]\n",
    "        if len(vals) < k:\n",
    "            return float(\"NaN\")\n",
    "\n",
    "        return np.mean(vals)\n",
    "\n",
    "    def mean_all(self):\n",
    "        \"\"\"Return the mean of all values.\"\"\"\n",
    "        return np.mean(self.values)\n",
    "\n",
    "    def update(self, value=0):\n",
    "        \"\"\"Update the Metric by appending a new value.\"\"\"\n",
    "        self.values.append(value)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset AverageMeter.\"\"\"\n",
    "        self.values.clear()\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Print.\"\"\"\n",
    "        return f\"{self.mean_last_k(1):.2f}\"\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "    \"\"\"Keep track of all metrics and losses of an epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, heads, logger: Logger):\n",
    "        \"\"\"Initialize a Tracker.\"\"\"\n",
    "        self.metrics = self.init_metrics(heads=heads)\n",
    "        self.losses, self.combined_losses = self.init_losses(heads=heads)\n",
    "        self.logger = logger\n",
    "\n",
    "    def init_losses(self, heads):\n",
    "        \"\"\"Initialize the losses.\"\"\"\n",
    "        train_losses = {f\"{st_id}\": AverageMeter(name=f\"{st_id}_train_loss\") for st_id, head in heads.items()}\n",
    "        dev_losses = {f\"{st_id}\": AverageMeter(name=f\"{st_id}_dev_loss\") for st_id, head in heads.items()}\n",
    "        eval_losses = {f\"{st_id}\": AverageMeter(name=f\"{st_id}_eval_loss\") for st_id, head in heads.items()}\n",
    "        test_losses = {f\"{st_id}\": AverageMeter(name=f\"{st_id}_test_loss\") for st_id, head in heads.items()}\n",
    "        combined_losses = {\n",
    "            Split.TRAIN: AverageMeter(name=\"combined_train_loss\"),\n",
    "            Split.DEV: AverageMeter(name=\"combined_dev_loss\"),\n",
    "            Split.TEST: AverageMeter(name=\"combined_test_loss\"),\n",
    "            Split.EVAL: AverageMeter(name=\"combined_eval_loss\"),\n",
    "        }\n",
    "        return {\n",
    "            Split.TRAIN: train_losses,\n",
    "            Split.DEV: dev_losses,\n",
    "            Split.TEST: test_losses,\n",
    "            Split.EVAL: eval_losses,\n",
    "        }, combined_losses\n",
    "\n",
    "    def init_metrics(self, heads=Dict):\n",
    "        \"\"\"Initialize the AverageMeters for the metrics.\"\"\"\n",
    "        train_metrics = {\n",
    "            st_id: {m: AverageMeter(name=f\"{st_id}_train_{m}\") for m in head.metrics.keys()}\n",
    "            for st_id, head in heads.items()\n",
    "        }\n",
    "\n",
    "        dev_metrics = {\n",
    "            st_id: {m: AverageMeter(name=f\"{st_id}_dev_{m}\") for m in head.metrics.keys()}\n",
    "            for st_id, head in heads.items()\n",
    "        }\n",
    "\n",
    "        eval_metrics = {\n",
    "            st_id: {m: AverageMeter(name=f\"{st_id}_eval_{m}\") for m in head.metrics.keys()}\n",
    "            for st_id, head in heads.items()\n",
    "        }\n",
    "\n",
    "        test_metrics = {\n",
    "            st_id: {m: AverageMeter(name=f\"{st_id}_test_{m}\") for m in head.metrics.keys()}\n",
    "            for st_id, head in heads.items()\n",
    "        }\n",
    "        return {Split.TRAIN: train_metrics, Split.DEV: dev_metrics, Split.TEST: test_metrics, Split.EVAL: eval_metrics}\n",
    "\n",
    "    def update_metric(self, split, st_id, metric, value):\n",
    "        \"\"\"Update the metric, given a split and subtask id.\"\"\"\n",
    "        self.metrics[split][st_id][metric].update(value=value)\n",
    "\n",
    "    def update_loss(self, split, st_id, value):\n",
    "        \"\"\"Update the loss, given a split and subtask id.\"\"\"\n",
    "        self.losses[split][st_id].update(value)\n",
    "\n",
    "    def update_combined_loss(self, split, value):\n",
    "        \"\"\"Update the combined losses, given a split.\"\"\"\n",
    "        self.combined_losses[split].update(value)\n",
    "\n",
    "    def get_last_st_loss(self, split, st_id, k):\n",
    "        \"\"\"Get mean of last subtask loss.\"\"\"\n",
    "        return self.losses[split][st_id].mean_last_k(k=k)\n",
    "\n",
    "    def get_last_st_metric(self, split, st_id, k):\n",
    "        \"\"\"Get mean of last subtask metric.\"\"\"\n",
    "        return self.metrics[split][st_id][next(iter(self.metrics[split][st_id]))].mean_last_k(k=k)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a Tracker.\"\"\"\n",
    "        return f\"TRAIN LOSS: {self.combined_losses[Split.TRAIN]} - DEV LOSS: {self.combined_losses[Split.DEV]} - EVAL LOSS: {self.combined_losses[Split.EVAL]}\"\n",
    "\n",
    "    def log(self, splits: List[Split], additional_payload: Dict[str, float] = {}):\n",
    "        \"\"\"Log the metrics & losses of a list of splits.\"\"\"\n",
    "        out: Dict[str, float] = {**additional_payload}\n",
    "        for split in splits:\n",
    "            if split in [Split.DEV, Split.TRAIN]:\n",
    "                metrics = {m.name: m.mean_last_k(1) for d in self.metrics[split].values() for m in d.values()}\n",
    "                combined_losses = self.combined_losses[split]\n",
    "                losses = {v.name: v.mean_last_k(1) for v in self.losses[split].values()}\n",
    "                out = {**out, **metrics, combined_losses.name: combined_losses.mean_last_k(1), **losses}\n",
    "            else:\n",
    "                metrics = {m.name: m.mean_all() for d in self.metrics[split].values() for m in d.values()}\n",
    "                combined_losses = self.combined_losses[split]\n",
    "                losses = {v.name: v.mean_all() for v in self.losses[split].values()}\n",
    "                out = {**out, **metrics, combined_losses.name: combined_losses.mean_all(), **losses}\n",
    "\n",
    "        self.logger.log(out)"
   ],
   "id": "9701bf95e04cf660",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:45:21.415292Z",
     "start_time": "2024-12-08T21:45:21.406466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict, Optional\n",
    "import torch\n",
    "from media_bias_detection.utils.enums import AggregationMethod\n",
    "import random  # Added missing import\n",
    "\n",
    "class GradientAggregator:\n",
    "    \"\"\"Aggregator class for combining possibly conflicting gradients into one \"optimal\" grad.\"\"\"\n",
    "    \n",
    "    def __init__(self, aggregation_method: AggregationMethod = AggregationMethod.MEAN):\n",
    "        \"\"\"Initialize GradientAggregator.\"\"\"\n",
    "        self.aggregation_method = aggregation_method\n",
    "        self.accumulator = (\n",
    "            RunningSumAccumulator() if aggregation_method == AggregationMethod.MEAN \n",
    "            else StackedAccumulator()\n",
    "        )\n",
    "        self._conflicting_gradient_count = 0\n",
    "        self._nonconflicting_gradient_count = 0\n",
    "\n",
    "    def reset_accumulator(self) -> None:\n",
    "        \"\"\"Reset the accumulator.\"\"\"\n",
    "        self.accumulator = (\n",
    "            RunningSumAccumulator() if self.aggregation_method == AggregationMethod.MEAN \n",
    "            else StackedAccumulator()\n",
    "        )\n",
    "\n",
    "    def find_nonconflicting_grad(self, grad_tensor: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Use on of the algorithms to find a nonconflicting gradient.\"\"\"\n",
    "        if self.aggregation_method == AggregationMethod.PCGRAD:\n",
    "            return self.pcgrad(grad_tensor).mean(dim=0)\n",
    "        elif self.aggregation_method == AggregationMethod.PCGRAD_ONLINE:\n",
    "            assert len(grad_tensor) == 2\n",
    "            return self.pcgrad_online(grad_tensor)\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "    def aggregate_gradients(self) -> torch.tensor:\n",
    "        \"\"\"Aggregate possibly conflicting set of gradients (given as a list of dictionaries).\"\"\"\n",
    "        conflicting_grads = self.accumulator.get_gradients()\n",
    "        length = len(conflicting_grads[list(conflicting_grads.keys())[0]])\n",
    "\n",
    "        if (self.aggregation_method == AggregationMethod.PCGRAD_ONLINE\n",
    "                or self.aggregation_method == AggregationMethod.MEAN):\n",
    "            assert length == 1\n",
    "            return self.accumulator.get_avg_gradients()\n",
    "        elif self.aggregation_method == AggregationMethod.PCGRAD:\n",
    "            conflicting_grads = [{k: v[i, ...] for k, v in conflicting_grads.items()} \n",
    "                               for i in range(length)]\n",
    "            final_grad: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "            if len(conflicting_grads) == 1:\n",
    "                return conflicting_grads[0]\n",
    "\n",
    "            keys = list(conflicting_grads[0].keys())\n",
    "            for layer_key in keys:\n",
    "                list_of_st_grads = [st_grad[layer_key] for st_grad in conflicting_grads]\n",
    "                final_grad.update({\n",
    "                    layer_key: self.find_nonconflicting_grad(torch.stack(list_of_st_grads, dim=0))\n",
    "                })\n",
    "            return final_grad\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "    def pcgrad(self, grad_tensor: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Project conflicting gradients onto orthogonal plane.\"\"\"\n",
    "        pc_grads, num_of_tasks = grad_tensor.clone(), len(grad_tensor)\n",
    "        original_shape = grad_tensor.shape\n",
    "        pc_grads = pc_grads.view(num_of_tasks, -1)\n",
    "        grad_tensor = grad_tensor.view(num_of_tasks, -1)\n",
    "\n",
    "        for g_i in range(num_of_tasks):\n",
    "            task_index = list(range(num_of_tasks))\n",
    "            random.shuffle(task_index)\n",
    "            for g_j in task_index:\n",
    "                dot_product = pc_grads[g_i].dot(grad_tensor[g_j])\n",
    "                if dot_product < 0:\n",
    "                    pc_grads[g_i] -= ((dot_product / (grad_tensor[g_j].norm() ** 2)) \n",
    "                                    * grad_tensor[g_j])\n",
    "                    self._conflicting_gradient_count += 1\n",
    "                else:\n",
    "                    self._nonconflicting_gradient_count += 1\n",
    "        return pc_grads.view(original_shape)\n",
    "\n",
    "    def pcgrad_online(self, grad_tensor: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Perform pcgrad (online) algorithm.\"\"\"\n",
    "        assert len(grad_tensor) == 2\n",
    "        p = grad_tensor[0]\n",
    "        g = grad_tensor[-1]\n",
    "\n",
    "        p = p.view(-1)\n",
    "        g = g.view(-1)\n",
    "\n",
    "        dot_product = p.dot(g)\n",
    "        if dot_product < 0:\n",
    "            p = p - (dot_product / (g.norm() ** 2)) * g\n",
    "            self._conflicting_gradient_count += 1\n",
    "        else:\n",
    "            self._nonconflicting_gradient_count += 1\n",
    "\n",
    "        p += g\n",
    "        return p.view(grad_tensor[0].shape)\n",
    "\n",
    "    def aggregate_gradients_online(self) -> Dict[str, torch.tensor]:\n",
    "        \"\"\"Aggregate the current overall gradient with a new gradient.\"\"\"\n",
    "        conflicting_grads = self.accumulator.get_gradients()\n",
    "        length = len(conflicting_grads[list(conflicting_grads.keys())[0]])\n",
    "        conflicting_grads = [{k: v[i, ...] for k, v in conflicting_grads.items()} \n",
    "                           for i in range(length)]\n",
    "        current_overall_grad: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "        if length == 1:\n",
    "            return conflicting_grads[0]\n",
    "        elif length == 2:\n",
    "            keys = list(conflicting_grads[0].keys())\n",
    "            for layer_key in keys:\n",
    "                list_of_st_grads = [st_grad[layer_key] for st_grad in conflicting_grads]\n",
    "                current_overall_grad.update({\n",
    "                    layer_key: self.find_nonconflicting_grad(torch.stack(list_of_st_grads, dim=0))\n",
    "                })\n",
    "            return current_overall_grad\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "    def update(self, gradients: Dict[str, torch.tensor], scaling_weight: float) -> None:\n",
    "        \"\"\"Update the gradients of the accumulator.\"\"\"\n",
    "        self.accumulator.update(gradients=gradients, weight=scaling_weight)\n",
    "        if self.aggregation_method == AggregationMethod.PCGRAD_ONLINE:\n",
    "            self.accumulator.set_gradients(gradients=self.aggregate_gradients_online())\n",
    "\n",
    "    def get_conflicting_gradients_ratio(self) -> Optional[float]:\n",
    "        \"\"\"Get the ratio of conflicting gradients.\"\"\"\n",
    "        if self.aggregation_method == AggregationMethod.MEAN:\n",
    "            raise Exception\n",
    "        if self._conflicting_gradient_count + self._nonconflicting_gradient_count == 0:\n",
    "            raise Exception\n",
    "        return (self._conflicting_gradient_count / \n",
    "                (self._conflicting_gradient_count + self._nonconflicting_gradient_count))        "
   ],
   "id": "ea50454b844bdda5",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:45:21.888966Z",
     "start_time": "2024-12-08T21:45:21.873343Z"
    }
   },
   "cell_type": "code",
   "source": [
    " \"\"\"This module contains the trainer class.\"\"\"\n",
    "import statistics as stats\n",
    "from typing import Any, Dict, List\n",
    "from media_bias_detection.config.config import MAX_NUMBER_OF_STEPS\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import get_polynomial_decay_schedule_with_warmup\n",
    "\n",
    "from media_bias_detection.utils.enums import AggregationMethod, LossScaling, Split\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Trainer class to train and evaluate a model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        task_list: List[Task],\n",
    "        initial_lr,\n",
    "        model_name: str,\n",
    "        pretrained_path: str,\n",
    "        sub_batch_size: int,\n",
    "        eval_batch_size: int,\n",
    "        early_stopping_mode,\n",
    "        resurrection: bool,\n",
    "        aggregation_method: AggregationMethod,\n",
    "        loss_scaling: LossScaling,\n",
    "        num_warmup_steps: int,\n",
    "        head_specific_lr_dict: Dict[str, float],\n",
    "        head_specific_patience_dict: Dict[str, int],\n",
    "        head_specific_max_epoch_dict: Dict[str, int],\n",
    "        logger: Logger,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Initialize a Trainer.\"\"\"\n",
    "        self.early_stopping_mode = early_stopping_mode\n",
    "        self.logger = logger\n",
    "        self.loss_scaling = loss_scaling\n",
    "        self.model, batch_list_train, batch_list_dev, batch_list_eval, batch_list_test = ModelFactory(\n",
    "            task_list=task_list,\n",
    "            sub_batch_size=sub_batch_size,\n",
    "            eval_batch_size=eval_batch_size,\n",
    "            pretrained_path=pretrained_path,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.batch_lists = {\n",
    "            Split.TRAIN: batch_list_train,\n",
    "            Split.DEV: batch_list_dev,\n",
    "            Split.EVAL: batch_list_eval,\n",
    "            Split.TEST: batch_list_test,\n",
    "        }\n",
    "\n",
    "        # shared backbone model optimizer\n",
    "        self.lm_optimizer = torch.optim.AdamW(self.model.language_model.backbone.parameters(), lr=initial_lr)\n",
    "        self.lm_lr_scheduler = get_polynomial_decay_schedule_with_warmup(\n",
    "            optimizer=self.lm_optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=max([len(dl) for dl in self.batch_lists[Split.TRAIN].dataloaders.values()])\n",
    "            * stats.median(head_specific_max_epoch_dict.values()),\n",
    "        )\n",
    "\n",
    "        # task-specifics optimizers\n",
    "        self.head_optimizers = {\n",
    "            str(st_id): torch.optim.AdamW(head.parameters(), lr=head_specific_lr_dict[st_id])\n",
    "            for st_id, head in self.model.heads.items()\n",
    "        }\n",
    "        self.head_lr_schedulers = {\n",
    "            str(st_id): get_polynomial_decay_schedule_with_warmup(\n",
    "                optimizer=self.head_optimizers[st_id],\n",
    "                num_warmup_steps=num_warmup_steps,\n",
    "                num_training_steps=len(self.batch_lists[Split.TRAIN].dataloaders[st_id])\n",
    "                * head_specific_max_epoch_dict[st_id],\n",
    "            )\n",
    "            for st_id in self.model.heads.keys()\n",
    "        }\n",
    "\n",
    "        # flags controlling stopping and resurrection\n",
    "        self.task_alive_flags = {str(st_id): True for st_id in self.model.heads.keys()}\n",
    "        self.task_zombie_flags = {str(st_id): False for st_id in self.model.heads.keys()}\n",
    "        self.early_stopper = EarlyStopper(\n",
    "            st_ids=self.model.heads.keys(),\n",
    "            mode=self.early_stopping_mode,\n",
    "            patience=head_specific_patience_dict,\n",
    "            resurrection=resurrection,\n",
    "        )\n",
    "\n",
    "        self.tracker = Tracker(heads=self.model.heads, logger=logger)\n",
    "        self.GA = GradientAggregator(aggregation_method=aggregation_method)\n",
    "        self.progress_bar = tqdm(range(len(self.model.heads)))\n",
    "        self.model_name = model_name\n",
    "        self.scaling_weights = {str(st.id): st.get_scaling_weight() for t in task_list for st in t.subtasks_list}\n",
    "        self.MAX_NUMBER_OF_STEPS = MAX_NUMBER_OF_STEPS\n",
    "        self.k = 50\n",
    "\n",
    "    def head_specific_optimization(self, st_id: str, lm_grads, scaling_weight):\n",
    "        \"\"\"\n",
    "        Perform the optimization of a task-specific head.\n",
    "\n",
    "        This method is only called when mode is training.\n",
    "        @param st_id: The subtask id.\n",
    "        @param lm_grads: The LM gradients.\n",
    "        @param scaling_weight: The scaling weight of that subtask.\n",
    "        @return: A dictionary with additional payload containing the conflicting gradients ratio.\n",
    "        \"\"\"\n",
    "        additional_payload = {}\n",
    "        last_dev_loss = self.tracker.get_last_st_loss(split=Split.DEV, st_id=st_id, k=self.k)\n",
    "        should_stop_now = (\n",
    "            self.early_stopper.early_stop(st_id=st_id, dev_loss=last_dev_loss)\n",
    "            if (self.task_alive_flags[st_id] or self.task_zombie_flags[st_id])\n",
    "            else False\n",
    "        )\n",
    "\n",
    "        should_resurrect_now = (\n",
    "            self.early_stopper.resurrect(st_id=st_id, dev_loss=last_dev_loss)\n",
    "            if (not self.task_zombie_flags[st_id] and not self.task_alive_flags[st_id])\n",
    "            else False\n",
    "        )\n",
    "\n",
    "        should_stay_zombie = not self.task_alive_flags[st_id] and self.task_zombie_flags[st_id] and not should_stop_now\n",
    "\n",
    "        # Eval + Log task when it DIES\n",
    "        if should_stop_now and self.task_alive_flags[st_id]:\n",
    "            print(f\"Subtask {st_id} is now DEAD.\")\n",
    "            self.eval_st(split=Split.EVAL, st_id=st_id)\n",
    "            self.tracker.log(splits=[Split.EVAL], additional_payload={st_id + \"_STOPPED\": 0})\n",
    "            self.progress_bar.update()\n",
    "\n",
    "        # Eval + Log task when it RESURRECTS\n",
    "        elif should_resurrect_now and not self.task_zombie_flags[st_id]:\n",
    "            print(f\"Subtask {st_id} is now ZOMBIE.\")\n",
    "            additional_payload[st_id + \"_ZOMBIE\"] = 0\n",
    "            self.early_stopper.reset_early_stopper(st_id=st_id)\n",
    "\n",
    "        # Eval + Log task when a ZOMBIE DIES\n",
    "        elif should_stop_now and self.task_zombie_flags[st_id]:\n",
    "            print(f\"Subtask {st_id} is now DEAD AGAIN.\")\n",
    "            additional_payload[st_id + \"_DEAD_ZOMBIE\"] = 0\n",
    "            self.early_stopper.reset_early_stopper(st_id=st_id)\n",
    "\n",
    "        self.task_alive_flags[st_id] = self.task_alive_flags[st_id] and not (\n",
    "            should_stop_now or self.tracker.get_last_st_metric(split=Split.DEV, st_id=st_id, k=10) == 1\n",
    "        )\n",
    "        self.task_zombie_flags[st_id] = should_resurrect_now or should_stay_zombie\n",
    "\n",
    "        # We optimize a task if it is alive or zombie\n",
    "        optimize_task = self.task_alive_flags[str(st_id)] or self.task_zombie_flags[str(st_id)]\n",
    "        if optimize_task:\n",
    "            self.head_optimizers[st_id].step()\n",
    "            self.head_lr_schedulers[st_id].step()\n",
    "\n",
    "        if self.early_stopping_mode != EarlyStoppingMode.BACKBONE or optimize_task:\n",
    "            self.GA.update(lm_grads, scaling_weight=scaling_weight)\n",
    "\n",
    "        return additional_payload\n",
    "\n",
    "    def backbone_optimization(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform the optimization of the backbone.\n",
    "\n",
    "        This method is only called when mode is training.\n",
    "        @return: A dictionary with additional payload containing the conflicting gradients ratio.\n",
    "        \"\"\"\n",
    "        # Optimize the LM such that: we aggregate gradients from subtasks and set the final\n",
    "        # gradient to the LM and subsequently optimize (only the LM)\n",
    "        additional_payload = {}\n",
    "        if any(self.task_alive_flags.values()):\n",
    "            aggregated_gradients = self.GA.aggregate_gradients()\n",
    "            self.model.language_model.set_grads(aggregated_gradients)\n",
    "            self.lm_optimizer.step()\n",
    "            self.lm_lr_scheduler.step()\n",
    "        if self.GA.aggregation_method in [AggregationMethod.PCGRAD, AggregationMethod.PCGRAD_ONLINE]:\n",
    "            conflicting_gradients_ratio = self.GA.get_conflicting_gradients_ratio()\n",
    "            additional_payload[\"conflicting_gradients_ratio\"] = conflicting_gradients_ratio\n",
    "\n",
    "        return additional_payload\n",
    "\n",
    "    def handle_batch(self, batch, split: Split = Split.TRAIN) -> Dict[str, Any]:\n",
    "        \"\"\"Handle a batch.\n",
    "\n",
    "         (always) Pass a batch of sub_batches through the network.\n",
    "         (in train-mode) For each sub_batch, accumulate the gradients of the LM.\n",
    "         For each sub_batch and each st_id,\n",
    "            - (in train-mode) accumulate the gradients of the respective head,\n",
    "            - (always) accumulate the metric of the respective head,\n",
    "            - (always) accumulate the loss of the respective head.\n",
    "        (always) Log all metrics and losses to wandb.\n",
    "         (in train-mode) After all sub_batches are processed, normalize the LM gradients and the head-specific gradients.\n",
    "         (in train-mode) Then, perform the step of the lr_scheduler and the optimizer.\n",
    "\n",
    "        @param batch: The batch containing sub-batches.\n",
    "        @param split: The split (TRAIN, DEV, TEST)\n",
    "        @return: A dictionary containing additional payload that needs to be logged.\n",
    "        \"\"\"\n",
    "        training = split == Split.TRAIN\n",
    "        losses = []\n",
    "        additional_payloads: Dict[str, Any] = {}\n",
    "        # reset accumulator only if it's a new batch for training, otherwise eval drops accumulated gradients\n",
    "        if training:\n",
    "            self.GA.reset_accumulator()\n",
    "\n",
    "        # sub_batch consists of data of one subtask only\n",
    "        for sub_batch in batch:\n",
    "            X, attention_masks, Y, st_id = sub_batch\n",
    "            loss, metric_values, lm_grads = self._step((X, attention_masks, Y, st_id.unique()), training=training)\n",
    "            st_id = str(st_id.unique().item())\n",
    "            scaling_weight = self.scaling_weights[st_id] if self.loss_scaling == LossScaling.STATIC else 1.0\n",
    "\n",
    "            if training:\n",
    "                additional_payload = self.head_specific_optimization(\n",
    "                    st_id=st_id, lm_grads=lm_grads, scaling_weight=scaling_weight\n",
    "                )\n",
    "                additional_payloads = {**additional_payload, **additional_payloads}\n",
    "\n",
    "            # Update losses & metrics\n",
    "            for metric, value in metric_values.items():\n",
    "                self.tracker.update_metric(split=split, st_id=st_id, metric=metric, value=value)\n",
    "            self.tracker.update_loss(split=split, st_id=st_id, value=loss.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if training:\n",
    "            additional_payload = self.backbone_optimization()\n",
    "            additional_payloads = {**additional_payload, **additional_payloads}\n",
    "\n",
    "        self.tracker.update_combined_loss(split=split, value=np.mean(losses))\n",
    "        return additional_payloads\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Fit a model.\"\"\"\n",
    "        step = 0\n",
    "        \n",
    "        for i in range(self.MAX_NUMBER_OF_STEPS):\n",
    "            additional_payload_train, additional_payload_dev = {}, {}\n",
    "            # Check if any task is still training\n",
    "            if not any(self.task_alive_flags.values()):\n",
    "                break\n",
    "            step += 1\n",
    "            batch = next(self.batch_lists[Split.TRAIN])\n",
    "            additional_payload_train = self.handle_batch(batch=batch, split=Split.TRAIN)\n",
    "            if step % 3 == 0:\n",
    "                batch = next(self.batch_lists[Split.DEV])\n",
    "                additional_payload_dev = self.handle_batch(batch=batch, split=Split.DEV)\n",
    "            self.refresh_pbar()\n",
    "            self.tracker.log(\n",
    "                splits=[Split.TRAIN, Split.DEV],\n",
    "                additional_payload={**additional_payload_train, **additional_payload_dev},\n",
    "            )\n",
    "\n",
    "        self.eval(split=Split.EVAL)\n",
    "\n",
    "    def _step(self, batch, training: bool = True):\n",
    "        \"\"\"\n",
    "        Make one step.\n",
    "\n",
    "        @param batch: A dictionary containing X, Y, std_ids and attention_masks.\n",
    "        \"\"\"\n",
    "        inputs = {\"X\": batch[0], \"attention_masks\": batch[1], \"Y\": batch[2], \"st_id\": batch[3]}\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "        if training:\n",
    "            self.model.train()\n",
    "            loss, heads_metrics_values = self.model(**inputs)\n",
    "            self.lm_optimizer.zero_grad()\n",
    "            for st_id, optim in self.head_optimizers.items():\n",
    "                optim.zero_grad()\n",
    "            loss.backward()\n",
    "            lm_gradients = self.model.language_model.get_grads()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            lm_gradients = None\n",
    "            with torch.no_grad():\n",
    "                loss, heads_metrics_values = self.model(**inputs)\n",
    "\n",
    "        del inputs\n",
    "        return loss, heads_metrics_values, lm_gradients\n",
    "\n",
    "    def eval(self, split):\n",
    "        \"\"\"Evaluate the model on the entire test or dev set.\"\"\"\n",
    "        assert split in [Split.EVAL, Split.TEST]\n",
    "\n",
    "        for st_id in self.batch_lists[split].iter_dataloaders.keys():\n",
    "            self.eval_st(split=split, st_id=st_id)\n",
    "\n",
    "        self.tracker.log(splits=[split])\n",
    "\n",
    "    def eval_st(self, split, st_id):\n",
    "        \"\"\"Evaluate on a subtask, given a certain split.\"\"\"\n",
    "        batch_list = self.batch_lists[split]\n",
    "        batch_list._reset()\n",
    "        idl = batch_list.iter_dataloaders[st_id]\n",
    "        for batch in idl:\n",
    "            _ = self.handle_batch(batch=[batch], split=split)\n",
    "\n",
    "    def refresh_pbar(self):\n",
    "        \"\"\"Update the progress bar.\"\"\"\n",
    "        desc = str(self.tracker)\n",
    "        self.progress_bar.set_description(desc=desc)\n",
    "        self.progress_bar.refresh()\n",
    "\n",
    "    def fit_debug(self, k: int):\n",
    "        \"\"\"Fit for k iterations only to check if a model can process the data.\"\"\"\n",
    "        step = 0\n",
    "        for _ in range(k):\n",
    "            step += 1\n",
    "            batch = next(self.batch_lists[Split.TRAIN])\n",
    "            self.handle_batch(batch=batch, split=Split.TRAIN)\n",
    "            # Evaluate on dev-batch\n",
    "            batch = next(self.batch_lists[Split.DEV])\n",
    "            self.handle_batch(batch=batch, split=Split.DEV)\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"Save the model.\"\"\"\n",
    "        os.makedirs(\"model_files\", exist_ok=True) # added \n",
    "        model_files_path = \"model_files/\" + self.model_name + \".pth\"\n",
    "        torch.save(self.model.state_dict(), model_files_path)"
   ],
   "id": "87b131cf65c76f07",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Running the experiment\n",
    "For actually running the experiment the configurations from the \"cotrain_random_tasks.py\" were taken and adapted to the changes (MFFLOW logging etc.). \n",
    "Instead of the .fit() method of the trainer class, I use the .fit_debug() method, to check the general ability of the model to process the dta.\n",
    "The experiment was run on the local machine."
   ],
   "id": "5191be2aeb0be3ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:45:23.093670Z",
     "start_time": "2024-12-08T21:45:23.090040Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Current working directory: {os.getcwd()}\")",
   "id": "24be4019d8626b2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/heddafiedler/Documents/MASTER_DATA_SCIENCE/Semester_3/DL/DL_Project\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:45:23.332165Z",
     "start_time": "2024-12-08T21:45:23.329624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# changing working directory to the root of the project:/Users/heddafiedler/Documents/MASTER_DATA_SCIENCE/Semester_3/DL/DL_Project\n",
    "os.chdir(\"/Users/heddafiedler/Documents/MASTER_DATA_SCIENCE/Semester_3/DL/DL_Project\")"
   ],
   "id": "d7f71cc02cf87461",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:45:23.540654Z",
     "start_time": "2024-12-08T21:45:23.536663Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Current working directory: {os.getcwd()}\")",
   "id": "d96efb81f4cdb87e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/heddafiedler/Documents/MASTER_DATA_SCIENCE/Semester_3/DL/DL_Project\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T22:03:48.760334Z",
     "start_time": "2024-12-08T22:03:45.156072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Script for executing the experiment 1. Run co-training of all families.\"\"\"\n",
    "import os\n",
    "import wandb\n",
    "from media_bias_detection.utils.enums import Split, AggregationMethod, LossScaling\n",
    "from media_bias_detection.utils.common import set_random_seed\n",
    "from media_bias_detection.config.config import (\n",
    "    head_specific_lr,\n",
    "    head_specific_max_epoch,\n",
    "    head_specific_patience)\n",
    "\n",
    "EXPERIMENT_NAME = \"experiment_baseline_check\"\n",
    "MODEL_NAME = \"baseline_check\"\n",
    "selected_tasks = [\n",
    "babe_10,\n",
    "cw_hard_03,\n",
    "]\n",
    "tasks = selected_tasks\n",
    "\n",
    "for t in tasks:\n",
    "    for st in t.subtasks_list:\n",
    "        st.process()\n",
    "\n",
    "\n",
    "# training config\n",
    "config = {\n",
    "   \"sub_batch_size\": 32,\n",
    "   \"eval_batch_size\": 128,\n",
    "   \"initial_lr\": 4e-5,\n",
    "   \"dropout_prob\": 0.1,\n",
    "   \"hidden_dimension\": 768,\n",
    "   \"input_dimension\": 768,\n",
    "   \"aggregation_method\": AggregationMethod.MEAN,\n",
    "   \"early_stopping_mode\": EarlyStoppingMode.HEADS,\n",
    "   \"loss_scaling\": LossScaling.STATIC,\n",
    "   \"num_warmup_steps\": 10,\n",
    "   \"pretrained_path\": None,\n",
    "   \"resurrection\": True,\n",
    "   \"model_name\": \"YOUR_MODEL_NAME\",\n",
    "   \"head_specific_lr_dict\": head_specific_lr,\n",
    "   \"head_specific_patience_dict\": head_specific_patience,\n",
    "   \"head_specific_max_epoch_dict\": head_specific_max_epoch,\n",
    "   \"logger\": Logger(EXPERIMENT_NAME),\n",
    " }\n",
    "\n",
    "set_random_seed() # default is 321\n",
    "#wandb.init(project=EXPERIMENT_NAME,name=MODEL_NAME)\n",
    "trainer = Trainer(task_list=tasks, **config)\n",
    "trainer.fit_debug(k=1)\n",
    "trainer.eval(split=Split.TEST)\n",
    "trainer.save_model()\n",
    "#wandb.finish()"
   ],
   "id": "6779ca63c6f7366d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-08 23:03:45,159: INFO: 2101821432: Processing SubTask 10001]\n",
      "[2024-12-08 23:03:45,461: INFO: 2101821432: SubTask 10001 processed successfully]\n",
      "[2024-12-08 23:03:45,462: INFO: 2101821432: Processing SubTask 10002]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [18:16<?, ?it/s]\n",
      "  0%|          | 0/3 [12:03<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-08 23:03:46,744: INFO: 2101821432: SubTask 10002 processed successfully]\n",
      "[2024-12-08 23:03:46,745: INFO: 2101821432: Processing SubTask 300001]\n",
      "[2024-12-08 23:03:47,193: INFO: 2101821432: SubTask 300001 processed successfully]\n",
      "[2024-12-08 23:03:47,552: INFO: 3535113285: Initialized ClassificationHead with 2 classes]\n",
      "[2024-12-08 23:03:47,554: INFO: 3535113285: Initialized TokenClassificationHead with 3 classes]\n",
      "[2024-12-08 23:03:47,557: INFO: 3535113285: Initialized ClassificationHead with 2 classes]\n",
      "[2024-12-08 23:03:47,558: INFO: 1443602028: Creating BatchList with 3 subtasks, batch size 32]\n",
      "[2024-12-08 23:03:47,559: INFO: 1443602028: Initializing dataset for subtask 10001 with split Split.TRAIN]\n",
      "[2024-12-08 23:03:47,559: INFO: 1443602028: Resetting dataset for subtask 10001]\n",
      "[2024-12-08 23:03:47,560: INFO: 1443602028: Initializing dataset for subtask 10002 with split Split.TRAIN]\n",
      "[2024-12-08 23:03:47,561: INFO: 1443602028: Resetting dataset for subtask 10002]\n",
      "[2024-12-08 23:03:47,562: INFO: 1443602028: Initializing dataset for subtask 300001 with split Split.TRAIN]\n",
      "[2024-12-08 23:03:47,562: INFO: 1443602028: Resetting dataset for subtask 300001]\n",
      "[2024-12-08 23:03:47,563: INFO: 1443602028: Creating BatchList with 3 subtasks, batch size 128]\n",
      "[2024-12-08 23:03:47,564: INFO: 1443602028: Initializing dataset for subtask 10001 with split Split.DEV]\n",
      "[2024-12-08 23:03:47,564: INFO: 1443602028: Resetting dataset for subtask 10001]\n",
      "[2024-12-08 23:03:47,565: INFO: 1443602028: Initializing dataset for subtask 10002 with split Split.DEV]\n",
      "[2024-12-08 23:03:47,566: INFO: 1443602028: Resetting dataset for subtask 10002]\n",
      "[2024-12-08 23:03:47,567: INFO: 1443602028: Initializing dataset for subtask 300001 with split Split.DEV]\n",
      "[2024-12-08 23:03:47,568: INFO: 1443602028: Resetting dataset for subtask 300001]\n",
      "[2024-12-08 23:03:47,569: INFO: 1443602028: Creating BatchListEvalTest with 3 subtasks]\n",
      "[2024-12-08 23:03:47,569: INFO: 1443602028: Initializing dataset for subtask 10001 with split Split.DEV]\n",
      "[2024-12-08 23:03:47,569: INFO: 1443602028: Resetting dataset for subtask 10001]\n",
      "[2024-12-08 23:03:47,570: INFO: 1443602028: Initializing dataset for subtask 10002 with split Split.DEV]\n",
      "[2024-12-08 23:03:47,571: INFO: 1443602028: Resetting dataset for subtask 10002]\n",
      "[2024-12-08 23:03:47,572: INFO: 1443602028: Initializing dataset for subtask 300001 with split Split.DEV]\n",
      "[2024-12-08 23:03:47,572: INFO: 1443602028: Resetting dataset for subtask 300001]\n",
      "[2024-12-08 23:03:47,573: INFO: 1443602028: Creating BatchListEvalTest with 3 subtasks]\n",
      "[2024-12-08 23:03:47,573: INFO: 1443602028: Initializing dataset for subtask 10001 with split Split.TEST]\n",
      "[2024-12-08 23:03:47,574: INFO: 1443602028: Resetting dataset for subtask 10001]\n",
      "[2024-12-08 23:03:47,575: INFO: 1443602028: Initializing dataset for subtask 10002 with split Split.TEST]\n",
      "[2024-12-08 23:03:47,575: INFO: 1443602028: Resetting dataset for subtask 10002]\n",
      "[2024-12-08 23:03:47,576: INFO: 1443602028: Initializing dataset for subtask 300001 with split Split.TEST]\n",
      "[2024-12-08 23:03:47,577: INFO: 1443602028: Resetting dataset for subtask 300001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-08 23:03:47,580: INFO: 1443602028: Generated batch with 3 sub-batches]\n",
      "logits tensor([[ 0.1898, -0.3170],\n",
      "        [ 0.0526, -0.1187],\n",
      "        [ 0.2379, -0.1059],\n",
      "        [ 0.4416, -0.1152],\n",
      "        [ 0.2685, -0.0324],\n",
      "        [ 0.2379, -0.1457],\n",
      "        [ 0.4363, -0.1404],\n",
      "        [ 0.3723, -0.2801],\n",
      "        [ 0.4980, -0.2195],\n",
      "        [ 0.3898, -0.2332],\n",
      "        [ 0.3908, -0.1490],\n",
      "        [ 0.2807, -0.1742],\n",
      "        [ 0.4823, -0.1689],\n",
      "        [ 0.3311, -0.1615],\n",
      "        [ 0.5035, -0.2540],\n",
      "        [ 0.3834, -0.0319],\n",
      "        [ 0.4263, -0.1270],\n",
      "        [ 0.3850, -0.2062],\n",
      "        [ 0.3589, -0.1606],\n",
      "        [ 0.2190, -0.2211],\n",
      "        [ 0.3428, -0.2286],\n",
      "        [ 0.2068, -0.2284],\n",
      "        [ 0.5168, -0.1300],\n",
      "        [ 0.4251, -0.1182],\n",
      "        [ 0.3485,  0.0497],\n",
      "        [ 0.3661, -0.2206],\n",
      "        [ 0.3199, -0.0578],\n",
      "        [ 0.4140, -0.1749],\n",
      "        [ 0.3747, -0.1306],\n",
      "        [ 0.3345, -0.1039],\n",
      "        [ 0.4083, -0.0165],\n",
      "        [ 0.3960, -0.1866]], grad_fn=<AddmmBackward0>)\n",
      "y tensor([[1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]])\n",
      "logits_after reshaping torch.Size([32, 2])\n",
      "logits_after reshaping tensor([[[ 0.1898],\n",
      "         [-0.3170]],\n",
      "\n",
      "        [[ 0.0526],\n",
      "         [-0.1187]],\n",
      "\n",
      "        [[ 0.2379],\n",
      "         [-0.1059]],\n",
      "\n",
      "        [[ 0.4416],\n",
      "         [-0.1152]],\n",
      "\n",
      "        [[ 0.2685],\n",
      "         [-0.0324]],\n",
      "\n",
      "        [[ 0.2379],\n",
      "         [-0.1457]],\n",
      "\n",
      "        [[ 0.4363],\n",
      "         [-0.1404]],\n",
      "\n",
      "        [[ 0.3723],\n",
      "         [-0.2801]],\n",
      "\n",
      "        [[ 0.4980],\n",
      "         [-0.2195]],\n",
      "\n",
      "        [[ 0.3898],\n",
      "         [-0.2332]],\n",
      "\n",
      "        [[ 0.3908],\n",
      "         [-0.1490]],\n",
      "\n",
      "        [[ 0.2807],\n",
      "         [-0.1742]],\n",
      "\n",
      "        [[ 0.4823],\n",
      "         [-0.1689]],\n",
      "\n",
      "        [[ 0.3311],\n",
      "         [-0.1615]],\n",
      "\n",
      "        [[ 0.5035],\n",
      "         [-0.2540]],\n",
      "\n",
      "        [[ 0.3834],\n",
      "         [-0.0319]],\n",
      "\n",
      "        [[ 0.4263],\n",
      "         [-0.1270]],\n",
      "\n",
      "        [[ 0.3850],\n",
      "         [-0.2062]],\n",
      "\n",
      "        [[ 0.3589],\n",
      "         [-0.1606]],\n",
      "\n",
      "        [[ 0.2190],\n",
      "         [-0.2211]],\n",
      "\n",
      "        [[ 0.3428],\n",
      "         [-0.2286]],\n",
      "\n",
      "        [[ 0.2068],\n",
      "         [-0.2284]],\n",
      "\n",
      "        [[ 0.5168],\n",
      "         [-0.1300]],\n",
      "\n",
      "        [[ 0.4251],\n",
      "         [-0.1182]],\n",
      "\n",
      "        [[ 0.3485],\n",
      "         [ 0.0497]],\n",
      "\n",
      "        [[ 0.3661],\n",
      "         [-0.2206]],\n",
      "\n",
      "        [[ 0.3199],\n",
      "         [-0.0578]],\n",
      "\n",
      "        [[ 0.4140],\n",
      "         [-0.1749]],\n",
      "\n",
      "        [[ 0.3747],\n",
      "         [-0.1306]],\n",
      "\n",
      "        [[ 0.3345],\n",
      "         [-0.1039]],\n",
      "\n",
      "        [[ 0.4083],\n",
      "         [-0.0165]],\n",
      "\n",
      "        [[ 0.3960],\n",
      "         [-0.1866]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "ename": "HeadError",
     "evalue": "Classification forward pass failed: Predictions and targets are expected to have the same shape, but got torch.Size([32, 2, 1]) and torch.Size([32, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 165\u001B[0m, in \u001B[0;36mClassificationHead.forward\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogits_after reshaping\u001B[39m\u001B[38;5;124m\"\u001B[39m, logits)\n\u001B[0;32m--> 165\u001B[0m metrics \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    166\u001B[0m     name: metric(logits\u001B[38;5;241m.\u001B[39mcpu(), y\u001B[38;5;241m.\u001B[39mcpu())\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m name, metric \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m    168\u001B[0m }\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m logits, loss, metrics\n",
      "Cell \u001B[0;32mIn[36], line 166\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogits_after reshaping\u001B[39m\u001B[38;5;124m\"\u001B[39m, logits)\n\u001B[1;32m    165\u001B[0m metrics \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m--> 166\u001B[0m     name: metric(logits\u001B[38;5;241m.\u001B[39mcpu(), y\u001B[38;5;241m.\u001B[39mcpu())\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m name, metric \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m    168\u001B[0m }\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m logits, loss, metrics\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_project/lib/python3.11/site-packages/torchmetrics/metric.py:316\u001B[0m, in \u001B[0;36mMetric.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_reduce_state_update(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cache\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_project/lib/python3.11/site-packages/torchmetrics/metric.py:385\u001B[0m, in \u001B[0;36mMetric._forward_reduce_state_update\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;66;03m# calculate batch state and compute batch value\u001B[39;00m\n\u001B[0;32m--> 385\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    386\u001B[0m batch_val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute()\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_project/lib/python3.11/site-packages/torchmetrics/metric.py:560\u001B[0m, in \u001B[0;36mMetric._wrap_update.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    553\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    554\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered different devices in metric calculation (see stacktrace for details).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    555\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m This could be due to the metric class not being on the same device as input.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    558\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m device corresponds to the device of the input.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    559\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m--> 560\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_on_cpu:\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_project/lib/python3.11/site-packages/torchmetrics/metric.py:550\u001B[0m, in \u001B[0;36mMetric._wrap_update.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    549\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 550\u001B[0m     update(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    551\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_project/lib/python3.11/site-packages/torchmetrics/classification/stat_scores.py:187\u001B[0m, in \u001B[0;36mBinaryStatScores.update\u001B[0;34m(self, preds, target)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate_args:\n\u001B[0;32m--> 187\u001B[0m     _binary_stat_scores_tensor_validation(preds, target, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmultidim_average, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_index)\n\u001B[1;32m    188\u001B[0m preds, target \u001B[38;5;241m=\u001B[39m _binary_stat_scores_format(preds, target, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthreshold, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_index)\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_project/lib/python3.11/site-packages/torchmetrics/functional/classification/stat_scores.py:67\u001B[0m, in \u001B[0;36m_binary_stat_scores_tensor_validation\u001B[0;34m(preds, target, multidim_average, ignore_index)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;66;03m# Check that they have same shape\u001B[39;00m\n\u001B[0;32m---> 67\u001B[0m _check_same_shape(preds, target)\n\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# Check that target only contains [0,1] values or value in ignore_index\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_project/lib/python3.11/site-packages/torchmetrics/utilities/checks.py:41\u001B[0m, in \u001B[0;36m_check_same_shape\u001B[0;34m(preds, target)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m preds\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m!=\u001B[39m target\u001B[38;5;241m.\u001B[39mshape:\n\u001B[0;32m---> 41\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPredictions and targets are expected to have the same shape, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpreds\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     43\u001B[0m     )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Predictions and targets are expected to have the same shape, but got torch.Size([32, 2, 1]) and torch.Size([32, 1]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mHeadError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 48\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m#wandb.init(project=EXPERIMENT_NAME,name=MODEL_NAME)\u001B[39;00m\n\u001B[1;32m     47\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(task_list\u001B[38;5;241m=\u001B[39mtasks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig)\n\u001B[0;32m---> 48\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit_debug(k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     49\u001B[0m trainer\u001B[38;5;241m.\u001B[39meval(split\u001B[38;5;241m=\u001B[39mSplit\u001B[38;5;241m.\u001B[39mTEST)\n\u001B[1;32m     50\u001B[0m trainer\u001B[38;5;241m.\u001B[39msave_model()\n",
      "Cell \u001B[0;32mIn[27], line 308\u001B[0m, in \u001B[0;36mTrainer.fit_debug\u001B[0;34m(self, k)\u001B[0m\n\u001B[1;32m    306\u001B[0m step \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    307\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_lists[Split\u001B[38;5;241m.\u001B[39mTRAIN])\n\u001B[0;32m--> 308\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_batch(batch\u001B[38;5;241m=\u001B[39mbatch, split\u001B[38;5;241m=\u001B[39mSplit\u001B[38;5;241m.\u001B[39mTRAIN)\n\u001B[1;32m    309\u001B[0m \u001B[38;5;66;03m# Evaluate on dev-batch\u001B[39;00m\n\u001B[1;32m    310\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_lists[Split\u001B[38;5;241m.\u001B[39mDEV])\n",
      "Cell \u001B[0;32mIn[27], line 207\u001B[0m, in \u001B[0;36mTrainer.handle_batch\u001B[0;34m(self, batch, split)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sub_batch \u001B[38;5;129;01min\u001B[39;00m batch:\n\u001B[1;32m    206\u001B[0m     X, attention_masks, Y, st_id \u001B[38;5;241m=\u001B[39m sub_batch\n\u001B[0;32m--> 207\u001B[0m     loss, metric_values, lm_grads \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step((X, attention_masks, Y, st_id\u001B[38;5;241m.\u001B[39munique()), training\u001B[38;5;241m=\u001B[39mtraining)\n\u001B[1;32m    208\u001B[0m     st_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(st_id\u001B[38;5;241m.\u001B[39munique()\u001B[38;5;241m.\u001B[39mitem())\n\u001B[1;32m    209\u001B[0m     scaling_weight \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaling_weights[st_id] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_scaling \u001B[38;5;241m==\u001B[39m LossScaling\u001B[38;5;241m.\u001B[39mSTATIC \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1.0\u001B[39m\n",
      "Cell \u001B[0;32mIn[27], line 264\u001B[0m, in \u001B[0;36mTrainer._step\u001B[0;34m(self, batch, training)\u001B[0m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m--> 264\u001B[0m     loss, heads_metrics_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n\u001B[1;32m    265\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m    266\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m st_id, optim \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_optimizers\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[13], line 33\u001B[0m, in \u001B[0;36mModel.forward\u001B[0;34m(self, X, attention_masks, Y, st_id)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Pass through the appropriate head\u001B[39;00m\n\u001B[1;32m     32\u001B[0m head \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheads[\u001B[38;5;28mstr\u001B[39m(st_id\u001B[38;5;241m.\u001B[39mitem())]\n\u001B[0;32m---> 33\u001B[0m logits, loss, metric_values \u001B[38;5;241m=\u001B[39m head(x_enc, Y)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss, metric_values\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[36], line 173\u001B[0m, in \u001B[0;36mClassificationHead.forward\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logits, loss, metrics\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 173\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HeadError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClassification forward pass failed: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mHeadError\u001B[0m: Classification forward pass failed: Predictions and targets are expected to have the same shape, but got torch.Size([32, 2, 1]) and torch.Size([32, 1])."
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:43:01.346495Z",
     "start_time": "2024-12-04T20:13:38.818324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model architecture check\n",
    "def print_model_summary(model):\n",
    "    \"\"\"Create a detailed custom model summary.\"\"\"\n",
    "    backbone_params = sum(p.numel() for p in model.language_model.backbone.parameters())\n",
    "    \n",
    "    print(\"=== MTL Model Summary ===\")\n",
    "    print(f\"\\nBackbone: DistilBERT\")\n",
    "    print(f\"Backbone Parameters: {backbone_params:,}\")\n",
    "    \n",
    "    print(\"\\nTask Heads:\")\n",
    "    for task_id, head in model.heads.items():\n",
    "        head_params = sum(p.numel() for p in head.parameters())\n",
    "        print(f\"\\nTask {task_id}:\")\n",
    "        print(f\"  Type: {head.__class__.__name__}\")\n",
    "        print(f\"  Parameters: {head_params:,}\")\n",
    "        \n",
    "        # Head-specific details\n",
    "        if isinstance(head, ClassificationHead):\n",
    "            print(f\"  Classes: {head.num_classes}\")\n",
    "            print(f\"  Labels: {head.num_labels}\")\n",
    "            print(f\"  Metrics: {list(head.metrics.keys())}\")\n",
    "            \n",
    "        elif isinstance(head, TokenClassificationHead):\n",
    "            print(f\"  Classes: {head.num_classes}\")\n",
    "            print(f\"  Metrics: {list(head.metrics.keys())}\")\n",
    "            \n",
    "        elif isinstance(head, RegressionHead):\n",
    "            print(f\"  Output Dimension: 1\")\n",
    "            print(f\"  Metrics: {list(head.metrics.keys())}\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "    \n",
    "    # Task distribution summary\n",
    "    head_types = {}\n",
    "    for head in model.heads.values():\n",
    "        head_type = head.__class__.__name__\n",
    "        head_types[head_type] = head_types.get(head_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nTask Distribution:\")\n",
    "    for head_type, count in head_types.items():\n",
    "        print(f\"  {head_type}: {count}\")\n",
    "\n",
    "# Use it\n",
    "print_model_summary(trainer.model)"
   ],
   "id": "48e77f1982b0bf1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MTL Model Summary ===\n",
      "\n",
      "Backbone: DistilBERT\n",
      "Backbone Parameters: 66,362,880\n",
      "\n",
      "Task Heads:\n",
      "\n",
      "Task 10001:\n",
      "  Type: ClassificationHead\n",
      "  Parameters: 592,130\n",
      "  Classes: 2\n",
      "  Labels: 1\n",
      "  Metrics: ['f1', 'acc']\n",
      "\n",
      "Task 10002:\n",
      "  Type: TokenClassificationHead\n",
      "  Parameters: 2,307\n",
      "  Classes: 3\n",
      "  Metrics: ['f1', 'acc']\n",
      "\n",
      "Task 300001:\n",
      "  Type: ClassificationHead\n",
      "  Parameters: 592,130\n",
      "  Classes: 2\n",
      "  Labels: 1\n",
      "  Metrics: ['f1', 'acc']\n",
      "\n",
      "Total Parameters: 67,549,447\n",
      "\n",
      "Task Distribution:\n",
      "  ClassificationHead: 2\n",
      "  TokenClassificationHead: 1\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8b01c3336c8986a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
