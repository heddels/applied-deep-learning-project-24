{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:30.710465Z",
     "start_time": "2024-12-04T17:48:30.707962Z"
    }
   },
   "source": "import os",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:31.285719Z",
     "start_time": "2024-12-04T17:48:31.284109Z"
    }
   },
   "cell_type": "code",
   "source": "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"",
   "id": "fc51803bfb9245a1",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:31.941946Z",
     "start_time": "2024-12-04T17:48:31.938684Z"
    }
   },
   "cell_type": "code",
   "source": "%pwd",
   "id": "e0e2eebc1f516b7f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/heddafiedler/Documents/MASTER_DATA_SCIENCE/Semester_3/DL'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Purpose of the Notebook\n",
    "\n",
    "In this notebook I create the pipeline for using the model built in the MAGPIE Repository in order to understand its components and test how the elements work together.\n",
    "Besides, I already look into the parts I want to change, like more detailed logging / debugging steps to better understand the process. Apart from that I will use the code provided by the Repository.\n",
    "\n",
    "In order to understand the core elements of the model architecture and pipeline, I will only display the main parts here and import utils and other functions."
   ],
   "id": "34ee8ec75e326ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Ingestion and Preprocessing\n",
    "Since the repository already provides the datasets in a preprocessed way, I will use these files for the model training according to the data sets I chose (see README file).\n",
    "Nevertheless, I will need to include data preprocessing in order to do inference on new data. Therefore, the following part tests the preporcessing of random text input and the tokenization of the text. This test is later used to try out how the model works with the data."
   ],
   "id": "4bfa411557c18a2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:50.779705Z",
     "start_time": "2024-12-04T17:48:50.494298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Tokenization\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Initialize the fast tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ],
   "id": "3aeab84bb699e614",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heddafiedler/anaconda3/envs/dl_project/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:50.811849Z",
     "start_time": "2024-12-04T17:48:50.809024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"This is a test\"\n",
    "tokenized = tokenizer(text, truncation=True, return_tensors=\"pt\")\n",
    "print(tokenized)"
   ],
   "id": "87f5ba0464052811",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Initialization with Task and Subtask Classes\n",
    "Since the MTL approach is about combining different tasks, the data needs to contain the information bout which model head it needs in the model pipeline.\n",
    "I will use the Task and Subtask classes from the MAGPIE Repository to create the tasks I want to use. The tasks are defined in the following part.\n",
    "\n",
    "The Sub Tasks I am going to use according to the datasets I chose are:\n",
    "- Token-Level Classification (POS) --> noch besser verstehen was das genau ist\n",
    "- Binary Classification\n",
    "- Multi-Class Classification\n",
    "- (Regression) - not used in current implementation\n",
    "- (Masked Language Modelling) - not used in current implementation\n",
    "\n",
    "The Subtask class defines how to load and structure the respective data set, as well as other functions like weight scaling and class weights for imbalanced datasets\n",
    "\n",
    "The Task class is a wrapper for the subtasks and contains the task id and the subtasks list. Since I am using only one dataset for each subtask, the subtask list contains only one subtask. Nevertheless, I am leaving the wrapper in the code in order to use the other steps in the same way as in the repository.\n",
    "\n",
    "The SubTaskDataset class creates then the actual data loaders and also contains the BatchList class for Training and for Evaluation.\n",
    "\n"
   ],
   "id": "a01c107148bf6d4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:53.149474Z",
     "start_time": "2024-12-04T17:48:53.146223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from config.config import DEV_RATIO, MAX_LENGTH, REGRESSION_SCALAR, TRAIN_RATIO\n",
    "from old_utils.common import get_class_weights\n",
    "from old_utils.enums import Split\n",
    "from old_utils.common import set_random_seed\n",
    "from old_utils.logger import general_logger\n"
   ],
   "id": "5bf648f5c7dc733c",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:53.442791Z",
     "start_time": "2024-12-04T17:48:53.438826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"This part contains the Task class.\"\"\"\n",
    "\n",
    "class Task:\n",
    "    \"\"\"Wrap subtasks.\"\"\"\n",
    "\n",
    "    def __init__(self, task_id, subtasks_list):\n",
    "        \"\"\"Initialize a Task.\"\"\"\n",
    "        self.task_id = task_id\n",
    "        self.subtasks_list = subtasks_list\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a task.\"\"\"\n",
    "        return (\n",
    "            f\"Task {self.task_id} with {len(self.subtasks_list)} subtask{'s' if len(self.subtasks_list) > 1 else ''}\"\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.task_id)"
   ],
   "id": "8c71e739a9294f01",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:53.708735Z",
     "start_time": "2024-12-04T17:48:53.681999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"This part contains the Subtask.\"\"\"\n",
    "\n",
    "def get_pos_idxs(pos: str, text: str):\n",
    "    \"\"\"\n",
    "    Get the correct idxs of the pos for a given text.\n",
    "\n",
    "    @param pos: A pattern as text.\n",
    "    @param text: The text to search trough.\n",
    "    @return: The ids of the tokens in the text that match the pattern.\n",
    "    \"\"\"\n",
    "    if pos == text:\n",
    "        mask = np.array(np.ones((len(text))), dtype=\"int\")\n",
    "    else:\n",
    "        pos = pos.replace(\"[\", \"\\[\")\n",
    "        pos = pos.replace(\"$\", \"\\$\")\n",
    "        pos = pos.replace(\"?\", \"\\?\")\n",
    "        pos = pos.replace(\")\", \"\\)\")\n",
    "        pos = pos.replace(\"(\", \"\\(\")\n",
    "        pos = pos.replace(\"*\", \"\\*\")\n",
    "        pos = pos.replace(\"+\", \"\\+\")\n",
    "        start, end = re.search(pos, text).span()\n",
    "\n",
    "        mask = np.zeros((len(text)), dtype=int)\n",
    "        mask[start:end] = 1\n",
    "    c, idx_list = 0, []\n",
    "    for t in text.split():\n",
    "        idx_list.append(c)\n",
    "        c += len(t) + 1\n",
    "    mask_idxs = [mask[i] for i in idx_list]\n",
    "    return mask_idxs\n",
    "\n",
    "\n",
    "def align_labels_with_tokens(labels: List[int], word_ids: List[int]):\n",
    "    \"\"\"Align labels with tokens.\n",
    "\n",
    "    C/p from https://huggingface.co/course/chapter7/2\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = (\n",
    "                -100 if word_id is None else labels[word_id]\n",
    "            )  # -100 is an index that will be ignored by cross entropy\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def get_tokens_and_labels(pos_list_list, text_list, labels):\n",
    "    \"\"\"Get tokens and labels for scattered POS.\n",
    "\n",
    "    In this objective, we have a list of consecutive spans.\n",
    "    For each of these consecutive spans, find the correct index of the corresponding tokens in the text_list.\n",
    "    Returns the bitwise or ('union') of this ids.\n",
    "    \"\"\"\n",
    "    mask_idxs_list = []\n",
    "    for i, pos_list in enumerate(pos_list_list):\n",
    "        label = labels[i]\n",
    "        text = text_list[i]\n",
    "        observation_mask_idxs = []\n",
    "        for pos in pos_list:\n",
    "            if len(pos) == 0:\n",
    "                # If there is no POS, we just return zeros\n",
    "                observation_mask_idxs.append(get_pos_idxs(\"\", text))\n",
    "            else:\n",
    "                for pos in pos_list:\n",
    "                    if label == 0:  # In that case, the label is the neutral class\n",
    "                        observation_mask_idxs.append(get_pos_idxs(pos, text))\n",
    "                    else:\n",
    "                        pos_idxs = get_pos_idxs(pos, text)\n",
    "                        pos_idxs = [label if idx == 1 else 0 for idx in pos_idxs]\n",
    "                        observation_mask_idxs.append(pos_idxs)\n",
    "\n",
    "        # reduce observation_mask_idxs\n",
    "        observation_mask_idxs = np.bitwise_or.reduce(observation_mask_idxs, axis=0)\n",
    "        mask_idxs_list.append(observation_mask_idxs)\n",
    "\n",
    "    return [t.split() for t in text_list], mask_idxs_list\n",
    "\n",
    "\n",
    "class SubTask:\n",
    "    \"\"\"A Subtask.\"\"\"\n",
    "\n",
    "    def __init__(self, id, task_id, filename, src_col=\"text\", tgt_cols_list=[\"label\"], *args, **kwargs):\n",
    "        \"\"\"Raise RuntimeError if this SubTask is instantiated.\"\"\"\n",
    "        general_logger.info(f\"Initializing SubTask {id} for task {task_id}\")\n",
    "        if type(self) == SubTask:\n",
    "            raise RuntimeError(\"Abstract class <SubTask> must not be instantiated.\")\n",
    "        self.attention_masks = None\n",
    "        self.Y = None\n",
    "        self.X = None\n",
    "        self.class_weights = None\n",
    "        self.id = id\n",
    "        self.src_col = src_col\n",
    "        self.tgt_cols_list = tgt_cols_list\n",
    "        self.task_id = task_id\n",
    "        self.filename = os.path.join(\"datasets\", filename)\n",
    "        self.processed = False\n",
    "\n",
    "    def process(self, force_download: bool = False):\n",
    "        \"\"\"Process a SubTask.\n",
    "\n",
    "        Load the data for this subtask, set properties X, Y and attention_mask.\n",
    "        \"\"\"\n",
    "        general_logger.info(f\"Processing SubTask {self.id}\")\n",
    "        X, Y, attention_masks = self.load_data()\n",
    "\n",
    "        train_split = int(len(X) * TRAIN_RATIO)\n",
    "        dev_split = train_split + int(len(X) * DEV_RATIO)\n",
    "\n",
    "        self.X = {Split.TRAIN: X[:train_split], Split.DEV: X[train_split:dev_split], Split.TEST: X[dev_split:]}\n",
    "        self.attention_masks = {\n",
    "            Split.TRAIN: attention_masks[:train_split],\n",
    "            Split.DEV: attention_masks[train_split:dev_split],\n",
    "            Split.TEST: attention_masks[dev_split:],\n",
    "        }\n",
    "        self.Y = {Split.TRAIN: Y[:train_split], Split.DEV: Y[train_split:dev_split], Split.TEST: Y[dev_split:]}\n",
    "        self.create_class_weights()\n",
    "        self.processed = True\n",
    "        general_logger.info(f\"SubTask {self.id} processed successfully\")\n",
    "\n",
    "    def load_data(self) -> Tuple:\n",
    "        \"\"\"Load the data of a SubTask.\n",
    "\n",
    "        Must be implemented for inherited.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_class_weights(self):\n",
    "        \"\"\"Compute the weights for imbalanced classes.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_scaling_weight(self):\n",
    "        \"\"\"Get the scaling weight of a Subtask.\n",
    "\n",
    "        Needs to be overwritten.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_X(self, split: Split):\n",
    "        \"\"\"Get all X of a given split.\"\"\"\n",
    "        return self.X[split]\n",
    "\n",
    "    def get_att_mask(self, split: Split):\n",
    "        \"\"\"Get attention_masks for inputs of a given split.\"\"\"\n",
    "        return self.attention_masks[split]\n",
    "\n",
    "    def get_Y(self, split: Split):\n",
    "        \"\"\"Get all Y of a given split.\"\"\"\n",
    "        return self.Y[split]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.id)\n",
    "\n",
    "\n",
    "# a[43485:43500]\n",
    "class ClassificationSubTask(SubTask):\n",
    "    \"\"\"A ClassificationSubTask.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=2, *args, **kwargs):\n",
    "        \"\"\"Initialize a ClassificationSubTask.\"\"\"\n",
    "        super(ClassificationSubTask, self).__init__(num_classes=num_classes, *args, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def load_data(self) -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"Load the data of a ClassificationSubTask.\"\"\"\n",
    "        general_logger.info(f\"Loading data from {self.filename}\")\n",
    "        df = pd.read_csv(self.filename)\n",
    "\n",
    "        X, Y = df[self.src_col], df[self.tgt_cols_list]\n",
    "        tokenized_inputs = tokenizer(X.to_list(), padding=\"max_length\", truncation=True,\n",
    "                                     max_length=MAX_LENGTH)\n",
    "        X = tokenized_inputs.get(\"input_ids\")\n",
    "        attention_masks = tokenized_inputs.get(\"attention_mask\")\n",
    "        assert Y.nunique().squeeze() == self.num_classes\n",
    "        assert Y[self.tgt_cols_list[0]].min(axis=0) == 0\n",
    "        if self.num_classes == 2:  # if it's binary classification\n",
    "            Y = Y.to_numpy()\n",
    "        else:\n",
    "            Y = Y[self.tgt_cols_list].to_numpy()\n",
    "        general_logger.info(f\"Data loaded successfully: {len(X)} samples\")\n",
    "        return torch.LongTensor(X), torch.LongTensor(Y), torch.LongTensor(attention_masks)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a Classification Subtask.\"\"\"\n",
    "        return f\"{'Multi-class' if self.num_classes != 2 else 'Binary'} Classification\"\n",
    "\n",
    "    def create_class_weights(self):\n",
    "        \"\"\"Compute the weights.\"\"\"\n",
    "        self.class_weights = get_class_weights(self.Y[Split.TRAIN], method=\"isns\")\n",
    "\n",
    "    def get_scaling_weight(self):\n",
    "        \"\"\"Get the weight of a Classification Subtask.\n",
    "\n",
    "        As with the other tasks, we normalize by the natural logarithm of the domain size.\n",
    "        \"\"\"\n",
    "        return 1 / np.log(self.num_classes)\n",
    "\n",
    "\n",
    "# in current implementation, the regression subtask is not used\n",
    "class RegressionSubTask(SubTask):\n",
    "    \"\"\"A RegressionSubTask.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize a RegressionSubTask.\"\"\"\n",
    "        super(RegressionSubTask, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def load_data(self) -> Tuple[torch.LongTensor, torch.FloatTensor, torch.LongTensor]:\n",
    "        \"\"\"Load the data of a RegressionSubTask.\"\"\"\n",
    "        general_logger.info(f\"Loading data from {self.filename}\")\n",
    "        df = pd.read_csv(self.filename)\n",
    "        X, Y = df[self.src_col], df[self.tgt_cols_list]\n",
    "        tokenized_inputs = tokenizer(X.to_list(), padding=\"max_length\", truncation=True,\n",
    "                                     max_length=MAX_LENGTH)\n",
    "        X = tokenized_inputs.get(\"input_ids\")\n",
    "        attention_masks = tokenized_inputs.get(\"attention_mask\")\n",
    "        Y = (((Y - Y.min()) / (Y.max() - Y.min())).to_numpy()).astype(\"float32\")  # scale from 0 to 1\n",
    "        general_logger.info(f\"Data loaded successfully: {len(X)} samples\")\n",
    "        return torch.LongTensor(X), torch.FloatTensor(Y), torch.LongTensor(attention_masks)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a Regression Subtask.\"\"\"\n",
    "        return \"Regression\"\n",
    "\n",
    "    def get_scaling_weight(self):\n",
    "        \"\"\"Get the scaling weight of a Regression Subtask.\n",
    "\n",
    "        As of now, this scaling weight is a simple scalar and is a mere heuristic-based approximation (ie. we eyeballed it).\n",
    "        \"\"\"\n",
    "        return REGRESSION_SCALAR\n",
    "\n",
    "\n",
    "class MultiLabelClassificationSubTask(SubTask):\n",
    "    \"\"\"A MultiLabelClassificationSubTask.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=2, num_labels=2, *args, **kwargs):\n",
    "        \"\"\"Initialize a MultiLabelClassificationSubTask.\"\"\"\n",
    "        super(MultiLabelClassificationSubTask, self).__init__(num_classes=2, num_labels=2, *args, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def load_data(self) -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"Load the data of a MultiLabelClassificationSubTask.\"\"\"\n",
    "        general_logger.info(f\"Loading data from {self.filename}\")\n",
    "        df = pd.read_csv(self.filename)\n",
    "        X, Y = df[self.src_col], df[self.tgt_cols_list]\n",
    "        tokenized_inputs = tokenizer(X.to_list(), padding=\"max_length\", truncation=True,\n",
    "                                     max_length=MAX_LENGTH)\n",
    "        X = tokenized_inputs.get(\"input_ids\")\n",
    "        attention_masks = tokenized_inputs.get(\"attention_mask\")\n",
    "        assert Y.max(axis=0).to_numpy().max() == 1\n",
    "        Y = Y.to_numpy()\n",
    "        general_logger.info(f\"Data loaded successfully: {len(X)} samples\")\n",
    "        return torch.LongTensor(X), torch.LongTensor(Y), torch.LongTensor(attention_masks)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a Multi-label Classification Subtask.\"\"\"\n",
    "        return \"Multi-label Classification\"\n",
    "\n",
    "    def get_scaling_weight(self):\n",
    "        \"\"\"Get the weight of a Multi-label Classification Subtask.\n",
    "\n",
    "        As with the other tasks, we normalize by the natural logarithm of the domain size.\n",
    "        \"\"\"\n",
    "        return 1 / np.log(self.num_classes * self.num_labels)\n",
    "\n",
    "\n",
    "class POSSubTask(SubTask):\n",
    "    \"\"\"A POSSubTask.\n",
    "\n",
    "    Each POSSubTask can be either binary classification or multiclass classification.\n",
    "    If it is binary classification, zero (0) must be the neutral class.\n",
    "    This neutral class is also applied to all other, 'normal' tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tgt_cols_list, label_col=None, *args, **kwargs):\n",
    "        \"\"\"Initialize a POSSubTask.\n",
    "\n",
    "        Normally, we have 3 classes: (0=no-tag, 1=tag-start, 2=tag-continue)\n",
    "        However, we have POS-tasks where we have more than just 'binary token level classification'.\n",
    "        In these scenarios, each class has two tags: 'tag-start' and 'tag-continue'.\n",
    "        The 'no-class' tag has no 'tag-continue'.\n",
    "        \"\"\"\n",
    "        super(POSSubTask, self).__init__(tgt_cols_list=tgt_cols_list, *args, **kwargs)\n",
    "        self.num_classes = 3   # The default num_classes is 2 or 3 (0=no-tag, 1=tag-start, 2=tag-continue)\n",
    "        self.label_col = label_col\n",
    "        assert len(tgt_cols_list) == 1\n",
    "\n",
    "    def load_data(self) -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"Load the data of a POSSubTask.\"\"\"\n",
    "        general_logger.info(f\"Loading data from {self.filename}\")\n",
    "        df = pd.read_csv(self.filename)\n",
    "\n",
    "        df[self.tgt_cols_list] = df[self.tgt_cols_list].fillna(\"\")\n",
    "        mask = df.apply(\n",
    "            lambda row: all([p in row[self.src_col] for p in row[self.tgt_cols_list[0]].split(\";\")]), axis=1\n",
    "        )\n",
    "        df = df[mask].reset_index(drop=True)\n",
    "        assert sum(mask) == len(df[self.tgt_cols_list]), \"At least one POS is not contained in the source column.\"\n",
    "\n",
    "        pos_list_list = df[self.tgt_cols_list[0]].apply(lambda x: x.split(\";\")).to_list()\n",
    "        X = df[self.src_col].values\n",
    "        # If we do not provide a labels column, we assume that, whenever a pos is present, that is the non-neutral class\n",
    "        labels = (\n",
    "            df[self.label_col]\n",
    "            if self.label_col\n",
    "            else [1 if len(pos) > 0 else 0 for pos in df[self.tgt_cols_list[0]].to_list()]\n",
    "        )\n",
    "        tokens, labels = get_tokens_and_labels(pos_list_list=pos_list_list, text_list=X, labels=labels)\n",
    "        tokenized_inputs = tokenizer(\n",
    "            tokens, padding=\"max_length\", is_split_into_words=True, truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        new_labels = []\n",
    "        for i, labels in enumerate(labels):\n",
    "            word_ids = tokenized_inputs.word_ids(i)\n",
    "            new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        Y = np.array(new_labels)\n",
    "        # This should in most cases not alter self.num_classes, as we only use binary tags (+ tag-continue = 3 classes).\n",
    "        # However, we leave this generic implementation for future tasks.\n",
    "        self.num_classes = len(np.unique(Y)) - 1\n",
    "        X = tokenized_inputs.get(\"input_ids\")\n",
    "        attention_masks = tokenized_inputs.get(\"attention_mask\")\n",
    "        general_logger.info(f\"Data loaded successfully: {len(X)} samples\")\n",
    "        return torch.LongTensor(X), torch.LongTensor(Y), torch.LongTensor(attention_masks)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a Token-level classification Subtask.\"\"\"\n",
    "        return \"Token-level classification\"\n",
    "\n",
    "    def create_class_weights(self):\n",
    "        \"\"\"Compute the weights.\"\"\"\n",
    "        labels = self.Y[Split.TRAIN]\n",
    "        only_class_labels = labels[labels != -100]\n",
    "        self.class_weights = get_class_weights(only_class_labels, method=\"isns\")\n",
    "\n",
    "    def get_scaling_weight(self):\n",
    "        \"\"\"Get the weight of a POS Subtask.\n",
    "\n",
    "        As with the other tasks, we normalize by the natural logarithm of the domain size.\n",
    "        In case of POS subtask, the domain size equals the vocab size.\n",
    "        \"\"\"\n",
    "        return 1 / np.log(self.num_classes)\n",
    "\n",
    "# not used in current implementation, but important for testing?\n",
    "class MLMSubTask(SubTask):\n",
    "    \"\"\"A Masked Language Modelling Subtask.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize a MLMSubTask.\"\"\"\n",
    "        super(MLMSubTask, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def load_data(self) -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]:\n",
    "        \"\"\"Load the data of a MLMSubTask.\"\"\"\n",
    "        general_logger.info(f\"Loading data from {self.filename}\")\n",
    "        df = pd.read_csv(self.filename)\n",
    "        X = df[self.src_col]\n",
    "        tokenized_inputs = tokenizer(X.to_list(), padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "        X = torch.LongTensor(tokenized_inputs.get(\"input_ids\"))\n",
    "        attention_masks = tokenized_inputs.get(\"attention_mask\")\n",
    "\n",
    "        MASK_TOKEN = tokenizer.mask_token_id\n",
    "        SEP_TOKEN = tokenizer.sep_token_id\n",
    "        CLS_TOKEN = tokenizer.cls_token_id\n",
    "        PAD_TOKEN = tokenizer.pad_token_id\n",
    "\n",
    "        Y = X.clone()\n",
    "        rand = torch.rand(X.shape)\n",
    "        masking_mask = (rand < 0.15) * (X != SEP_TOKEN) * (X != CLS_TOKEN) * (X != PAD_TOKEN)\n",
    "        X[masking_mask] = MASK_TOKEN\n",
    "        Y[~masking_mask] = -100\n",
    "        general_logger.info(f\"Data loaded successfully: {len(X)} samples\")\n",
    "        return torch.LongTensor(X), torch.LongTensor(Y), torch.LongTensor(attention_masks)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a MLM Subtask.\"\"\"\n",
    "        return \"Masked Language Modelling\"\n",
    "\n",
    "    def get_scaling_weight(self):\n",
    "        \"\"\"Get the weights for imbalanced classes.\"\"\"\n",
    "        return 1 / np.log(len(tokenizer))"
   ],
   "id": "2d702ae4dd058b68",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:53.891352Z",
     "start_time": "2024-12-04T17:48:53.882570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"This module contains the SubTaskDataset.\"\"\"\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from old_utils.logger import general_logger\n",
    "from old_utils.enums import Split\n",
    "\n",
    "\n",
    "class SubTaskDataset(Dataset):\n",
    "    \"\"\"A Datset for a single SubTask.\"\"\"\n",
    "\n",
    "    def __init__(self, subtask: SubTask, split: Split):\n",
    "        \"\"\"Initialize a SubTaskDataset.\"\"\"\n",
    "        general_logger.info(f\"Initializing dataset for subtask {subtask.id} with split {split}\")\n",
    "        self.split = split\n",
    "        self.subtask = subtask\n",
    "        self.observations: List = []\n",
    "        self._reset()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the length of the Dataset.\"\"\"\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Get the next observation from the Dataset.\"\"\"\n",
    "        if self._counter == len(self.observations):\n",
    "            self._reset()\n",
    "        i = self.observations[self._counter]\n",
    "        x = self.subtask.get_X(split=self.split)[i]\n",
    "        masks = self.subtask.get_att_mask(split=self.split)[i]\n",
    "        y = self.subtask.get_Y(split=self.split)[i]\n",
    "        self._counter += 1\n",
    "        return x, masks, y, self.subtask.id\n",
    "\n",
    "    def _reset(self):\n",
    "        general_logger.info(f\"Resetting dataset for subtask {self.subtask.id}\")\n",
    "        self.observations = [i for i in range(len(self.subtask.get_X(split=self.split)))]\n",
    "        set_random_seed()\n",
    "        np.random.shuffle(self.observations)  # Not a real 'reshuffling' as it will always arrange same.\n",
    "        self._counter = 0\n",
    "\n",
    "\n",
    "class BatchList:\n",
    "    \"\"\"A BatchList is a wrapper around dataloaders for each subtask.\n",
    "\n",
    "    This BatchList will never stop; it will always yield super-batches containing one sub-batch per task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, subtask_list: List[SubTask], sub_batch_size, split=Split.TRAIN):\n",
    "        \"\"\"Initialize a BatchList.\"\"\"\n",
    "        general_logger.info(f\"Creating BatchList with {len(subtask_list)} subtasks, batch size {sub_batch_size}\")\n",
    "        self.sub_batch_size = sub_batch_size\n",
    "        self.datasets = {f\"{st.id}\": SubTaskDataset(subtask=st, split=split) for st in subtask_list}\n",
    "        self.dataloaders = {\n",
    "            f\"{st_id}\": DataLoader(ds, batch_size=self.sub_batch_size) for st_id, ds in self.datasets.items()\n",
    "        }\n",
    "        self.iter_dataloaders = {f\"{st_id}\": iter(dl) for st_id, dl in self.dataloaders.items()}\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Yield a batch of sub-batches.\"\"\"\n",
    "        data = []\n",
    "        items = list(self.iter_dataloaders.items())  # List of tuples of (key,values)\n",
    "        random.shuffle(items)\n",
    "        for st_id, dl in items:\n",
    "            try:\n",
    "                batch = next(dl)\n",
    "            except StopIteration:\n",
    "                self.iter_dataloaders[st_id] = iter(self.dataloaders[st_id])  # Reset the iter_dataloader\n",
    "                batch = next(self.iter_dataloaders[st_id])\n",
    "            data.append(batch)\n",
    "        general_logger.info(f\"Generated batch with {len(data)} sub-batches\")\n",
    "        return data  # Batch contains Sub-batches.\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset this BatchListEvalTest.\"\"\"\n",
    "        self.iter_dataloaders = {f\"{st_id}\": iter(dl) for st_id, dl in self.dataloaders.items()}\n",
    "\n",
    "class BatchListEvalTest:\n",
    "    \"\"\"A BatchListEvalTest is a wrapper around dataloaders for each subtask.\n",
    "\n",
    "    If one task is exhausted, it will stop yielding sub-batches from this task.\n",
    "    Instead, it will continue until it has yielded all sub-batches from all tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, subtask_list: List[SubTask], sub_batch_size, split=Split.TRAIN):\n",
    "        \"\"\"Initialize a BatchList.\"\"\"\n",
    "        general_logger.info(f\"Creating BatchListEvalTest with {len(subtask_list)} subtasks\")\n",
    "        self.sub_batch_size = sub_batch_size\n",
    "        self.datasets = {f\"{st.id}\": SubTaskDataset(subtask=st, split=split) for st in subtask_list}\n",
    "        self.dataloaders = {\n",
    "            f\"{st_id}\": DataLoader(ds, batch_size=self.sub_batch_size) for st_id, ds in self.datasets.items()\n",
    "        }\n",
    "        self.iter_dataloaders = {f\"{st_id}\": iter(dl) for st_id, dl in self.dataloaders.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of this BatchListEvalTest.\n",
    "\n",
    "        The length is the maximum length of all subtask-datadloaders.\n",
    "        \"\"\"\n",
    "        return sum([len(dl) for dl in self.dataloaders.values()])\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset this BatchListEvalTest.\"\"\"\n",
    "        self.iter_dataloaders = {f\"{st_id}\": iter(dl) for st_id, dl in self.dataloaders.items()}"
   ],
   "id": "a4d50837491e4497",
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:54.259477Z",
     "start_time": "2024-12-04T17:48:54.250221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "# initializing the sub-tasks I want to use\n",
    "st_1_cw_hard_03 = ClassificationSubTask(\n",
    "task_id=3,\n",
    "filename=\"03_CW_HARD/preprocessed.csv\",\n",
    "id=300001)\n",
    "st_1_me_too_ma_108 = MultiLabelClassificationSubTask(\n",
    "num_classes=2,\n",
    "num_labels=2,\n",
    "task_id=108,\n",
    "filename=\"108_MeTooMA/preprocessed.csv\",\n",
    "id=10801,\n",
    "tgt_cols_list=[\"hate_speech_label\", \"sarcasm_label\"],\n",
    ")\n",
    "st_1_mdgender_116 = ClassificationSubTask(\n",
    "task_id=116,\n",
    "id=11601,\n",
    "filename=\"116_MDGender/preprocessed.csv\",\n",
    "num_classes=6\n",
    ")\n",
    "st_1_mpqa_103 = ClassificationSubTask(\n",
    "task_id=103,\n",
    "id=10301,\n",
    "filename=\"103_MPQA/preprocessed.csv\")\n",
    "st_1_stereotype_109 = ClassificationSubTask(\n",
    "task_id=109,\n",
    "id=10901,\n",
    "filename=\"109_stereotype/preprocessed.csv\")\n",
    "st_2_stereotype_109 = MultiLabelClassificationSubTask(\n",
    "task_id=109,\n",
    "id=10902,\n",
    "filename=\"109_stereotype/preprocessed.csv\",\n",
    "tgt_cols_list=[\"stereotype_explicit_label\", \"stereotype_explicit_label\"],\n",
    "num_classes=2,\n",
    "num_labels=2,\n",
    ")\n",
    "st_1_good_news_everyone_42 = POSSubTask(\n",
    "tgt_cols_list=[\"cue_pos\"],\n",
    "task_id=42,\n",
    "id=42001,\n",
    "filename=\"42_GoodNewsEveryone/preprocessed.csv\"\n",
    ")\n",
    "st_2_good_news_everyone_42 = POSSubTask(\n",
    "tgt_cols_list=[\"experiencer_pos\"],\n",
    "task_id=42,\n",
    "id=42002,\n",
    "filename=\"42_GoodNewsEveryone/preprocessed.csv\",\n",
    ")\n",
    "st_1_pheme_12 = ClassificationSubTask(\n",
    "task_id=12,\n",
    "id=12001,\n",
    "filename=\"12_PHEME/preprocessed.csv\")\n",
    "st_2_pheme_12 = ClassificationSubTask(\n",
    "task_id=12,\n",
    "id=12002,\n",
    "filename=\"12_PHEME/preprocessed.csv\",\n",
    "tgt_cols_list=[\"veracity_label\"],\n",
    "num_classes=3,\n",
    ")\n",
    "st_1_babe_10 = ClassificationSubTask(\n",
    "task_id=10,\n",
    "id=10001,\n",
    "filename=\"10_BABE/preprocessed.csv\",\n",
    "num_classes=2)\n",
    "st_2_babe_10 = POSSubTask(\n",
    "task_id=10,\n",
    "id=10002,\n",
    "filename=\"10_BABE/preprocessed.csv\",\n",
    "tgt_cols_list=[\"biased_words\"])\n",
    "st_1_gwsd_128 = ClassificationSubTask(\n",
    "task_id=128,\n",
    "num_classes=3,\n",
    "filename=\"128_GWSD/preprocessed.csv\",\n",
    "id=12801)\n",
    "\n",
    "# Tasks\n",
    "cw_hard_03 = Task(task_id=3, subtasks_list=[st_1_cw_hard_03])\n",
    "babe_10 = Task(task_id=10, subtasks_list=[st_1_babe_10, st_2_babe_10])\n",
    "me_too_ma_108 = Task(task_id=108, subtasks_list=[st_1_me_too_ma_108])\n",
    "mdgender_116 = Task(task_id=116, subtasks_list=[st_1_mdgender_116])\n",
    "pheme_12 = Task(task_id=12, subtasks_list=[st_2_pheme_12, st_1_pheme_12])\n",
    "mpqa_103 = Task(task_id=103, subtasks_list=[st_1_mpqa_103])\n",
    "stereotype_109 = Task(task_id=109, subtasks_list=[st_1_stereotype_109,\n",
    "                                              st_2_stereotype_109])\n",
    "good_news_everyone_42 = Task(task_id=42,\n",
    "                         subtasks_list=[st_1_good_news_everyone_42,\n",
    "                                        st_2_good_news_everyone_42])\n",
    "gwsd_128 = Task(task_id=128, subtasks_list=[st_1_gwsd_128])\n",
    "\n",
    "\n",
    "# MBIB ###\n",
    "# st_linguistic = ClassificationSubTask(task_id=11111, id=11111, filename=\"mbib_linguistic/preprocessed.csv\", num_classes=2)\n",
    "# mbib_lingustic = Task(task_id=11111, subtasks_list=[st_linguistic])\n",
    "\n",
    "# Create task object\n",
    "all_tasks = [\n",
    "babe_10,\n",
    "cw_hard_03,\n",
    "me_too_ma_108,\n",
    "pheme_12,\n",
    "mdgender_116,\n",
    "mpqa_103,\n",
    "stereotype_109,\n",
    "good_news_everyone_42,\n",
    "gwsd_128,\n",
    "]\n",
    "\n",
    "# Get all subtasks\n",
    "all_subtasks = list(itertools.chain.from_iterable(t.subtasks_list for t in all_tasks))\n",
    "\n",
    "# Task families\n",
    "media_bias = [babe_10]\n",
    "subjective_bias = [cw_hard_03]\n",
    "hate_speech = [me_too_ma_108]\n",
    "gender_bias = [mdgender_116]\n",
    "sentiment_analysis = [mpqa_103]\n",
    "fake_news = [pheme_12]\n",
    "group_bias = [stereotype_109]\n",
    "emotionality = [good_news_everyone_42]\n",
    "stance_detection = [gwsd_128]\n",
    "#mlm = [mlm_0]"
   ],
   "id": "6b35e68946eadf4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-04 18:48:54,254: INFO: 3233285385: Initializing SubTask 300001 for task 3]\n",
      "[2024-12-04 18:48:54,254: INFO: 3233285385: Initializing SubTask 10801 for task 108]\n",
      "[2024-12-04 18:48:54,255: INFO: 3233285385: Initializing SubTask 11601 for task 116]\n",
      "[2024-12-04 18:48:54,255: INFO: 3233285385: Initializing SubTask 10301 for task 103]\n",
      "[2024-12-04 18:48:54,256: INFO: 3233285385: Initializing SubTask 10901 for task 109]\n",
      "[2024-12-04 18:48:54,256: INFO: 3233285385: Initializing SubTask 10902 for task 109]\n",
      "[2024-12-04 18:48:54,256: INFO: 3233285385: Initializing SubTask 42001 for task 42]\n",
      "[2024-12-04 18:48:54,256: INFO: 3233285385: Initializing SubTask 42002 for task 42]\n",
      "[2024-12-04 18:48:54,257: INFO: 3233285385: Initializing SubTask 12001 for task 12]\n",
      "[2024-12-04 18:48:54,257: INFO: 3233285385: Initializing SubTask 12002 for task 12]\n",
      "[2024-12-04 18:48:54,257: INFO: 3233285385: Initializing SubTask 10001 for task 10]\n",
      "[2024-12-04 18:48:54,258: INFO: 3233285385: Initializing SubTask 10002 for task 10]\n",
      "[2024-12-04 18:48:54,258: INFO: 3233285385: Initializing SubTask 12801 for task 128]\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Building the Model\n",
    "After initializing the datasets I want to use in the last step, I now build the model:\n",
    "- The backbone is changes to DistilBERT as it is a smaller model and therefore faster to train.\n",
    "- For each task a specific model ehad is needed to fulfill the task.For this a head factory is used to decide which head to use for the specific task type.\n",
    "- Apart from that the model needs a GradsWrapper to get and set the gradients of the weights and biases of all trainable layers.\n",
    "- In the model factory the model is then instantiated by combining the backbone with the different model heads for the different tasks."
   ],
   "id": "a4e6c80fbf45a85d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:55.175505Z",
     "start_time": "2024-12-04T17:48:55.172159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict\n",
    "from torch import nn\n",
    "from old_utils.common import rsetattr\n",
    "\n",
    "class GradsWrapper(nn.Module):\n",
    "    \"\"\"Abstract class for a GradsWrapper.\n",
    "\n",
    "    This class must be extended and not instantiated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Raise RuntimeError if this class is instantiated.\"\"\"\n",
    "        if type(self) == GradsWrapper:\n",
    "            raise RuntimeError(\"Abstract class <GradsWrapper> must not be instantiated.\")\n",
    "        super(GradsWrapper, self).__init__()\n",
    "\n",
    "    def get_grads(self) -> Dict:\n",
    "        \"\"\"Get the gradients of the weights and biases of all trainable layers.\"\"\"\n",
    "        return {k: v.grad.clone() if v.grad is not None else None for k, v in dict(self.named_parameters()).items()}\n",
    "\n",
    "    def set_grads(self, grads: Dict):\n",
    "        \"\"\"Set the gradients of the weights and biases of all trainable layers.\"\"\"\n",
    "        for k, v in grads.items():\n",
    "            rsetattr(self, f\"{k}.grad\", v)"
   ],
   "id": "de475ddd25fedd07",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:56.019060Z",
     "start_time": "2024-12-04T17:48:55.838806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"This module contains the implementation of the heads for specific tasks as well a factory-method for deciding which head to use.\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torchmetrics import Accuracy, F1Score, MeanSquaredError, Perplexity, R2Score\n",
    "\n",
    "\n",
    "\n",
    "def HeadFactory(st: SubTask, *args, **kwargs):\n",
    "    \"\"\"Decide which head to use for the specific task type\n",
    "       st: subtask\"\"\"\n",
    "    if isinstance(st, ClassificationSubTask):\n",
    "        return ClassificationHead(num_classes=st.num_classes, class_weights=st.class_weights, *args, **kwargs)\n",
    "    elif isinstance(st, MultiLabelClassificationSubTask):\n",
    "        return ClassificationHead(\n",
    "            num_classes=st.num_classes, num_labels=st.num_labels, class_weights=st.class_weights, *args, **kwargs)\n",
    "    elif isinstance(st, POSSubTask):\n",
    "        return TokenClassificationHead(num_classes=st.num_classes, class_weights=st.class_weights, *args, **kwargs)\n",
    "    elif isinstance(st, RegressionSubTask):\n",
    "        return RegressionHead(*args, **kwargs)\n",
    "    elif isinstance(st, MLMSubTask):\n",
    "        return LanguageModellingHead(*args, **kwargs)\n",
    "\n",
    "\n",
    "class ClassificationHead(GradsWrapper):\n",
    "    \"\"\"Classifier inspired by one used in RoBERTa.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dimension: int,\n",
    "        hidden_dimension: int,\n",
    "        dropout_prob: float,\n",
    "        num_classes=2,\n",
    "        num_labels=1,\n",
    "        class_weights=None,\n",
    "    ):\n",
    "        \"\"\"Initialize the head.\"\"\"\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dimension, hidden_dimension)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.out_proj = nn.Linear(hidden_dimension, num_classes * num_labels)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_labels = num_labels\n",
    "        self.loss = CrossEntropyLoss(weight=class_weights)\n",
    "        self.metrics = {\n",
    "            \"f1\": F1Score(num_classes=num_classes, mdmc_reduce=\"global\", average=\"macro\"),\n",
    "            \"acc\": Accuracy(mdmc_reduce=\"global\"),\n",
    "        }\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        \"\"\"Feed the data through head accordingly to RoBERTa approach and compute loss.\"\"\"\n",
    "        batch_size = y.shape[0]  # size of data in this subbatch\n",
    "\n",
    "        x = X[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "\n",
    "        # pass CLS through classifier\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out_proj(x)\n",
    "\n",
    "        loss = self.loss(logits.view(-1, self.num_classes), y.view(-1))\n",
    "        logits = logits.view(batch_size, self.num_classes, self.num_labels)  # reshape logits into prediction\n",
    "        metrics_values = {k: metric(logits.cpu(), y.cpu()) for k, metric in self.metrics.items()}\n",
    "        return logits, loss, metrics_values\n",
    "\n",
    "\n",
    "class TokenClassificationHead(GradsWrapper):\n",
    "    \"\"\"TokenClassificationHead inspired by one used in RoBERTa.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, class_weights, hidden_dimension: int, dropout_prob: float, *args, **kwargs):\n",
    "        \"\"\"Initialize the TokenClassificationHead.\"\"\"\n",
    "        super(TokenClassificationHead, self).__init__(*args, **kwargs)\n",
    "        self.dropout_LM = nn.Dropout(p=dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_dimension, num_classes)\n",
    "        self.loss = CrossEntropyLoss(weight=class_weights)\n",
    "        self.num_classes = num_classes\n",
    "        self.metrics = {\n",
    "            \"f1\": F1Score(num_classes=num_classes, mdmc_reduce=\"global\", average=\"macro\"),\n",
    "            \"acc\": Accuracy(mdmc_reduce=\"global\"),\n",
    "        }\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        \"\"\"Feed the data through head accordingly to RoBERTa approach and compute loss.\"\"\"\n",
    "        sequence_output = self.dropout_LM(X)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        loss = self.loss(logits.view(-1, self.num_classes), y.view(-1))\n",
    "\n",
    "        # Ignore class -100 when computing metrics\n",
    "        mask = torch.where(y != -100, 1, 0)\n",
    "        logits = torch.masked_select(logits, (mask.unsqueeze(-1).expand(logits.size()) == 1))\n",
    "\n",
    "        y = torch.masked_select(y, (mask == 1))\n",
    "        logits = logits.view(y.shape[0], self.num_classes)\n",
    "        metrics_values = {k: metric(logits.cpu(), y.cpu()) for k, metric in self.metrics.items()}\n",
    "\n",
    "        return logits, loss, metrics_values\n",
    "\n",
    "\n",
    "class RegressionHead(GradsWrapper):\n",
    "    \"\"\"Regression head inspired by one used in RoBERTa.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dimension: int, hidden_dimension: int, dropout_prob: float):\n",
    "        \"\"\"Initialize the RegressionHead.\"\"\"\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dimension, hidden_dimension)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.out_proj = nn.Linear(hidden_dimension, 1)\n",
    "        self.loss = MSELoss()\n",
    "        self.metrics = {\"R2\": R2Score(), \"MSE\": MeanSquaredError()}  # Needs at least 2 samples\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        \"\"\"Feed the data through head accordingly to RoBERTa approach and compute loss.\"\"\"\n",
    "        x = X[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out_proj(x)\n",
    "\n",
    "        loss = self.loss(logits.squeeze(), y.squeeze())\n",
    "\n",
    "        metrics_values = {k: metric(logits.cpu(), y.cpu()).detach() for k, metric in self.metrics.items()}\n",
    "\n",
    "        return logits, loss, metrics_values\n",
    "\n",
    "\n",
    "class LanguageModellingHead(GradsWrapper):\n",
    "    \"\"\"Roberta Head for masked language modeling.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dimension: int, hidden_dimension: int, dropout_prob: float):\n",
    "        \"\"\"Initialize LM head.\"\"\"\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dimension, hidden_dimension)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dimension, eps=1e-5)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        self.loss = CrossEntropyLoss()\n",
    "\n",
    "        # output dimension is of size of all possible tokens\n",
    "        self.decoder = nn.Linear(hidden_dimension, tokenizer.vocab_size)\n",
    "        self.bias = nn.Parameter(torch.zeros(tokenizer.vocab_size))\n",
    "        self.decoder.bias = self.bias\n",
    "        self.metrics = {\"perplexity\": Perplexity()}\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        \"\"\"Feed the data through one layer and then project to vocab size.\"\"\"\n",
    "        x = self.dense(X)\n",
    "        x = self.gelu(x)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # project back to size of vocabulary\n",
    "        logits = self.decoder(x)\n",
    "        loss = self.loss(logits.view(-1, tokenizer.vocab_size), y.view(-1))\n",
    "\n",
    "        metrics_values = {k: metric(logits.cpu(), y.cpu()) for k, metric in self.metrics.items()}\n",
    "\n",
    "        return logits, loss, metrics_values\n"
   ],
   "id": "b9ddb6cdf35bbf95",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:56.386486Z",
     "start_time": "2024-12-04T17:48:56.382999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BackboneLM(GradsWrapper):\n",
    "    \"\"\"Language encoder model which is shared across all tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Fetch Language model from huggingface.\"\"\"\n",
    "        super(BackboneLM, self).__init__()\n",
    "        from transformers import DistilBertModel\n",
    "        self.backbone = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"Torch-based module.\"\"\"\n",
    "    \n",
    "    def __init__(self, stl: List, *args, **kwargs):\n",
    "        \"\"\"Initialize model and create heads.\"\"\"\n",
    "        super().__init__()\n",
    "        self.stl = stl\n",
    "        self.subtask_id_to_subtask = {int(f\"{st.id}\"): st for st in stl}\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        \n",
    "        # Initialize the backbone language model\n",
    "        self.language_model = BackboneLM()\n",
    "        self.language_model.backbone.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        # Create the task-specific heads\n",
    "        self.heads = nn.ModuleDict({str(st.id): HeadFactory(st, *args, **kwargs) for st in stl})\n",
    "\n",
    "    def forward(self, X, attention_masks, Y, st_id):\n",
    "        \"\"\"Pass the data through the model and according head decided from heads dict.\"\"\"\n",
    "        # Pass through the backbone model\n",
    "        x_enc = self.language_model.backbone(input_ids=X, attention_mask=attention_masks).last_hidden_state\n",
    "        # Pass through the appropriate head\n",
    "        head = self.heads[str(st_id.item())]\n",
    "        logits, loss, metric_values = head(x_enc, Y)\n",
    "        return loss, metric_values"
   ],
   "id": "21c1ea37f8afd8d2",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:48:56.929966Z",
     "start_time": "2024-12-04T17:48:56.926442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Module for creating instantiating the appropriate model defined by the task list only.\"\"\"\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def ModelFactory(\n",
    "    task_list: List, \n",
    "    sub_batch_size: int, \n",
    "    eval_batch_size: int, \n",
    "    pretrained_path: str = None,\n",
    "    *args, \n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Create model and return it along with dataloaders.\"\"\"\n",
    "    # Get all subtasks from task list\n",
    "    subtask_list = [st for t in task_list for st in t.subtasks_list]\n",
    "    \n",
    "    # Verify data is processed\n",
    "    for st in subtask_list:\n",
    "        assert st.processed, \"Data must be loaded at this point.\"\n",
    "\n",
    "    # Create model\n",
    "    model = Model(stl=subtask_list, **kwargs)\n",
    "\n",
    "    if pretrained_path is not None:\n",
    "        model = load_pretrained_weights(model, pretrained_path=pretrained_path)\n",
    "\n",
    "    # Move model to appropriate device\n",
    "    model.to(model.device)\n",
    "\n",
    "    # Create dataloaders\n",
    "    batch_list_train = BatchList(\n",
    "        subtask_list=subtask_list, \n",
    "        sub_batch_size=sub_batch_size, \n",
    "        split=Split.TRAIN\n",
    "    )\n",
    "    \n",
    "    batch_list_dev = BatchList(\n",
    "        subtask_list=subtask_list, \n",
    "        sub_batch_size=eval_batch_size, \n",
    "        split=Split.DEV\n",
    "    )\n",
    "    \n",
    "    batch_list_eval = BatchListEvalTest(\n",
    "        subtask_list=subtask_list, \n",
    "        sub_batch_size=sub_batch_size, \n",
    "        split=Split.DEV\n",
    "    )\n",
    "    \n",
    "    batch_list_test = BatchListEvalTest(\n",
    "        subtask_list=subtask_list, \n",
    "        sub_batch_size=sub_batch_size, \n",
    "        split=Split.TEST\n",
    "    )\n",
    "\n",
    "    return model, batch_list_train, batch_list_dev, batch_list_eval, batch_list_test\n",
    "\n",
    "\n",
    "def save_head_initializations(model):\n",
    "    \"\"\"Save weight initialization of the head. This method will not be called anymore.\n",
    "     It's only for the initial saving of weight inits for all tasks.\"\"\"\n",
    "    for head_name in model.heads.keys():\n",
    "        torch.save(model.heads[head_name].state_dict(), 'model_files/heads/' + head_name + '_init.pth')\n",
    "    \n",
    "def load_head_initializations(model):\n",
    "    \"\"\"Load fixed weight initialization for each head in order to ensure reproducibility.\"\"\"\n",
    "    for head_name in model.heads.keys():\n",
    "        weights_path = 'model_files/heads/' + head_name + '_init.pth'\n",
    "        head_weights = torch.load(weights_path)\n",
    "        model.heads[head_name].load_state_dict(head_weights,strict=True)\n",
    "\n",
    "def load_pretrained_weights(model, pretrained_path):\n",
    "    \"\"\"Load the weights of a pretrained model.\"\"\"\n",
    "    weight_dict = torch.load(pretrained_path)\n",
    "    model.load_state_dict(weight_dict, strict=False)\n",
    "    return model\n"
   ],
   "id": "5dc74770576b2888",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training the Model\n",
    "\n",
    "For the model training the MAGPIE repository first introduces some helper functions. Since they are specific to the training, I include them into the notebook, instead of using them as a separate module like the other utility functions."
   ],
   "id": "58fc758d8464619"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:49:00.832171Z",
     "start_time": "2024-12-04T17:49:00.808212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "\"\"\"This module contains helper classes for model training.\"\"\"\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import math\n",
    "from enum import Enum\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from old_utils.enums import Split\n",
    "import wandb\n",
    "\n",
    "        \n",
    "class Logger:\n",
    "    \"\"\"Logger to keep track of metrics, losses and artifacts.\n",
    "\n",
    "    This logger is used as an abstraction. If we want to integrate with third party providers (wandb, GCS, ...),\n",
    "    use this logger.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, experiment_name: str):\n",
    "        \"\"\"Initialize a Logger.\"\"\"\n",
    "        PATH = \"logging/\" + experiment_name\n",
    "        os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "        self.experiment_logfilename = PATH + \"/train_data.log\"\n",
    "        experiment_logfile_handler = logging.FileHandler(filename=self.experiment_logfilename)\n",
    "        experiment_logfile_formatter = logging.Formatter(fmt=\"%(message)s\")\n",
    "        experiment_logfile_handler.setFormatter(experiment_logfile_formatter)\n",
    "\n",
    "        self.experiment_logger = logging.getLogger(\"experiment_logger\")\n",
    "        self.experiment_logger.addHandler(experiment_logfile_handler)\n",
    "        self.experiment_logger.setLevel(\"INFO\")\n",
    "\n",
    "    def log(self, out):\n",
    "        \"\"\"Log.\"\"\"\n",
    "        self.experiment_logger.info(out)\n",
    "        wandb.log(out)\n",
    "\n",
    "\n",
    "class EarlyStopperSingle:\n",
    "    \"\"\"\n",
    "    EarlyStopper for a single branch of the model.\n",
    "\n",
    "    Inspired by .https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int, min_delta: int, resurrection: bool):\n",
    "        \"\"\"Initialize an EarlyStopperSingle.\"\"\"\n",
    "        self.patience = patience\n",
    "        self.patience_zombie = 10\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.counter_zombie = 0\n",
    "        self.min_dev_loss = np.inf\n",
    "        self.min_dev_loss_zombie = np.inf\n",
    "        self.resurrection = resurrection\n",
    "\n",
    "    def early_stop(self, dev_loss):\n",
    "        \"\"\"Return True if dev_loss is steadily increasing.\"\"\"\n",
    "        if math.isnan(dev_loss):\n",
    "            return False\n",
    "        if dev_loss < self.min_dev_loss:\n",
    "            self.min_dev_loss = dev_loss\n",
    "            self.counter = 0\n",
    "        elif dev_loss > (self.min_dev_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def resurrect(self, dev_loss):\n",
    "        \"\"\"Return True if dev_loss is steadily increasing and a dead task should resurrect.\"\"\"\n",
    "        if math.isnan(dev_loss) or not self.resurrection:\n",
    "            return False\n",
    "        if dev_loss < self.min_dev_loss_zombie:\n",
    "            self.min_dev_loss_zombie = dev_loss\n",
    "            self.counter_zombie = 0\n",
    "        elif dev_loss > self.min_dev_loss_zombie:\n",
    "            self.counter_zombie += 1\n",
    "            if self.counter_zombie >= self.patience_zombie:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def reset_early_stopper(self):\n",
    "        \"\"\"Reset the state of an early stopper.\n",
    "\n",
    "        As zombies can resurrect and die multiple times, we have to reset their internal variables,\n",
    "        counter and the min_dev_loss each time a zombie resurrects or dies.\n",
    "        \"\"\"\n",
    "        self.counter_zombie = 0\n",
    "        self.counter = 0\n",
    "        self.min_dev_loss_zombie = np.inf\n",
    "        self.min_dev_loss = np.inf\n",
    "\n",
    "\n",
    "class EarlyStoppingMode(Enum):\n",
    "    \"\"\"Enum for early stopping mode.\"\"\"\n",
    "\n",
    "    HEADS = \"heads\"  # Only stop heads\n",
    "    BACKBONE = \"backbone\"  # Also stop backbone\n",
    "    NONE = \"none\"\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    \"\"\"EarlyStopper container for all heads.\"\"\"\n",
    "\n",
    "    def __init__(self, st_ids: List[str], mode: EarlyStoppingMode, patience, resurrection: bool, min_delta=0):\n",
    "        \"\"\"Initialize an EarlyStopper.\"\"\"\n",
    "        self.mode = mode\n",
    "        self.early_stoppers = {\n",
    "            st_id: EarlyStopperSingle(patience=patience[st_id], min_delta=min_delta, resurrection=resurrection)\n",
    "            for st_id in st_ids\n",
    "        }\n",
    "\n",
    "    def early_stop(self, st_id, dev_loss):\n",
    "        \"\"\"Return True if dev_loss is steadily increasing.\"\"\"\n",
    "        return (\n",
    "            False if self.mode == EarlyStoppingMode.NONE else self.early_stoppers[st_id].early_stop(dev_loss=dev_loss)\n",
    "        )\n",
    "\n",
    "    def resurrect(self, st_id, dev_loss):\n",
    "        \"\"\"Return True if dev_loss is steadily increasing and a dead task should resurrect.\"\"\"\n",
    "        return (\n",
    "            False if self.mode == EarlyStoppingMode.NONE else self.early_stoppers[st_id].resurrect(dev_loss=dev_loss)\n",
    "        )\n",
    "\n",
    "    def reset_early_stopper(self, st_id):\n",
    "        \"\"\"Reset the state of an early stopper.\"\"\"\n",
    "        self.early_stoppers[st_id].reset_early_stopper()\n",
    "\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"Abstract Accumulator.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Raise RuntimeError if this Accumulator is instantiated.\"\"\"\n",
    "        if type(self) == Accumulator:\n",
    "            raise RuntimeError(\"Abstract class <Accumulator> must not be instantiated.\")\n",
    "        self.gradients = None\n",
    "        self.n = 0\n",
    "\n",
    "    def update(self, gradients):\n",
    "        \"\"\"Update the values of a gradient.\n",
    "\n",
    "        Must be overwritten by concrete implementation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_avg_gradients(self):\n",
    "        \"\"\"Return the gradients, normalized across 0-axis.\"\"\"\n",
    "        out_gradients = copy.deepcopy(self.gradients)\n",
    "        for k, v in self.gradients.items():\n",
    "            out_gradients[k] /= self.n\n",
    "            out_gradients[k] = out_gradients[k].squeeze(dim=0)\n",
    "        return out_gradients\n",
    "\n",
    "    def get_gradients(self):\n",
    "        \"\"\"Return the gradients.\n",
    "\n",
    "        Must be overwritten by concrete implementation.\n",
    "        \"\"\"\n",
    "        return self.gradients\n",
    "\n",
    "\n",
    "class StackedAccumulator(Accumulator):\n",
    "    \"\"\"Accumulate the gradients for one SubTask within on Super-Batch.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize a StackedAccumulator.\"\"\"\n",
    "        super(StackedAccumulator, self).__init__()\n",
    "\n",
    "    def update(self, gradients, weight=1.0):\n",
    "        \"\"\"Update. Concatenate new set of gradients along 0-axis.\"\"\"\n",
    "        if not self.gradients:\n",
    "            self.gradients = gradients\n",
    "            # unsqueeze all gradients for later concatenation\n",
    "            for k, v in self.gradients.items():\n",
    "                self.gradients[k] = self.gradients[k].unsqueeze(dim=0) * weight\n",
    "        else:\n",
    "            for k, v in self.gradients.items():\n",
    "                new_value = gradients[k].unsqueeze(dim=0) * weight\n",
    "                self.gradients[k] = torch.cat((v, new_value), dim=0)\n",
    "        self.n += 1\n",
    "\n",
    "    def set_gradients(self, gradients: Dict[str, torch.tensor]):\n",
    "        \"\"\"Set the gradients.\"\"\"\n",
    "        for k, v in self.gradients.items():\n",
    "            self.gradients[k] = gradients[k].unsqueeze(dim=0)\n",
    "\n",
    "\n",
    "class RunningSumAccumulator(Accumulator):\n",
    "    \"\"\"Keep track of the running sum of gradients.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize a RunningSumAccumulator.\"\"\"\n",
    "        super(RunningSumAccumulator, self).__init__()\n",
    "\n",
    "    def update(self, gradients: Dict[str, torch.tensor], weight=1.0) -> None:\n",
    "        \"\"\"Update. Sum the gradients along 0-axis.\"\"\"\n",
    "        if not self.gradients:\n",
    "            self.gradients = gradients\n",
    "            # unsqueeze all gradients for later concatenation\n",
    "            for k, v in self.gradients.items():\n",
    "                self.gradients[k] = self.gradients[k].unsqueeze(dim=0) * weight\n",
    "        else:\n",
    "            for k, v in self.gradients.items():\n",
    "                new_value = gradients[k].unsqueeze(dim=0) * weight\n",
    "                self.gradients[k] = torch.add(v, new_value)\n",
    "        self.n += 1\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"The AverageMeter keeps track of a metric.\"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"Initialize an AverageMeter.\"\"\"\n",
    "        self.values = []\n",
    "        self.name = name\n",
    "\n",
    "    def mean_last_k(self, k=10):\n",
    "        \"\"\"Return the mean of the last k values.\"\"\"\n",
    "        assert 1 <= k\n",
    "        vals = self.values[-k:]\n",
    "        if len(vals) < k:\n",
    "            return float(\"NaN\")\n",
    "\n",
    "        return np.mean(vals)\n",
    "\n",
    "    def mean_all(self):\n",
    "        \"\"\"Return the mean of all values.\"\"\"\n",
    "        return np.mean(self.values)\n",
    "\n",
    "    def update(self, value=0):\n",
    "        \"\"\"Update the Metric by appending a new value.\"\"\"\n",
    "        self.values.append(value)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset AverageMeter.\"\"\"\n",
    "        self.values.clear()\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Print.\"\"\"\n",
    "        return f\"{self.mean_last_k(1):.2f}\"\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "    \"\"\"Keep track of all metrics and losses of an epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, heads, logger: Logger):\n",
    "        \"\"\"Initialize a Tracker.\"\"\"\n",
    "        self.metrics = self.init_metrics(heads=heads)\n",
    "        self.losses, self.combined_losses = self.init_losses(heads=heads)\n",
    "        self.logger = logger\n",
    "\n",
    "    def init_losses(self, heads):\n",
    "        \"\"\"Initialize the losses.\"\"\"\n",
    "        train_losses = {f\"{st_id}\": AverageMeter(name=f\"{st_id}_train_loss\") for st_id, head in heads.items()}\n",
    "        dev_losses = {f\"{st_id}\": AverageMeter(name=f\"{st_id}_dev_loss\") for st_id, head in heads.items()}\n",
    "        eval_losses = {f\"{st_id}\": AverageMeter(name=f\"{st_id}_eval_loss\") for st_id, head in heads.items()}\n",
    "        test_losses = {f\"{st_id}\": AverageMeter(name=f\"{st_id}_test_loss\") for st_id, head in heads.items()}\n",
    "        combined_losses = {\n",
    "            Split.TRAIN: AverageMeter(name=\"combined_train_loss\"),\n",
    "            Split.DEV: AverageMeter(name=\"combined_dev_loss\"),\n",
    "            Split.TEST: AverageMeter(name=\"combined_test_loss\"),\n",
    "            Split.EVAL: AverageMeter(name=\"combined_eval_loss\"),\n",
    "        }\n",
    "        return {\n",
    "            Split.TRAIN: train_losses,\n",
    "            Split.DEV: dev_losses,\n",
    "            Split.TEST: test_losses,\n",
    "            Split.EVAL: eval_losses,\n",
    "        }, combined_losses\n",
    "\n",
    "    def init_metrics(self, heads=Dict):\n",
    "        \"\"\"Initialize the AverageMeters for the metrics.\"\"\"\n",
    "        train_metrics = {\n",
    "            st_id: {m: AverageMeter(name=f\"{st_id}_train_{m}\") for m in head.metrics.keys()}\n",
    "            for st_id, head in heads.items()\n",
    "        }\n",
    "\n",
    "        dev_metrics = {\n",
    "            st_id: {m: AverageMeter(name=f\"{st_id}_dev_{m}\") for m in head.metrics.keys()}\n",
    "            for st_id, head in heads.items()\n",
    "        }\n",
    "\n",
    "        eval_metrics = {\n",
    "            st_id: {m: AverageMeter(name=f\"{st_id}_eval_{m}\") for m in head.metrics.keys()}\n",
    "            for st_id, head in heads.items()\n",
    "        }\n",
    "\n",
    "        test_metrics = {\n",
    "            st_id: {m: AverageMeter(name=f\"{st_id}_test_{m}\") for m in head.metrics.keys()}\n",
    "            for st_id, head in heads.items()\n",
    "        }\n",
    "        return {Split.TRAIN: train_metrics, Split.DEV: dev_metrics, Split.TEST: test_metrics, Split.EVAL: eval_metrics}\n",
    "\n",
    "    def update_metric(self, split, st_id, metric, value):\n",
    "        \"\"\"Update the metric, given a split and subtask id.\"\"\"\n",
    "        self.metrics[split][st_id][metric].update(value=value)\n",
    "\n",
    "    def update_loss(self, split, st_id, value):\n",
    "        \"\"\"Update the loss, given a split and subtask id.\"\"\"\n",
    "        self.losses[split][st_id].update(value)\n",
    "\n",
    "    def update_combined_loss(self, split, value):\n",
    "        \"\"\"Update the combined losses, given a split.\"\"\"\n",
    "        self.combined_losses[split].update(value)\n",
    "\n",
    "    def get_last_st_loss(self, split, st_id, k):\n",
    "        \"\"\"Get mean of last subtask loss.\"\"\"\n",
    "        return self.losses[split][st_id].mean_last_k(k=k)\n",
    "\n",
    "    def get_last_st_metric(self, split, st_id, k):\n",
    "        \"\"\"Get mean of last subtask metric.\"\"\"\n",
    "        return self.metrics[split][st_id][next(iter(self.metrics[split][st_id]))].mean_last_k(k=k)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent a Tracker.\"\"\"\n",
    "        return f\"TRAIN LOSS: {self.combined_losses[Split.TRAIN]} - DEV LOSS: {self.combined_losses[Split.DEV]} - EVAL LOSS: {self.combined_losses[Split.EVAL]}\"\n",
    "\n",
    "    def log(self, splits: List[Split], additional_payload: Dict[str, float] = {}):\n",
    "        \"\"\"Log the metrics & losses of a list of splits.\"\"\"\n",
    "        out: Dict[str, float] = {**additional_payload}\n",
    "        for split in splits:\n",
    "            if split in [Split.DEV, Split.TRAIN]:\n",
    "                metrics = {m.name: m.mean_last_k(1) for d in self.metrics[split].values() for m in d.values()}\n",
    "                combined_losses = self.combined_losses[split]\n",
    "                losses = {v.name: v.mean_last_k(1) for v in self.losses[split].values()}\n",
    "                out = {**out, **metrics, combined_losses.name: combined_losses.mean_last_k(1), **losses}\n",
    "            else:\n",
    "                metrics = {m.name: m.mean_all() for d in self.metrics[split].values() for m in d.values()}\n",
    "                combined_losses = self.combined_losses[split]\n",
    "                losses = {v.name: v.mean_all() for v in self.losses[split].values()}\n",
    "                out = {**out, **metrics, combined_losses.name: combined_losses.mean_all(), **losses}\n",
    "\n",
    "        self.logger.log(out)"
   ],
   "id": "9701bf95e04cf660",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:49:01.445392Z",
     "start_time": "2024-12-04T17:49:01.432154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict, Optional\n",
    "import torch\n",
    "from old_utils.enums import AggregationMethod\n",
    "import random  # Added missing import\n",
    "\n",
    "class GradientAggregator:\n",
    "    \"\"\"Aggregator class for combining possibly conflicting gradients into one \"optimal\" grad.\"\"\"\n",
    "    \n",
    "    def __init__(self, aggregation_method: AggregationMethod = AggregationMethod.MEAN):\n",
    "        \"\"\"Initialize GradientAggregator.\"\"\"\n",
    "        self.aggregation_method = aggregation_method\n",
    "        self.accumulator = (\n",
    "            RunningSumAccumulator() if aggregation_method == AggregationMethod.MEAN \n",
    "            else StackedAccumulator()\n",
    "        )\n",
    "        self._conflicting_gradient_count = 0\n",
    "        self._nonconflicting_gradient_count = 0\n",
    "\n",
    "    def reset_accumulator(self) -> None:\n",
    "        \"\"\"Reset the accumulator.\"\"\"\n",
    "        self.accumulator = (\n",
    "            RunningSumAccumulator() if self.aggregation_method == AggregationMethod.MEAN \n",
    "            else StackedAccumulator()\n",
    "        )\n",
    "\n",
    "    def find_nonconflicting_grad(self, grad_tensor: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Use on of the algorithms to find a nonconflicting gradient.\"\"\"\n",
    "        if self.aggregation_method == AggregationMethod.PCGRAD:\n",
    "            return self.pcgrad(grad_tensor).mean(dim=0)\n",
    "        elif self.aggregation_method == AggregationMethod.PCGRAD_ONLINE:\n",
    "            assert len(grad_tensor) == 2\n",
    "            return self.pcgrad_online(grad_tensor)\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "    def aggregate_gradients(self) -> torch.tensor:\n",
    "        \"\"\"Aggregate possibly conflicting set of gradients (given as a list of dictionaries).\"\"\"\n",
    "        conflicting_grads = self.accumulator.get_gradients()\n",
    "        length = len(conflicting_grads[list(conflicting_grads.keys())[0]])\n",
    "\n",
    "        if (self.aggregation_method == AggregationMethod.PCGRAD_ONLINE\n",
    "                or self.aggregation_method == AggregationMethod.MEAN):\n",
    "            assert length == 1\n",
    "            return self.accumulator.get_avg_gradients()\n",
    "        elif self.aggregation_method == AggregationMethod.PCGRAD:\n",
    "            conflicting_grads = [{k: v[i, ...] for k, v in conflicting_grads.items()} \n",
    "                               for i in range(length)]\n",
    "            final_grad: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "            if len(conflicting_grads) == 1:\n",
    "                return conflicting_grads[0]\n",
    "\n",
    "            keys = list(conflicting_grads[0].keys())\n",
    "            for layer_key in keys:\n",
    "                list_of_st_grads = [st_grad[layer_key] for st_grad in conflicting_grads]\n",
    "                final_grad.update({\n",
    "                    layer_key: self.find_nonconflicting_grad(torch.stack(list_of_st_grads, dim=0))\n",
    "                })\n",
    "            return final_grad\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "    def pcgrad(self, grad_tensor: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Project conflicting gradients onto orthogonal plane.\"\"\"\n",
    "        pc_grads, num_of_tasks = grad_tensor.clone(), len(grad_tensor)\n",
    "        original_shape = grad_tensor.shape\n",
    "        pc_grads = pc_grads.view(num_of_tasks, -1)\n",
    "        grad_tensor = grad_tensor.view(num_of_tasks, -1)\n",
    "\n",
    "        for g_i in range(num_of_tasks):\n",
    "            task_index = list(range(num_of_tasks))\n",
    "            random.shuffle(task_index)\n",
    "            for g_j in task_index:\n",
    "                dot_product = pc_grads[g_i].dot(grad_tensor[g_j])\n",
    "                if dot_product < 0:\n",
    "                    pc_grads[g_i] -= ((dot_product / (grad_tensor[g_j].norm() ** 2)) \n",
    "                                    * grad_tensor[g_j])\n",
    "                    self._conflicting_gradient_count += 1\n",
    "                else:\n",
    "                    self._nonconflicting_gradient_count += 1\n",
    "        return pc_grads.view(original_shape)\n",
    "\n",
    "    def pcgrad_online(self, grad_tensor: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Perform pcgrad (online) algorithm.\"\"\"\n",
    "        assert len(grad_tensor) == 2\n",
    "        p = grad_tensor[0]\n",
    "        g = grad_tensor[-1]\n",
    "\n",
    "        p = p.view(-1)\n",
    "        g = g.view(-1)\n",
    "\n",
    "        dot_product = p.dot(g)\n",
    "        if dot_product < 0:\n",
    "            p = p - (dot_product / (g.norm() ** 2)) * g\n",
    "            self._conflicting_gradient_count += 1\n",
    "        else:\n",
    "            self._nonconflicting_gradient_count += 1\n",
    "\n",
    "        p += g\n",
    "        return p.view(grad_tensor[0].shape)\n",
    "\n",
    "    def aggregate_gradients_online(self) -> Dict[str, torch.tensor]:\n",
    "        \"\"\"Aggregate the current overall gradient with a new gradient.\"\"\"\n",
    "        conflicting_grads = self.accumulator.get_gradients()\n",
    "        length = len(conflicting_grads[list(conflicting_grads.keys())[0]])\n",
    "        conflicting_grads = [{k: v[i, ...] for k, v in conflicting_grads.items()} \n",
    "                           for i in range(length)]\n",
    "        current_overall_grad: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "        if length == 1:\n",
    "            return conflicting_grads[0]\n",
    "        elif length == 2:\n",
    "            keys = list(conflicting_grads[0].keys())\n",
    "            for layer_key in keys:\n",
    "                list_of_st_grads = [st_grad[layer_key] for st_grad in conflicting_grads]\n",
    "                current_overall_grad.update({\n",
    "                    layer_key: self.find_nonconflicting_grad(torch.stack(list_of_st_grads, dim=0))\n",
    "                })\n",
    "            return current_overall_grad\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "    def update(self, gradients: Dict[str, torch.tensor], scaling_weight: float) -> None:\n",
    "        \"\"\"Update the gradients of the accumulator.\"\"\"\n",
    "        self.accumulator.update(gradients=gradients, weight=scaling_weight)\n",
    "        if self.aggregation_method == AggregationMethod.PCGRAD_ONLINE:\n",
    "            self.accumulator.set_gradients(gradients=self.aggregate_gradients_online())\n",
    "\n",
    "    def get_conflicting_gradients_ratio(self) -> Optional[float]:\n",
    "        \"\"\"Get the ratio of conflicting gradients.\"\"\"\n",
    "        if self.aggregation_method == AggregationMethod.MEAN:\n",
    "            raise Exception\n",
    "        if self._conflicting_gradient_count + self._nonconflicting_gradient_count == 0:\n",
    "            raise Exception\n",
    "        return (self._conflicting_gradient_count / \n",
    "                (self._conflicting_gradient_count + self._nonconflicting_gradient_count))"
   ],
   "id": "ea50454b844bdda5",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T18:01:00.321916Z",
     "start_time": "2024-12-04T18:01:00.298259Z"
    }
   },
   "cell_type": "code",
   "source": [
    " \"\"\"This module contains the trainer class.\"\"\"\n",
    "import statistics as stats\n",
    "from typing import Any, Dict, List\n",
    "from config.config import MAX_NUMBER_OF_STEPS\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import get_polynomial_decay_schedule_with_warmup\n",
    "\n",
    "from old_utils.enums import AggregationMethod, LossScaling, Split\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Trainer class to train and evaluate a model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        task_list: List[Task],\n",
    "        initial_lr,\n",
    "        model_name: str,\n",
    "        pretrained_path: str,\n",
    "        sub_batch_size: int,\n",
    "        eval_batch_size: int,\n",
    "        early_stopping_mode,\n",
    "        resurrection: bool,\n",
    "        aggregation_method: AggregationMethod,\n",
    "        loss_scaling: LossScaling,\n",
    "        num_warmup_steps: int,\n",
    "        head_specific_lr_dict: Dict[str, float],\n",
    "        head_specific_patience_dict: Dict[str, int],\n",
    "        head_specific_max_epoch_dict: Dict[str, int],\n",
    "        logger: Logger,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Initialize a Trainer.\"\"\"\n",
    "        self.early_stopping_mode = early_stopping_mode\n",
    "        self.logger = logger\n",
    "        self.loss_scaling = loss_scaling\n",
    "        self.model, batch_list_train, batch_list_dev, batch_list_eval, batch_list_test = ModelFactory(\n",
    "            task_list=task_list,\n",
    "            sub_batch_size=sub_batch_size,\n",
    "            eval_batch_size=eval_batch_size,\n",
    "            pretrained_path=pretrained_path,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.batch_lists = {\n",
    "            Split.TRAIN: batch_list_train,\n",
    "            Split.DEV: batch_list_dev,\n",
    "            Split.EVAL: batch_list_eval,\n",
    "            Split.TEST: batch_list_test,\n",
    "        }\n",
    "\n",
    "        # shared backbone model optimizer\n",
    "        self.lm_optimizer = torch.optim.AdamW(self.model.language_model.backbone.parameters(), lr=initial_lr)\n",
    "        self.lm_lr_scheduler = get_polynomial_decay_schedule_with_warmup(\n",
    "            optimizer=self.lm_optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=max([len(dl) for dl in self.batch_lists[Split.TRAIN].dataloaders.values()])\n",
    "            * stats.median(head_specific_max_epoch_dict.values()),\n",
    "        )\n",
    "\n",
    "        # task-specifics optimizers\n",
    "        self.head_optimizers = {\n",
    "            str(st_id): torch.optim.AdamW(head.parameters(), lr=head_specific_lr_dict[st_id])\n",
    "            for st_id, head in self.model.heads.items()\n",
    "        }\n",
    "        self.head_lr_schedulers = {\n",
    "            str(st_id): get_polynomial_decay_schedule_with_warmup(\n",
    "                optimizer=self.head_optimizers[st_id],\n",
    "                num_warmup_steps=num_warmup_steps,\n",
    "                num_training_steps=len(self.batch_lists[Split.TRAIN].dataloaders[st_id])\n",
    "                * head_specific_max_epoch_dict[st_id],\n",
    "            )\n",
    "            for st_id in self.model.heads.keys()\n",
    "        }\n",
    "\n",
    "        # flags controlling stopping and resurrection\n",
    "        self.task_alive_flags = {str(st_id): True for st_id in self.model.heads.keys()}\n",
    "        self.task_zombie_flags = {str(st_id): False for st_id in self.model.heads.keys()}\n",
    "        self.early_stopper = EarlyStopper(\n",
    "            st_ids=self.model.heads.keys(),\n",
    "            mode=self.early_stopping_mode,\n",
    "            patience=head_specific_patience_dict,\n",
    "            resurrection=resurrection,\n",
    "        )\n",
    "\n",
    "        self.tracker = Tracker(heads=self.model.heads, logger=logger)\n",
    "        self.GA = GradientAggregator(aggregation_method=aggregation_method)\n",
    "        self.progress_bar = tqdm(range(len(self.model.heads)))\n",
    "        self.model_name = model_name\n",
    "        self.scaling_weights = {str(st.id): st.get_scaling_weight() for t in task_list for st in t.subtasks_list}\n",
    "        self.MAX_NUMBER_OF_STEPS = MAX_NUMBER_OF_STEPS\n",
    "        self.k = 50\n",
    "\n",
    "    def head_specific_optimization(self, st_id: str, lm_grads, scaling_weight):\n",
    "        \"\"\"\n",
    "        Perform the optimization of a task-specific head.\n",
    "\n",
    "        This method is only called when mode is training.\n",
    "        @param st_id: The subtask id.\n",
    "        @param lm_grads: The LM gradients.\n",
    "        @param scaling_weight: The scaling weight of that subtask.\n",
    "        @return: A dictionary with additional payload containing the conflicting gradients ratio.\n",
    "        \"\"\"\n",
    "        additional_payload = {}\n",
    "        last_dev_loss = self.tracker.get_last_st_loss(split=Split.DEV, st_id=st_id, k=self.k)\n",
    "        should_stop_now = (\n",
    "            self.early_stopper.early_stop(st_id=st_id, dev_loss=last_dev_loss)\n",
    "            if (self.task_alive_flags[st_id] or self.task_zombie_flags[st_id])\n",
    "            else False\n",
    "        )\n",
    "\n",
    "        should_resurrect_now = (\n",
    "            self.early_stopper.resurrect(st_id=st_id, dev_loss=last_dev_loss)\n",
    "            if (not self.task_zombie_flags[st_id] and not self.task_alive_flags[st_id])\n",
    "            else False\n",
    "        )\n",
    "\n",
    "        should_stay_zombie = not self.task_alive_flags[st_id] and self.task_zombie_flags[st_id] and not should_stop_now\n",
    "\n",
    "        # Eval + Log task when it DIES\n",
    "        if should_stop_now and self.task_alive_flags[st_id]:\n",
    "            print(f\"Subtask {st_id} is now DEAD.\")\n",
    "            self.eval_st(split=Split.EVAL, st_id=st_id)\n",
    "            self.tracker.log(splits=[Split.EVAL], additional_payload={st_id + \"_STOPPED\": 0})\n",
    "            self.progress_bar.update()\n",
    "\n",
    "        # Eval + Log task when it RESURRECTS\n",
    "        elif should_resurrect_now and not self.task_zombie_flags[st_id]:\n",
    "            print(f\"Subtask {st_id} is now ZOMBIE.\")\n",
    "            additional_payload[st_id + \"_ZOMBIE\"] = 0\n",
    "            self.early_stopper.reset_early_stopper(st_id=st_id)\n",
    "\n",
    "        # Eval + Log task when a ZOMBIE DIES\n",
    "        elif should_stop_now and self.task_zombie_flags[st_id]:\n",
    "            print(f\"Subtask {st_id} is now DEAD AGAIN.\")\n",
    "            additional_payload[st_id + \"_DEAD_ZOMBIE\"] = 0\n",
    "            self.early_stopper.reset_early_stopper(st_id=st_id)\n",
    "\n",
    "        self.task_alive_flags[st_id] = self.task_alive_flags[st_id] and not (\n",
    "            should_stop_now or self.tracker.get_last_st_metric(split=Split.DEV, st_id=st_id, k=10) == 1\n",
    "        )\n",
    "        self.task_zombie_flags[st_id] = should_resurrect_now or should_stay_zombie\n",
    "\n",
    "        # We optimize a task if it is alive or zombie\n",
    "        optimize_task = self.task_alive_flags[str(st_id)] or self.task_zombie_flags[str(st_id)]\n",
    "        if optimize_task:\n",
    "            self.head_optimizers[st_id].step()\n",
    "            self.head_lr_schedulers[st_id].step()\n",
    "\n",
    "        if self.early_stopping_mode != EarlyStoppingMode.BACKBONE or optimize_task:\n",
    "            self.GA.update(lm_grads, scaling_weight=scaling_weight)\n",
    "\n",
    "        return additional_payload\n",
    "\n",
    "    def backbone_optimization(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform the optimization of the backbone.\n",
    "\n",
    "        This method is only called when mode is training.\n",
    "        @return: A dictionary with additional payload containing the conflicting gradients ratio.\n",
    "        \"\"\"\n",
    "        # Optimize the LM such that: we aggregate gradients from subtasks and set the final\n",
    "        # gradient to the LM and subsequently optimize (only the LM)\n",
    "        additional_payload = {}\n",
    "        if any(self.task_alive_flags.values()):\n",
    "            aggregated_gradients = self.GA.aggregate_gradients()\n",
    "            self.model.language_model.set_grads(aggregated_gradients)\n",
    "            self.lm_optimizer.step()\n",
    "            self.lm_lr_scheduler.step()\n",
    "        if self.GA.aggregation_method in [AggregationMethod.PCGRAD, AggregationMethod.PCGRAD_ONLINE]:\n",
    "            conflicting_gradients_ratio = self.GA.get_conflicting_gradients_ratio()\n",
    "            additional_payload[\"conflicting_gradients_ratio\"] = conflicting_gradients_ratio\n",
    "\n",
    "        return additional_payload\n",
    "\n",
    "    def handle_batch(self, batch, split: Split = Split.TRAIN) -> Dict[str, Any]:\n",
    "        \"\"\"Handle a batch.\n",
    "\n",
    "         (always) Pass a batch of sub_batches through the network.\n",
    "         (in train-mode) For each sub_batch, accumulate the gradients of the LM.\n",
    "         For each sub_batch and each st_id,\n",
    "            - (in train-mode) accumulate the gradients of the respective head,\n",
    "            - (always) accumulate the metric of the respective head,\n",
    "            - (always) accumulate the loss of the respective head.\n",
    "        (always) Log all metrics and losses to wandb.\n",
    "         (in train-mode) After all sub_batches are processed, normalize the LM gradients and the head-specific gradients.\n",
    "         (in train-mode) Then, perform the step of the lr_scheduler and the optimizer.\n",
    "\n",
    "        @param batch: The batch containing sub-batches.\n",
    "        @param split: The split (TRAIN, DEV, TEST)\n",
    "        @return: A dictionary containing additional payload that needs to be logged.\n",
    "        \"\"\"\n",
    "        training = split == Split.TRAIN\n",
    "        losses = []\n",
    "        additional_payloads: Dict[str, Any] = {}\n",
    "        # reset accumulator only if it's a new batch for training, otherwise eval drops accumulated gradients\n",
    "        if training:\n",
    "            self.GA.reset_accumulator()\n",
    "\n",
    "        # sub_batch consists of data of one subtask only\n",
    "        for sub_batch in batch:\n",
    "            X, attention_masks, Y, st_id = sub_batch\n",
    "            loss, metric_values, lm_grads = self._step((X, attention_masks, Y, st_id.unique()), training=training)\n",
    "            st_id = str(st_id.unique().item())\n",
    "            scaling_weight = self.scaling_weights[st_id] if self.loss_scaling == LossScaling.STATIC else 1.0\n",
    "\n",
    "            if training:\n",
    "                additional_payload = self.head_specific_optimization(\n",
    "                    st_id=st_id, lm_grads=lm_grads, scaling_weight=scaling_weight\n",
    "                )\n",
    "                additional_payloads = {**additional_payload, **additional_payloads}\n",
    "\n",
    "            # Update losses & metrics\n",
    "            for metric, value in metric_values.items():\n",
    "                self.tracker.update_metric(split=split, st_id=st_id, metric=metric, value=value)\n",
    "            self.tracker.update_loss(split=split, st_id=st_id, value=loss.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if training:\n",
    "            additional_payload = self.backbone_optimization()\n",
    "            additional_payloads = {**additional_payload, **additional_payloads}\n",
    "\n",
    "        self.tracker.update_combined_loss(split=split, value=np.mean(losses))\n",
    "        return additional_payloads\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Fit a model.\"\"\"\n",
    "        step = 0\n",
    "        \n",
    "        for i in range(self.MAX_NUMBER_OF_STEPS):\n",
    "            additional_payload_train, additional_payload_dev = {}, {}\n",
    "            # Check if any task is still training\n",
    "            if not any(self.task_alive_flags.values()):\n",
    "                break\n",
    "            step += 1\n",
    "            batch = next(self.batch_lists[Split.TRAIN])\n",
    "            additional_payload_train = self.handle_batch(batch=batch, split=Split.TRAIN)\n",
    "            if step % 3 == 0:\n",
    "                batch = next(self.batch_lists[Split.DEV])\n",
    "                additional_payload_dev = self.handle_batch(batch=batch, split=Split.DEV)\n",
    "            self.refresh_pbar()\n",
    "            self.tracker.log(\n",
    "                splits=[Split.TRAIN, Split.DEV],\n",
    "                additional_payload={**additional_payload_train, **additional_payload_dev},\n",
    "            )\n",
    "\n",
    "        self.eval(split=Split.EVAL)\n",
    "\n",
    "    def _step(self, batch, training: bool = True):\n",
    "        \"\"\"\n",
    "        Make one step.\n",
    "\n",
    "        @param batch: A dictionary containing X, Y, std_ids and attention_masks.\n",
    "        \"\"\"\n",
    "        inputs = {\"X\": batch[0], \"attention_masks\": batch[1], \"Y\": batch[2], \"st_id\": batch[3]}\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "        if training:\n",
    "            self.model.train()\n",
    "            loss, heads_metrics_values = self.model(**inputs)\n",
    "            self.lm_optimizer.zero_grad()\n",
    "            for st_id, optim in self.head_optimizers.items():\n",
    "                optim.zero_grad()\n",
    "            loss.backward()\n",
    "            lm_gradients = self.model.language_model.get_grads()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            lm_gradients = None\n",
    "            with torch.no_grad():\n",
    "                loss, heads_metrics_values = self.model(**inputs)\n",
    "\n",
    "        del inputs\n",
    "        return loss, heads_metrics_values, lm_gradients\n",
    "\n",
    "    def eval(self, split):\n",
    "        \"\"\"Evaluate the model on the entire test or dev set.\"\"\"\n",
    "        assert split in [Split.EVAL, Split.TEST]\n",
    "\n",
    "        for st_id in self.batch_lists[split].iter_dataloaders.keys():\n",
    "            self.eval_st(split=split, st_id=st_id)\n",
    "\n",
    "        self.tracker.log(splits=[split])\n",
    "\n",
    "    def eval_st(self, split, st_id):\n",
    "        \"\"\"Evaluate on a subtask, given a certain split.\"\"\"\n",
    "        batch_list = self.batch_lists[split]\n",
    "        batch_list._reset()\n",
    "        idl = batch_list.iter_dataloaders[st_id]\n",
    "        for batch in idl:\n",
    "            _ = self.handle_batch(batch=[batch], split=split)\n",
    "\n",
    "    def refresh_pbar(self):\n",
    "        \"\"\"Update the progress bar.\"\"\"\n",
    "        desc = str(self.tracker)\n",
    "        self.progress_bar.set_description(desc=desc)\n",
    "        self.progress_bar.refresh()\n",
    "\n",
    "    def fit_debug(self, k: int):\n",
    "        \"\"\"Fit for k iterations only to check if a model can process the data.\"\"\"\n",
    "        step = 0\n",
    "        for _ in range(k):\n",
    "            step += 1\n",
    "            batch = next(self.batch_lists[Split.TRAIN])\n",
    "            self.handle_batch(batch=batch, split=Split.TRAIN)\n",
    "            # Evaluate on dev-batch\n",
    "            batch = next(self.batch_lists[Split.DEV])\n",
    "            self.handle_batch(batch=batch, split=Split.DEV)\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"Save the model.\"\"\"\n",
    "        os.makedirs(\"model_files\", exist_ok=True) # added \n",
    "        model_files_path = \"model_files/\" + self.model_name + \".pth\"\n",
    "        torch.save(self.model.state_dict(), model_files_path)"
   ],
   "id": "87b131cf65c76f07",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Running the experiment\n",
    "For actually running the experiment the configurations from the \"cotrain_random_tasks.py\" were taken and adapted to the changes (MFFLOW logging etc.). \n",
    "Instead of the .fit() method of the trainer class, I use the .fit_debug() method, to check the general ability of the model to process the dta.\n",
    "The experiment was run on the local machine."
   ],
   "id": "5191be2aeb0be3ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T18:01:03.216803Z",
     "start_time": "2024-12-04T18:01:03.211903Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Current working directory: {os.getcwd()}\")",
   "id": "24be4019d8626b2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/heddafiedler/Documents/MASTER_DATA_SCIENCE/Semester_3/DL/DL_Project\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T18:01:05.898635Z",
     "start_time": "2024-12-04T18:01:05.895615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# changing working directory to the root of the project:/Users/heddafiedler/Documents/MASTER_DATA_SCIENCE/Semester_3/DL/DL_Project\n",
    "os.chdir(\"/Users/heddafiedler/Documents/MASTER_DATA_SCIENCE/Semester_3/DL/DL_Project\")"
   ],
   "id": "d7f71cc02cf87461",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T18:01:06.510013Z",
     "start_time": "2024-12-04T18:01:06.506965Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Current working directory: {os.getcwd()}\")",
   "id": "d96efb81f4cdb87e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/heddafiedler/Documents/MASTER_DATA_SCIENCE/Semester_3/DL/DL_Project\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T18:02:15.817566Z",
     "start_time": "2024-12-04T18:01:26.235722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Script for executing the experiment 1. Run co-training of all families.\"\"\"\n",
    "import os\n",
    "import wandb\n",
    "from old_utils.enums import Split, AggregationMethod, LossScaling\n",
    "from old_utils.common import set_random_seed\n",
    "from config.config import (\n",
    "    head_specific_lr,\n",
    "    head_specific_max_epoch,\n",
    "    head_specific_patience)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "EXPERIMENT_NAME = \"experiment_baseline_check\"\n",
    "selected_tasks = [\n",
    "babe_10,\n",
    "cw_hard_03,\n",
    "]\n",
    "tasks = selected_tasks\n",
    "\n",
    "for t in tasks:\n",
    "    for st in t.subtasks_list:\n",
    "        st.process()\n",
    "\n",
    "\n",
    "# training config\n",
    "config = {\n",
    "   \"sub_batch_size\": 32,\n",
    "   \"eval_batch_size\": 128,\n",
    "   \"initial_lr\": 4e-5,\n",
    "   \"dropout_prob\": 0.1,\n",
    "   \"hidden_dimension\": 768,\n",
    "   \"input_dimension\": 768,\n",
    "   \"aggregation_method\": AggregationMethod.MEAN,\n",
    "   \"early_stopping_mode\": EarlyStoppingMode.HEADS,\n",
    "   \"loss_scaling\": LossScaling.STATIC,\n",
    "   \"num_warmup_steps\": 10,\n",
    "   \"pretrained_path\": None,\n",
    "   \"resurrection\": True,\n",
    "   \"model_name\": \"YOUR_MODEL_NAME\",\n",
    "   \"head_specific_lr_dict\": head_specific_lr,\n",
    "   \"head_specific_patience_dict\": head_specific_patience,\n",
    "   \"head_specific_max_epoch_dict\": head_specific_max_epoch,\n",
    "   \"logger\": Logger(EXPERIMENT_NAME),\n",
    " }\n",
    "\n",
    "set_random_seed() # default is 321\n",
    "wandb.init(project=EXPERIMENT_NAME,name=\"YOUR_MODEL_NAME\")\n",
    "trainer = Trainer(task_list=tasks, **config)\n",
    "trainer.fit_debug(k=1)\n",
    "trainer.eval(split=Split.TEST)\n",
    "trainer.save_model()\n",
    "wandb.finish()"
   ],
   "id": "6779ca63c6f7366d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-04 19:01:26,252: INFO: 3233285385: Processing SubTask 10001]\n",
      "[2024-12-04 19:01:26,253: INFO: 3233285385: Loading data from datasets/10_BABE/preprocessed.csv]\n",
      "[2024-12-04 19:01:26,533: INFO: 3233285385: Data loaded successfully: 3672 samples]\n",
      "[2024-12-04 19:01:26,574: INFO: 3233285385: SubTask 10001 processed successfully]\n",
      "[2024-12-04 19:01:26,575: INFO: 3233285385: Processing SubTask 10002]\n",
      "[2024-12-04 19:01:26,575: INFO: 3233285385: Loading data from datasets/10_BABE/preprocessed.csv]\n",
      "[2024-12-04 19:01:27,258: INFO: 3233285385: Data loaded successfully: 3672 samples]\n",
      "[2024-12-04 19:01:27,310: INFO: 3233285385: SubTask 10002 processed successfully]\n",
      "[2024-12-04 19:01:27,311: INFO: 3233285385: Processing SubTask 300001]\n",
      "[2024-12-04 19:01:27,312: INFO: 3233285385: Loading data from datasets/03_CW_HARD/preprocessed.csv]\n",
      "[2024-12-04 19:01:27,714: INFO: 3233285385: Data loaded successfully: 6843 samples]\n",
      "[2024-12-04 19:01:27,792: INFO: 3233285385: SubTask 300001 processed successfully]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing last run (ID:o1vhnavz) before initializing another..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>10001_test_acc</td><td></td></tr><tr><td>10001_test_f1</td><td></td></tr><tr><td>10001_test_loss</td><td></td></tr><tr><td>10002_test_acc</td><td></td></tr><tr><td>10002_test_f1</td><td></td></tr><tr><td>10002_test_loss</td><td></td></tr><tr><td>300001_test_acc</td><td></td></tr><tr><td>300001_test_f1</td><td></td></tr><tr><td>300001_test_loss</td><td></td></tr><tr><td>combined_test_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>10001_test_acc</td><td>0.52344</td></tr><tr><td>10001_test_f1</td><td>0.43736</td></tr><tr><td>10001_test_loss</td><td>0.68885</td></tr><tr><td>10002_test_acc</td><td>0.14876</td></tr><tr><td>10002_test_f1</td><td>0.10404</td></tr><tr><td>10002_test_loss</td><td>1.15814</td></tr><tr><td>300001_test_acc</td><td>0.30802</td></tr><tr><td>300001_test_f1</td><td>0.29569</td></tr><tr><td>300001_test_loss</td><td>0.7169</td></tr><tr><td>combined_test_loss</td><td>0.82469</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">YOUR_MODEL_NAME</strong> at: <a href='https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check/runs/o1vhnavz' target=\"_blank\">https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check/runs/o1vhnavz</a><br/> View project at: <a href='https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check' target=\"_blank\">https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20241204_185553-o1vhnavz/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Successfully finished last run (ID:o1vhnavz). Initializing new run:<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/heddafiedler/Documents/MASTER_DATA_SCIENCE/Semester_3/DL/DL_Project/wandb/run-20241204_190127-du852nfc</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check/runs/du852nfc' target=\"_blank\">YOUR_MODEL_NAME</a></strong> to <a href='https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check' target=\"_blank\">https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check/runs/du852nfc' target=\"_blank\">https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check/runs/du852nfc</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heddafiedler/anaconda3/envs/dl_project/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-04 19:01:31,730: INFO: 4155765917: Creating BatchList with 3 subtasks, batch size 32]\n",
      "[2024-12-04 19:01:31,731: INFO: 4155765917: Initializing dataset for subtask 10001 with split Split.TRAIN]\n",
      "[2024-12-04 19:01:31,731: INFO: 4155765917: Resetting dataset for subtask 10001]\n",
      "[2024-12-04 19:01:31,733: INFO: 4155765917: Initializing dataset for subtask 10002 with split Split.TRAIN]\n",
      "[2024-12-04 19:01:31,734: INFO: 4155765917: Resetting dataset for subtask 10002]\n",
      "[2024-12-04 19:01:31,735: INFO: 4155765917: Initializing dataset for subtask 300001 with split Split.TRAIN]\n",
      "[2024-12-04 19:01:31,736: INFO: 4155765917: Resetting dataset for subtask 300001]\n",
      "[2024-12-04 19:01:31,737: INFO: 4155765917: Creating BatchList with 3 subtasks, batch size 128]\n",
      "[2024-12-04 19:01:31,738: INFO: 4155765917: Initializing dataset for subtask 10001 with split Split.DEV]\n",
      "[2024-12-04 19:01:31,738: INFO: 4155765917: Resetting dataset for subtask 10001]\n",
      "[2024-12-04 19:01:31,740: INFO: 4155765917: Initializing dataset for subtask 10002 with split Split.DEV]\n",
      "[2024-12-04 19:01:31,740: INFO: 4155765917: Resetting dataset for subtask 10002]\n",
      "[2024-12-04 19:01:31,741: INFO: 4155765917: Initializing dataset for subtask 300001 with split Split.DEV]\n",
      "[2024-12-04 19:01:31,741: INFO: 4155765917: Resetting dataset for subtask 300001]\n",
      "[2024-12-04 19:01:31,743: INFO: 4155765917: Creating BatchListEvalTest with 3 subtasks]\n",
      "[2024-12-04 19:01:31,743: INFO: 4155765917: Initializing dataset for subtask 10001 with split Split.DEV]\n",
      "[2024-12-04 19:01:31,743: INFO: 4155765917: Resetting dataset for subtask 10001]\n",
      "[2024-12-04 19:01:31,745: INFO: 4155765917: Initializing dataset for subtask 10002 with split Split.DEV]\n",
      "[2024-12-04 19:01:31,745: INFO: 4155765917: Resetting dataset for subtask 10002]\n",
      "[2024-12-04 19:01:31,746: INFO: 4155765917: Initializing dataset for subtask 300001 with split Split.DEV]\n",
      "[2024-12-04 19:01:31,746: INFO: 4155765917: Resetting dataset for subtask 300001]\n",
      "[2024-12-04 19:01:31,748: INFO: 4155765917: Creating BatchListEvalTest with 3 subtasks]\n",
      "[2024-12-04 19:01:31,748: INFO: 4155765917: Initializing dataset for subtask 10001 with split Split.TEST]\n",
      "[2024-12-04 19:01:31,748: INFO: 4155765917: Resetting dataset for subtask 10001]\n",
      "[2024-12-04 19:01:31,749: INFO: 4155765917: Initializing dataset for subtask 10002 with split Split.TEST]\n",
      "[2024-12-04 19:01:31,750: INFO: 4155765917: Resetting dataset for subtask 10002]\n",
      "[2024-12-04 19:01:31,751: INFO: 4155765917: Initializing dataset for subtask 300001 with split Split.TEST]\n",
      "[2024-12-04 19:01:31,751: INFO: 4155765917: Resetting dataset for subtask 300001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-04 19:01:31,755: INFO: 4155765917: Generated batch with 3 sub-batches]\n",
      "[2024-12-04 19:01:38,826: INFO: 4155765917: Generated batch with 3 sub-batches]\n",
      "[2024-12-04 19:02:12,734: INFO: 2804225146: {'10001_test_f1': 0.4373633, '10001_test_acc': 0.5234375, '10002_test_f1': 0.10404127, '10002_test_acc': 0.14875808, '300001_test_f1': 0.29568994, '300001_test_acc': 0.3080201, 'combined_test_loss': 0.8246867604877638, '10001_test_loss': 0.6888488233089447, '10002_test_loss': 1.1581404407819111, '300001_test_loss': 0.7168963551521301}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>10001_test_acc</td><td></td></tr><tr><td>10001_test_f1</td><td></td></tr><tr><td>10001_test_loss</td><td></td></tr><tr><td>10002_test_acc</td><td></td></tr><tr><td>10002_test_f1</td><td></td></tr><tr><td>10002_test_loss</td><td></td></tr><tr><td>300001_test_acc</td><td></td></tr><tr><td>300001_test_f1</td><td></td></tr><tr><td>300001_test_loss</td><td></td></tr><tr><td>combined_test_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>10001_test_acc</td><td>0.52344</td></tr><tr><td>10001_test_f1</td><td>0.43736</td></tr><tr><td>10001_test_loss</td><td>0.68885</td></tr><tr><td>10002_test_acc</td><td>0.14876</td></tr><tr><td>10002_test_f1</td><td>0.10404</td></tr><tr><td>10002_test_loss</td><td>1.15814</td></tr><tr><td>300001_test_acc</td><td>0.30802</td></tr><tr><td>300001_test_f1</td><td>0.29569</td></tr><tr><td>300001_test_loss</td><td>0.7169</td></tr><tr><td>combined_test_loss</td><td>0.82469</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">YOUR_MODEL_NAME</strong> at: <a href='https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check/runs/du852nfc' target=\"_blank\">https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check/runs/du852nfc</a><br/> View project at: <a href='https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check' target=\"_blank\">https://wandb.ai/hedfied-leuphana-universit-t-l-neburg/experiment_baseline_check</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20241204_190127-du852nfc/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4ce7bd697953908f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
